{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1606139-b496-4e53-a8f2-c7bcb6e47719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pygame\n",
    "import cv2  # for video capture\n",
    "\n",
    "# Enable cuDNN benchmarking for performance (if using GPU)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Constants for gridworld and visualization\n",
    "GRID_SIZE = 15\n",
    "CELL_SIZE = 50   # decreased cell size so that the grid is smaller on a 1440p screen\n",
    "WINDOW_SIZE = GRID_SIZE * CELL_SIZE\n",
    "NUM_OBSTACLES = 25  # increased number of obstacles\n",
    "\n",
    "# ----------------------------\n",
    "# Environment (Gridworld)\n",
    "# ----------------------------\n",
    "class Gridworld:\n",
    "    def __init__(self):\n",
    "        self.grid_size = GRID_SIZE\n",
    "        self.cell_size = CELL_SIZE\n",
    "        self.vision_radius = 5  # drones can sense obstacles within 5 cells\n",
    "        \n",
    "        # Define goals:\n",
    "        # Agent 0's goal is bottom-right; Agent 1's goal is bottom-left.\n",
    "        self.goals = [(GRID_SIZE - 1, GRID_SIZE - 1), (0, GRID_SIZE - 1)]\n",
    "        \n",
    "        self.obstacles = self.generate_obstacles()\n",
    "        \n",
    "        # Compute best paths using A* (only considering obstacles)\n",
    "        self.best_path_agent0 = self.a_star((0, 0), self.goals[0])\n",
    "        self.best_path_agent1 = self.a_star((0, 0), self.goals[1])\n",
    "        \n",
    "        # Congestion zones (dynamic) with a timer\n",
    "        self.congestion_zones = []\n",
    "        self.congestion_duration = 50  # congestion zones persist for 50 steps\n",
    "        self.congestion_timer = 0\n",
    "        \n",
    "        # Both agents start at top-left (0,0)\n",
    "        self.agents = [(0, 0), (0, 0)]\n",
    "        \n",
    "        # Penalty and reward values\n",
    "        self.movement_penalty = -1\n",
    "        self.closeness_penalty = -10\n",
    "        self.collision_penalty = -50\n",
    "        self.congestion_penalty = -20\n",
    "        self.goal_reward = 50\n",
    "        self.shaping_factor = 0.5\n",
    "        \n",
    "        # Parameters for following the best path\n",
    "        self.best_path_bonus = 10\n",
    "        self.deviation_penalty = -10\n",
    "\n",
    "    def generate_obstacles(self):\n",
    "        obstacles = set()\n",
    "        while len(obstacles) < NUM_OBSTACLES:\n",
    "            x = random.randint(0, self.grid_size - 1)\n",
    "            y = random.randint(0, self.grid_size - 1)\n",
    "            if (x, y) != (0, 0) and (x, y) not in self.goals:\n",
    "                obstacles.add((x, y))\n",
    "        return list(obstacles)\n",
    "\n",
    "    def heuristic(self, node, goal):\n",
    "        return abs(node[0] - goal[0]) + abs(node[1] - goal[1])\n",
    "\n",
    "    def get_neighbors(self, node):\n",
    "        (x, y) = node\n",
    "        neighbors = []\n",
    "        for dx, dy in [(1,0), (-1,0), (0,1), (0,-1)]:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if 0 <= nx < self.grid_size and 0 <= ny < self.grid_size:\n",
    "                if (nx, ny) not in self.obstacles:\n",
    "                    neighbors.append((nx, ny))\n",
    "        return neighbors\n",
    "\n",
    "    def a_star(self, start, goal):\n",
    "        import heapq\n",
    "        open_set = []\n",
    "        heapq.heappush(open_set, (0, start))\n",
    "        came_from = {}\n",
    "        g_score = {start: 0}\n",
    "        while open_set:\n",
    "            current_f, current = heapq.heappop(open_set)\n",
    "            if current == goal:\n",
    "                path = [current]\n",
    "                while current in came_from:\n",
    "                    current = came_from[current]\n",
    "                    path.append(current)\n",
    "                path.reverse()\n",
    "                return path\n",
    "            for neighbor in self.get_neighbors(current):\n",
    "                tentative_g = g_score[current] + 1\n",
    "                if neighbor not in g_score or tentative_g < g_score[neighbor]:\n",
    "                    came_from[neighbor] = current\n",
    "                    g_score[neighbor] = tentative_g\n",
    "                    f_score = tentative_g + self.heuristic(neighbor, goal)\n",
    "                    heapq.heappush(open_set, (f_score, neighbor))\n",
    "        return []\n",
    "\n",
    "    def update_congestion_zones(self):\n",
    "        if self.congestion_timer > 0:\n",
    "            self.congestion_timer -= 1\n",
    "            return\n",
    "        num_zones = 6  # increased number of congestion zones\n",
    "        possible_zones = list(set(self.best_path_agent0 + self.best_path_agent1))\n",
    "        if possible_zones:\n",
    "            self.congestion_zones = random.sample(possible_zones, min(num_zones, len(possible_zones)))\n",
    "        else:\n",
    "            self.congestion_zones = []\n",
    "        self.congestion_timer = self.congestion_duration\n",
    "\n",
    "    def get_obstacle_view(self, agent_position):\n",
    "        view = []\n",
    "        ax, ay = agent_position\n",
    "        for dy in range(-self.vision_radius, self.vision_radius + 1):\n",
    "            for dx in range(-self.vision_radius, self.vision_radius + 1):\n",
    "                cx, cy = ax + dx, ay + dy\n",
    "                if 0 <= cx < self.grid_size and 0 <= cy < self.grid_size:\n",
    "                    view.append(1.0 if (cx, cy) in self.obstacles else 0.0)\n",
    "                else:\n",
    "                    view.append(0.0)\n",
    "        return view\n",
    "\n",
    "    def get_congestion_view(self, agent_position):\n",
    "        view = []\n",
    "        ax, ay = agent_position\n",
    "        for dy in range(-self.vision_radius, self.vision_radius + 1):\n",
    "            for dx in range(-self.vision_radius, self.vision_radius + 1):\n",
    "                cx, cy = ax + dx, ay + dy\n",
    "                if 0 <= cx < self.grid_size and 0 <= cy < self.grid_size:\n",
    "                    view.append(1.0 if (cx, cy) in self.congestion_zones else 0.0)\n",
    "                else:\n",
    "                    view.append(0.0)\n",
    "        return view\n",
    "\n",
    "    def reset(self):\n",
    "        self.agents = [(0, 0), (0, 0)]\n",
    "        return self.get_states()\n",
    "\n",
    "    def get_states(self):\n",
    "        states = []\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            goal = self.goals[idx]\n",
    "            base_state = [pos[0], pos[1], goal[0], goal[1]]\n",
    "            obstacle_view = self.get_obstacle_view(pos)\n",
    "            congestion_view = self.get_congestion_view(pos)\n",
    "            states.append(base_state + obstacle_view + congestion_view)\n",
    "        return states\n",
    "\n",
    "    def step(self, actions):\n",
    "        self.update_congestion_zones()\n",
    "        reward = 0\n",
    "        reward += self.movement_penalty * len(self.agents)\n",
    "        \n",
    "        old_distances = []\n",
    "        for idx, (x, y) in enumerate(self.agents):\n",
    "            goal = self.goals[idx]\n",
    "            old_distances.append(abs(x - goal[0]) + abs(y - goal[1]))\n",
    "        \n",
    "        new_positions = []\n",
    "        for idx, (x, y) in enumerate(self.agents):\n",
    "            dx, dy = 0, 0\n",
    "            action = actions[idx]\n",
    "            if action == 0:\n",
    "                dy = -1\n",
    "            elif action == 1:\n",
    "                dy = 1\n",
    "            elif action == 2:\n",
    "                dx = -1\n",
    "            elif action == 3:\n",
    "                dx = 1\n",
    "            new_x, new_y = x + dx, y + dy\n",
    "            if new_x < 0 or new_x >= self.grid_size or new_y < 0 or new_y >= self.grid_size:\n",
    "                new_x, new_y = x, y\n",
    "            if (new_x, new_y) in self.obstacles:\n",
    "                reward += self.collision_penalty\n",
    "                new_x, new_y = x, y\n",
    "            new_positions.append((new_x, new_y))\n",
    "        self.agents = new_positions\n",
    "        \n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            if pos in self.congestion_zones:\n",
    "                reward += self.congestion_penalty\n",
    "        \n",
    "        for idx, (x, y) in enumerate(self.agents):\n",
    "            goal = self.goals[idx]\n",
    "            new_distance = abs(x - goal[0]) + abs(y - goal[1])\n",
    "            reward += self.shaping_factor * (old_distances[idx] - new_distance)\n",
    "        \n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            best_path = self.best_path_agent0 if idx == 0 else self.best_path_agent1\n",
    "            if pos in best_path:\n",
    "                reward += self.best_path_bonus\n",
    "            else:\n",
    "                congestion_view = self.get_congestion_view(pos)\n",
    "                if not any(cell == 1.0 for cell in congestion_view):\n",
    "                    reward += self.deviation_penalty\n",
    "        \n",
    "        manhattan_distance = abs(self.agents[0][0] - self.agents[1][0]) + abs(self.agents[0][1] - self.agents[1][1])\n",
    "        if manhattan_distance < 3:\n",
    "            reward += self.closeness_penalty\n",
    "        \n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            if pos == self.goals[idx]:\n",
    "                reward += self.goal_reward\n",
    "        \n",
    "        done = (self.agents[0] == self.goals[0] and self.agents[1] == self.goals[1])\n",
    "        next_states = self.get_states()\n",
    "        return next_states, reward, done\n",
    "\n",
    "    def render(self, screen):\n",
    "        screen.fill((255, 255, 255))\n",
    "        for x in range(0, WINDOW_SIZE, self.cell_size):\n",
    "            pygame.draw.line(screen, (200, 200, 200), (x, 0), (x, WINDOW_SIZE))\n",
    "        for y in range(0, WINDOW_SIZE, self.cell_size):\n",
    "            pygame.draw.line(screen, (200, 200, 200), (0, y), (WINDOW_SIZE, y))\n",
    "        for obs in self.obstacles:\n",
    "            rect = pygame.Rect(obs[0]*self.cell_size, obs[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (0, 0, 0), rect)\n",
    "        for cz in self.congestion_zones:\n",
    "            rect = pygame.Rect(cz[0]*self.cell_size, cz[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (255, 165, 0), rect)\n",
    "        # Draw computed best paths\n",
    "        if self.best_path_agent0 and len(self.best_path_agent0) > 1:\n",
    "            points0 = [(x * self.cell_size + self.cell_size//2, y * self.cell_size + self.cell_size//2)\n",
    "                       for (x, y) in self.best_path_agent0]\n",
    "            pygame.draw.lines(screen, (255, 0, 255), False, points0, 3)\n",
    "        if self.best_path_agent1 and len(self.best_path_agent1) > 1:\n",
    "            points1 = [(x * self.cell_size + self.cell_size//2, y * self.cell_size + self.cell_size//2)\n",
    "                       for (x, y) in self.best_path_agent1]\n",
    "            pygame.draw.lines(screen, (0, 255, 255), False, points1, 3)\n",
    "        # Draw goals with colors matching the drones:\n",
    "        goal_colors = [(255, 0, 0), (0, 0, 255)]\n",
    "        for idx, goal in enumerate(self.goals):\n",
    "            rect = pygame.Rect(goal[0]*self.cell_size, goal[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, goal_colors[idx], rect)\n",
    "        # Draw agents as circles (agent 0 in red, agent 1 in blue)\n",
    "        colors = [(255, 0, 0), (0, 0, 255)]\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            center = (pos[0]*self.cell_size + self.cell_size//2,\n",
    "                      pos[1]*self.cell_size + self.cell_size//2)\n",
    "            pygame.draw.circle(screen, colors[idx], center, self.cell_size//3)\n",
    "        pygame.display.flip()\n",
    "\n",
    "# ----------------------------\n",
    "# Replay Buffer for DQN\n",
    "# ----------------------------\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# ----------------------------\n",
    "# DQN Network and Agent (GPU-enabled)\n",
    "# ----------------------------\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_dim, output_dim, lr=1e-3, gamma=0.99,\n",
    "                 buffer_capacity=10000, batch_size=32):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def select_action(self, state, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(4)\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "        return q_values.argmax().item()\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.replay_buffer.sample(self.batch_size)\n",
    "        batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*transitions)\n",
    "        \n",
    "        batch_state = torch.FloatTensor(batch_state).to(self.device)\n",
    "        batch_action = torch.LongTensor(batch_action).unsqueeze(1).to(self.device)\n",
    "        batch_reward = torch.FloatTensor(batch_reward).unsqueeze(1).to(self.device)\n",
    "        batch_next_state = torch.FloatTensor(batch_next_state).to(self.device)\n",
    "        batch_done = torch.FloatTensor(batch_done).unsqueeze(1).to(self.device)\n",
    "        \n",
    "        current_q = self.policy_net(batch_state).gather(1, batch_action)\n",
    "        next_q = self.target_net(batch_next_state).max(1)[0].unsqueeze(1)\n",
    "        target_q = batch_reward + self.gamma * next_q * (1 - batch_done)\n",
    "        \n",
    "        loss = nn.MSELoss()(current_q, target_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "# ----------------------------\n",
    "# Training Loop with Milestone Rendering and Video Capture\n",
    "# ----------------------------\n",
    "def train():\n",
    "    num_episodes = 1000\n",
    "    max_steps = 500\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.05\n",
    "    epsilon_decay = 0.9967\n",
    "    target_update_freq = 10\n",
    "    milestones = {int(num_episodes * 0.5), int(num_episodes * 0.75), num_episodes}\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    env = Gridworld()\n",
    "    state_dim = 4 + 2 * ((2 * env.vision_radius + 1) ** 2)\n",
    "    action_dim = 4\n",
    "    agents = [DQNAgent(state_dim, action_dim), DQNAgent(state_dim, action_dim)]\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        states = env.reset()\n",
    "        total_reward = 0\n",
    "        do_render = (episode + 1) in milestones\n",
    "        video_writer = None\n",
    "        if do_render:\n",
    "            pygame.init()\n",
    "            screen = pygame.display.set_mode((WINDOW_SIZE, WINDOW_SIZE))\n",
    "            pygame.display.set_caption(f\"Training Episode {episode + 1}\")\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            video_filename = f\"episode_{episode+1}.mp4\"\n",
    "            fps = 20\n",
    "            video_writer = cv2.VideoWriter(video_filename, fourcc, fps, (WINDOW_SIZE, WINDOW_SIZE))\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            actions = []\n",
    "            for i in range(2):\n",
    "                action = agents[i].select_action(states[i], epsilon)\n",
    "                actions.append(action)\n",
    "            next_states, reward, done = env.step(actions)\n",
    "            total_reward += reward\n",
    "            \n",
    "            for i in range(2):\n",
    "                agents[i].replay_buffer.push(states[i], actions[i], reward, next_states[i], done)\n",
    "            \n",
    "            states = next_states\n",
    "            \n",
    "            for i in range(2):\n",
    "                agents[i].update()\n",
    "            \n",
    "            if do_render:\n",
    "                env.render(screen)\n",
    "                frame = pygame.surfarray.array3d(screen)\n",
    "                frame = np.transpose(frame, (1, 0, 2))\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                video_writer.write(frame)\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.QUIT:\n",
    "                        pygame.quit()\n",
    "                        if video_writer is not None:\n",
    "                            video_writer.release()\n",
    "                        return\n",
    "                pygame.time.wait(50)\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        if do_render:\n",
    "            pygame.time.wait(1000)\n",
    "            pygame.quit()\n",
    "            if video_writer is not None:\n",
    "                video_writer.release()\n",
    "        \n",
    "        if (episode + 1) % target_update_freq == 0:\n",
    "            for agent in agents:\n",
    "                agent.update_target()\n",
    "        \n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        print(f\"Episode {episode + 1}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "    # Save the trained model for each agent after all episodes have completed.\n",
    "    for i, agent in enumerate(agents):\n",
    "        torch.save(agent.policy_net.state_dict(), f\"agent_{i}_trained.pth\")\n",
    "    print(\"Training complete. Models saved.\")\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
