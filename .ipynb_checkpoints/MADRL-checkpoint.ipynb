{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "472bdfad-7839-4809-905d-f169d591ae22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent positions: [(1, 0), (1, 0)], Reward: -10\n",
      "Agent positions: [(1, 0), (2, 0)], Reward: -10\n",
      "Agent positions: [(1, 1), (2, 0)], Reward: -10\n",
      "Agent positions: [(0, 1), (2, 0)], Reward: 0\n",
      "Agent positions: [(1, 1), (3, 0)], Reward: 0\n",
      "Agent positions: [(1, 0), (2, 0)], Reward: -10\n",
      "Agent positions: [(0, 0), (1, 0)], Reward: -10\n",
      "Agent positions: [(0, 0), (1, 1)], Reward: -10\n",
      "Agent positions: [(0, 0), (1, 1)], Reward: -10\n",
      "Agent positions: [(0, 1), (1, 1)], Reward: -10\n",
      "Agent positions: [(0, 1), (1, 1)], Reward: -10\n",
      "Agent positions: [(1, 1), (1, 0)], Reward: -10\n",
      "Agent positions: [(1, 2), (0, 0)], Reward: 0\n",
      "Agent positions: [(0, 2), (0, 0)], Reward: 0\n",
      "Agent positions: [(0, 2), (0, 1)], Reward: -10\n",
      "Agent positions: [(0, 1), (0, 0)], Reward: -10\n",
      "Agent positions: [(0, 2), (0, 1)], Reward: -10\n",
      "Agent positions: [(0, 2), (1, 1)], Reward: -10\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Grid and cell configuration\n",
    "GRID_SIZE = 10\n",
    "CELL_SIZE = 50  # pixels per grid cell\n",
    "WINDOW_SIZE = GRID_SIZE * CELL_SIZE\n",
    "NUM_OBSTACLES = 10\n",
    "\n",
    "# Actions available\n",
    "ACTIONS = [\"up\", \"down\", \"left\", \"right\"]\n",
    "\n",
    "class Gridworld:\n",
    "    def __init__(self):\n",
    "        self.grid_size = GRID_SIZE\n",
    "        self.cell_size = CELL_SIZE\n",
    "        # Define goals: agent 0's goal is bottom-right, agent 1's goal is bottom-left\n",
    "        self.goals = [(GRID_SIZE - 1, GRID_SIZE - 1), (0, GRID_SIZE - 1)]\n",
    "        self.obstacles = self.generate_obstacles()\n",
    "        # Both agents start at top-left (0,0)\n",
    "        self.agents = [(0, 0), (0, 0)]\n",
    "        # Penalty for agents being too close\n",
    "        self.proximity_penalty = -10\n",
    "        \n",
    "    def generate_obstacles(self):\n",
    "        obstacles = set()\n",
    "        # Generate random fixed obstacles while avoiding start and goal positions\n",
    "        while len(obstacles) < NUM_OBSTACLES:\n",
    "            x = random.randint(0, self.grid_size - 1)\n",
    "            y = random.randint(0, self.grid_size - 1)\n",
    "            if (x, y) != (0, 0) and (x, y) not in self.goals:\n",
    "                obstacles.add((x, y))\n",
    "        return list(obstacles)\n",
    "        \n",
    "    def reset(self):\n",
    "        # Reset agents to the starting position; obstacles remain fixed\n",
    "        self.agents = [(0, 0), (0, 0)]\n",
    "    \n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Takes a list of actions (one per agent) and updates their positions.\n",
    "        Actions can be one of \"up\", \"down\", \"left\", \"right\".\n",
    "        Returns the new positions and a reward based on proximity and reaching goals.\n",
    "        \"\"\"\n",
    "        new_positions = []\n",
    "        for idx, (x, y) in enumerate(self.agents):\n",
    "            dx, dy = 0, 0\n",
    "            action = actions[idx]\n",
    "            if action == \"up\":\n",
    "                dy = -1\n",
    "            elif action == \"down\":\n",
    "                dy = 1\n",
    "            elif action == \"left\":\n",
    "                dx = -1\n",
    "            elif action == \"right\":\n",
    "                dx = 1\n",
    "            new_x, new_y = x + dx, y + dy\n",
    "            # Check boundaries\n",
    "            if new_x < 0 or new_x >= self.grid_size or new_y < 0 or new_y >= self.grid_size:\n",
    "                new_x, new_y = x, y  # invalid move, stay in place\n",
    "            # Check obstacles; cannot move into an obstacle\n",
    "            if (new_x, new_y) in self.obstacles:\n",
    "                new_x, new_y = x, y\n",
    "            new_positions.append((new_x, new_y))\n",
    "        self.agents = new_positions\n",
    "        \n",
    "        # Compute reward\n",
    "        reward = 0\n",
    "        # Penalty if agents are too close (Euclidean distance < 2)\n",
    "        dist = math.sqrt((self.agents[0][0] - self.agents[1][0])**2 +\n",
    "                         (self.agents[0][1] - self.agents[1][1])**2)\n",
    "        if dist < 2:\n",
    "            reward += self.proximity_penalty\n",
    "        # Reward for reaching the designated goal\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            if pos == self.goals[idx]:\n",
    "                reward += 10  # arbitrary reward for goal achievement\n",
    "        return self.agents, reward\n",
    "    \n",
    "    def render(self, screen):\n",
    "        # Clear screen\n",
    "        screen.fill((255, 255, 255))\n",
    "        \n",
    "        # Draw grid lines\n",
    "        for x in range(0, WINDOW_SIZE, self.cell_size):\n",
    "            pygame.draw.line(screen, (200, 200, 200), (x, 0), (x, WINDOW_SIZE))\n",
    "        for y in range(0, WINDOW_SIZE, self.cell_size):\n",
    "            pygame.draw.line(screen, (200, 200, 200), (0, y), (WINDOW_SIZE, y))\n",
    "        \n",
    "        # Draw obstacles as black rectangles\n",
    "        for obs in self.obstacles:\n",
    "            rect = pygame.Rect(obs[0]*self.cell_size, obs[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (0, 0, 0), rect)\n",
    "        \n",
    "        # Draw goals as green rectangles\n",
    "        for goal in self.goals:\n",
    "            rect = pygame.Rect(goal[0]*self.cell_size, goal[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (0, 255, 0), rect)\n",
    "        \n",
    "        # Draw agents as circles (agent 0 in red, agent 1 in blue)\n",
    "        colors = [(255, 0, 0), (0, 0, 255)]\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            center = (pos[0]*self.cell_size + self.cell_size//2, pos[1]*self.cell_size + self.cell_size//2)\n",
    "            pygame.draw.circle(screen, colors[idx], center, self.cell_size//3)\n",
    "        \n",
    "        pygame.display.flip()\n",
    "\n",
    "def main():\n",
    "    pygame.init()\n",
    "    screen = pygame.display.set_mode((WINDOW_SIZE, WINDOW_SIZE))\n",
    "    pygame.display.set_caption(\"Gridworld RL Framework\")\n",
    "    \n",
    "    gridworld = Gridworld()\n",
    "    clock = pygame.time.Clock()\n",
    "    running = True\n",
    "    \n",
    "    while running:\n",
    "        # Event handling: quit if the window is closed\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "        \n",
    "        # For demonstration, we take random actions for both agents\n",
    "        actions = [random.choice(ACTIONS) for _ in range(2)]\n",
    "        agents, reward = gridworld.step(actions)\n",
    "        print(f\"Agent positions: {agents}, Reward: {reward}\")\n",
    "        \n",
    "        gridworld.render(screen)\n",
    "        clock.tick(15)  # Run at 2 frames per second for visualization purposes\n",
    "        \n",
    "    pygame.quit()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a77cf3a9-ef77-4d71-bb40-69969a67c59f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -150.00, Epsilon: 0.995\n",
      "Episode 1, Total Reward: -200.00, Epsilon: 0.990\n",
      "Episode 2, Total Reward: -140.00, Epsilon: 0.985\n",
      "Episode 3, Total Reward: -160.00, Epsilon: 0.980\n",
      "Episode 4, Total Reward: -20.00, Epsilon: 0.975\n",
      "Episode 5, Total Reward: -260.00, Epsilon: 0.970\n",
      "Episode 6, Total Reward: -160.00, Epsilon: 0.966\n",
      "Episode 7, Total Reward: -140.00, Epsilon: 0.961\n",
      "Episode 8, Total Reward: -20.00, Epsilon: 0.956\n",
      "Episode 9, Total Reward: -190.00, Epsilon: 0.951\n",
      "Episode 10, Total Reward: -100.00, Epsilon: 0.946\n",
      "Episode 11, Total Reward: -270.00, Epsilon: 0.942\n",
      "Episode 12, Total Reward: -170.00, Epsilon: 0.937\n",
      "Episode 13, Total Reward: -130.00, Epsilon: 0.932\n",
      "Episode 14, Total Reward: -30.00, Epsilon: 0.928\n",
      "Episode 15, Total Reward: -90.00, Epsilon: 0.923\n",
      "Episode 16, Total Reward: -100.00, Epsilon: 0.918\n",
      "Episode 17, Total Reward: -120.00, Epsilon: 0.914\n",
      "Episode 18, Total Reward: -200.00, Epsilon: 0.909\n",
      "Episode 19, Total Reward: -70.00, Epsilon: 0.905\n",
      "Episode 20, Total Reward: -70.00, Epsilon: 0.900\n",
      "Episode 21, Total Reward: -50.00, Epsilon: 0.896\n",
      "Episode 22, Total Reward: -40.00, Epsilon: 0.891\n",
      "Episode 23, Total Reward: -150.00, Epsilon: 0.887\n",
      "Episode 24, Total Reward: -170.00, Epsilon: 0.882\n",
      "Episode 25, Total Reward: -240.00, Epsilon: 0.878\n",
      "Episode 26, Total Reward: -120.00, Epsilon: 0.873\n",
      "Episode 27, Total Reward: -250.00, Epsilon: 0.869\n",
      "Episode 28, Total Reward: -70.00, Epsilon: 0.865\n",
      "Episode 29, Total Reward: -190.00, Epsilon: 0.860\n",
      "Episode 30, Total Reward: -20.00, Epsilon: 0.856\n",
      "Episode 31, Total Reward: -180.00, Epsilon: 0.852\n",
      "Episode 32, Total Reward: -80.00, Epsilon: 0.848\n",
      "Episode 33, Total Reward: -240.00, Epsilon: 0.843\n",
      "Episode 34, Total Reward: -110.00, Epsilon: 0.839\n",
      "Episode 35, Total Reward: -20.00, Epsilon: 0.835\n",
      "Episode 36, Total Reward: -160.00, Epsilon: 0.831\n",
      "Episode 37, Total Reward: -160.00, Epsilon: 0.827\n",
      "Episode 38, Total Reward: -90.00, Epsilon: 0.822\n",
      "Episode 39, Total Reward: -180.00, Epsilon: 0.818\n",
      "Episode 40, Total Reward: -110.00, Epsilon: 0.814\n",
      "Episode 41, Total Reward: -220.00, Epsilon: 0.810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pygame\n",
    "\n",
    "# Constants for gridworld\n",
    "GRID_SIZE = 10\n",
    "CELL_SIZE = 50  # pixels per cell\n",
    "WINDOW_SIZE = GRID_SIZE * CELL_SIZE\n",
    "NUM_OBSTACLES = 10\n",
    "\n",
    "# ----------------------------\n",
    "# Environment (Gridworld)\n",
    "# ----------------------------\n",
    "class Gridworld:\n",
    "    def __init__(self):\n",
    "        self.grid_size = GRID_SIZE\n",
    "        self.cell_size = CELL_SIZE\n",
    "        # Define goals: agent 0's goal is bottom-right, agent 1's goal is bottom-left\n",
    "        self.goals = [(GRID_SIZE - 1, GRID_SIZE - 1), (0, GRID_SIZE - 1)]\n",
    "        self.obstacles = self.generate_obstacles()\n",
    "        # Both agents start at top-left (0,0)\n",
    "        self.agents = [(0, 0), (0, 0)]\n",
    "        # Penalty if agents are too close\n",
    "        self.proximity_penalty = -10\n",
    "\n",
    "    def generate_obstacles(self):\n",
    "        obstacles = set()\n",
    "        # Generate random fixed obstacles while avoiding start and goal positions\n",
    "        while len(obstacles) < NUM_OBSTACLES:\n",
    "            x = random.randint(0, self.grid_size - 1)\n",
    "            y = random.randint(0, self.grid_size - 1)\n",
    "            if (x, y) != (0, 0) and (x, y) not in self.goals:\n",
    "                obstacles.add((x, y))\n",
    "        return list(obstacles)\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset agents to starting positions; obstacles remain fixed.\n",
    "        self.agents = [(0, 0), (0, 0)]\n",
    "        return self.get_states()\n",
    "\n",
    "    def get_states(self):\n",
    "        # For each agent, state = [agent_x, agent_y, goal_x, goal_y]\n",
    "        states = []\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            goal = self.goals[idx]\n",
    "            states.append([pos[0], pos[1], goal[0], goal[1]])\n",
    "        return states\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Takes a list of actions (one per agent, where:\n",
    "           0: up, 1: down, 2: left, 3: right)\n",
    "        and updates their positions.\n",
    "        Returns:\n",
    "           next_states: list of states for each agent\n",
    "           reward: a scalar reward computed from proximity and goal achievements\n",
    "           done: True if both agents have reached their goals\n",
    "        \"\"\"\n",
    "        new_positions = []\n",
    "        for idx, (x, y) in enumerate(self.agents):\n",
    "            dx, dy = 0, 0\n",
    "            action = actions[idx]\n",
    "            if action == 0:  # up\n",
    "                dy = -1\n",
    "            elif action == 1:  # down\n",
    "                dy = 1\n",
    "            elif action == 2:  # left\n",
    "                dx = -1\n",
    "            elif action == 3:  # right\n",
    "                dx = 1\n",
    "            new_x, new_y = x + dx, y + dy\n",
    "            # Check boundaries\n",
    "            if new_x < 0 or new_x >= self.grid_size or new_y < 0 or new_y >= self.grid_size:\n",
    "                new_x, new_y = x, y  # invalid move, stay in place\n",
    "            # Check obstacles; if new cell is an obstacle, agent remains in place\n",
    "            if (new_x, new_y) in self.obstacles:\n",
    "                new_x, new_y = x, y\n",
    "            new_positions.append((new_x, new_y))\n",
    "        self.agents = new_positions\n",
    "\n",
    "        # Compute reward and check termination\n",
    "        reward = 0\n",
    "        done = False\n",
    "        # Apply proximity penalty if agents are too close (Euclidean distance < 2)\n",
    "        dist = math.sqrt((self.agents[0][0] - self.agents[1][0])**2 +\n",
    "                         (self.agents[0][1] - self.agents[1][1])**2)\n",
    "        if dist < 2:\n",
    "            reward += self.proximity_penalty\n",
    "        # Reward for each agent reaching its goal\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            if pos == self.goals[idx]:\n",
    "                reward += 10  # arbitrary reward for goal achievement\n",
    "\n",
    "        # Episode ends if both agents have reached their goals\n",
    "        if self.agents[0] == self.goals[0] and self.agents[1] == self.goals[1]:\n",
    "            done = True\n",
    "\n",
    "        next_states = self.get_states()\n",
    "        return next_states, reward, done\n",
    "\n",
    "    def render(self, screen):\n",
    "        # Clear screen\n",
    "        screen.fill((255, 255, 255))\n",
    "        # Draw grid lines\n",
    "        for x in range(0, WINDOW_SIZE, self.cell_size):\n",
    "            pygame.draw.line(screen, (200, 200, 200), (x, 0), (x, WINDOW_SIZE))\n",
    "        for y in range(0, WINDOW_SIZE, self.cell_size):\n",
    "            pygame.draw.line(screen, (200, 200, 200), (0, y), (WINDOW_SIZE, y))\n",
    "        # Draw obstacles (black)\n",
    "        for obs in self.obstacles:\n",
    "            rect = pygame.Rect(obs[0]*self.cell_size, obs[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (0, 0, 0), rect)\n",
    "        # Draw goals (green)\n",
    "        for goal in self.goals:\n",
    "            rect = pygame.Rect(goal[0]*self.cell_size, goal[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (0, 255, 0), rect)\n",
    "        # Draw agents (agent 0 in red, agent 1 in blue)\n",
    "        colors = [(255, 0, 0), (0, 0, 255)]\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            center = (pos[0]*self.cell_size + self.cell_size//2,\n",
    "                      pos[1]*self.cell_size + self.cell_size//2)\n",
    "            pygame.draw.circle(screen, colors[idx], center, self.cell_size//3)\n",
    "        pygame.display.flip()\n",
    "\n",
    "# ----------------------------\n",
    "# Replay Buffer for DQN\n",
    "# ----------------------------\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# ----------------------------\n",
    "# DQN Network and Agent\n",
    "# ----------------------------\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_dim, output_dim, lr=1e-3, gamma=0.99,\n",
    "                 buffer_capacity=10000, batch_size=32):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def select_action(self, state, epsilon):\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(4)  # 4 discrete actions\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "        return q_values.argmax().item()\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.replay_buffer.sample(self.batch_size)\n",
    "        batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*transitions)\n",
    "        \n",
    "        batch_state = torch.FloatTensor(batch_state).to(self.device)\n",
    "        batch_action = torch.LongTensor(batch_action).unsqueeze(1).to(self.device)\n",
    "        batch_reward = torch.FloatTensor(batch_reward).unsqueeze(1).to(self.device)\n",
    "        batch_next_state = torch.FloatTensor(batch_next_state).to(self.device)\n",
    "        batch_done = torch.FloatTensor(batch_done).unsqueeze(1).to(self.device)\n",
    "        \n",
    "        current_q = self.policy_net(batch_state).gather(1, batch_action)\n",
    "        next_q = self.target_net(batch_next_state).max(1)[0].unsqueeze(1)\n",
    "        target_q = batch_reward + self.gamma * next_q * (1 - batch_done)\n",
    "        \n",
    "        loss = nn.MSELoss()(current_q, target_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "# ----------------------------\n",
    "# Training Loop\n",
    "# ----------------------------\n",
    "def train():\n",
    "    num_episodes = 500\n",
    "    max_steps = 50\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.1\n",
    "    epsilon_decay = 0.995\n",
    "    target_update_freq = 10\n",
    "    \n",
    "    env = Gridworld()\n",
    "    \n",
    "    # Each agent's state: [agent_x, agent_y, goal_x, goal_y] -> 4 dimensions.\n",
    "    state_dim = 4\n",
    "    action_dim = 4  # up, down, left, right\n",
    "    agents = [DQNAgent(state_dim, action_dim), DQNAgent(state_dim, action_dim)]\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        states = env.reset()\n",
    "        total_reward = 0\n",
    "        for step in range(max_steps):\n",
    "            actions = []\n",
    "            # Each agent selects an action using its own DQN policy\n",
    "            for i in range(2):\n",
    "                action = agents[i].select_action(states[i], epsilon)\n",
    "                actions.append(action)\n",
    "            next_states, reward, done = env.step(actions)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Store experiences for each agent; here we use the same reward for both\n",
    "            for i in range(2):\n",
    "                agents[i].replay_buffer.push(states[i], actions[i], reward, next_states[i], done)\n",
    "            \n",
    "            states = next_states\n",
    "            \n",
    "            # Update each agent's policy network from its replay buffer\n",
    "            for i in range(2):\n",
    "                agents[i].update()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Update target networks periodically\n",
    "        if episode % target_update_freq == 0:\n",
    "            for agent in agents:\n",
    "                agent.update_target()\n",
    "        \n",
    "        # Epsilon decay\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.3f}\")\n",
    "        \n",
    "        # Optional: To visualize training, you can uncomment and run the render code below.\n",
    "        # pygame.init()\n",
    "        # screen = pygame.display.set_mode((WINDOW_SIZE, WINDOW_SIZE))\n",
    "        # env.render(screen)\n",
    "        # pygame.time.wait(200)\n",
    "        # pygame.quit()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e0c098f-4d78-4dc0-8fe8-863c497e6577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Episode 1, Total Reward: -100.00, Epsilon: 0.995\n",
      "Episode 2, Total Reward: -110.00, Epsilon: 0.990\n",
      "Episode 3, Total Reward: -70.00, Epsilon: 0.985\n",
      "Episode 4, Total Reward: -80.00, Epsilon: 0.980\n",
      "Episode 5, Total Reward: -220.00, Epsilon: 0.975\n",
      "Episode 6, Total Reward: -90.00, Epsilon: 0.970\n",
      "Episode 7, Total Reward: -30.00, Epsilon: 0.966\n",
      "Episode 8, Total Reward: -70.00, Epsilon: 0.961\n",
      "Episode 9, Total Reward: -150.00, Epsilon: 0.956\n",
      "Episode 10, Total Reward: -100.00, Epsilon: 0.951\n",
      "Episode 11, Total Reward: -140.00, Epsilon: 0.946\n",
      "Episode 12, Total Reward: -150.00, Epsilon: 0.942\n",
      "Episode 13, Total Reward: -150.00, Epsilon: 0.937\n",
      "Episode 14, Total Reward: -120.00, Epsilon: 0.932\n",
      "Episode 15, Total Reward: -150.00, Epsilon: 0.928\n",
      "Episode 16, Total Reward: -280.00, Epsilon: 0.923\n",
      "Episode 17, Total Reward: -130.00, Epsilon: 0.918\n",
      "Episode 18, Total Reward: -40.00, Epsilon: 0.914\n",
      "Episode 19, Total Reward: -190.00, Epsilon: 0.909\n",
      "Episode 20, Total Reward: -110.00, Epsilon: 0.905\n",
      "Episode 21, Total Reward: -70.00, Epsilon: 0.900\n",
      "Episode 22, Total Reward: -50.00, Epsilon: 0.896\n",
      "Episode 23, Total Reward: -170.00, Epsilon: 0.891\n",
      "Episode 24, Total Reward: -110.00, Epsilon: 0.887\n",
      "Episode 25, Total Reward: -50.00, Epsilon: 0.882\n",
      "Episode 26, Total Reward: -50.00, Epsilon: 0.878\n",
      "Episode 27, Total Reward: -80.00, Epsilon: 0.873\n",
      "Episode 28, Total Reward: -30.00, Epsilon: 0.869\n",
      "Episode 29, Total Reward: -200.00, Epsilon: 0.865\n",
      "Episode 30, Total Reward: -40.00, Epsilon: 0.860\n",
      "Episode 31, Total Reward: -150.00, Epsilon: 0.856\n",
      "Episode 32, Total Reward: -40.00, Epsilon: 0.852\n",
      "Episode 33, Total Reward: -60.00, Epsilon: 0.848\n",
      "Episode 34, Total Reward: -190.00, Epsilon: 0.843\n",
      "Episode 35, Total Reward: -50.00, Epsilon: 0.839\n",
      "Episode 36, Total Reward: -70.00, Epsilon: 0.835\n",
      "Episode 37, Total Reward: -70.00, Epsilon: 0.831\n",
      "Episode 38, Total Reward: -170.00, Epsilon: 0.827\n",
      "Episode 39, Total Reward: -10.00, Epsilon: 0.822\n",
      "Episode 40, Total Reward: -30.00, Epsilon: 0.818\n",
      "Episode 41, Total Reward: 50.00, Epsilon: 0.814\n",
      "Episode 42, Total Reward: -30.00, Epsilon: 0.810\n",
      "Episode 43, Total Reward: -90.00, Epsilon: 0.806\n",
      "Episode 44, Total Reward: -60.00, Epsilon: 0.802\n",
      "Episode 45, Total Reward: -60.00, Epsilon: 0.798\n",
      "Episode 46, Total Reward: -80.00, Epsilon: 0.794\n",
      "Episode 47, Total Reward: -60.00, Epsilon: 0.790\n",
      "Episode 48, Total Reward: -50.00, Epsilon: 0.786\n",
      "Episode 49, Total Reward: -10.00, Epsilon: 0.782\n",
      "Episode 50, Total Reward: -260.00, Epsilon: 0.778\n",
      "Episode 51, Total Reward: -10.00, Epsilon: 0.774\n",
      "Episode 52, Total Reward: -110.00, Epsilon: 0.771\n",
      "Episode 53, Total Reward: -340.00, Epsilon: 0.767\n",
      "Episode 54, Total Reward: -120.00, Epsilon: 0.763\n",
      "Episode 55, Total Reward: -100.00, Epsilon: 0.759\n",
      "Episode 56, Total Reward: -190.00, Epsilon: 0.755\n",
      "Episode 57, Total Reward: -150.00, Epsilon: 0.751\n",
      "Episode 58, Total Reward: -100.00, Epsilon: 0.748\n",
      "Episode 59, Total Reward: -30.00, Epsilon: 0.744\n",
      "Episode 60, Total Reward: -170.00, Epsilon: 0.740\n",
      "Episode 61, Total Reward: -20.00, Epsilon: 0.737\n",
      "Episode 62, Total Reward: -10.00, Epsilon: 0.733\n",
      "Episode 63, Total Reward: -70.00, Epsilon: 0.729\n",
      "Episode 64, Total Reward: -20.00, Epsilon: 0.726\n",
      "Episode 65, Total Reward: -130.00, Epsilon: 0.722\n",
      "Episode 66, Total Reward: -130.00, Epsilon: 0.718\n",
      "Episode 67, Total Reward: -20.00, Epsilon: 0.715\n",
      "Episode 68, Total Reward: -140.00, Epsilon: 0.711\n",
      "Episode 69, Total Reward: -30.00, Epsilon: 0.708\n",
      "Episode 70, Total Reward: -70.00, Epsilon: 0.704\n",
      "Episode 71, Total Reward: -60.00, Epsilon: 0.701\n",
      "Episode 72, Total Reward: -110.00, Epsilon: 0.697\n",
      "Episode 73, Total Reward: -70.00, Epsilon: 0.694\n",
      "Episode 74, Total Reward: -40.00, Epsilon: 0.690\n",
      "Episode 75, Total Reward: 0.00, Epsilon: 0.687\n",
      "Episode 76, Total Reward: -60.00, Epsilon: 0.683\n",
      "Episode 77, Total Reward: -70.00, Epsilon: 0.680\n",
      "Episode 78, Total Reward: -50.00, Epsilon: 0.676\n",
      "Episode 79, Total Reward: -20.00, Epsilon: 0.673\n",
      "Episode 80, Total Reward: -10.00, Epsilon: 0.670\n",
      "Episode 81, Total Reward: -20.00, Epsilon: 0.666\n",
      "Episode 82, Total Reward: -60.00, Epsilon: 0.663\n",
      "Episode 83, Total Reward: -20.00, Epsilon: 0.660\n",
      "Episode 84, Total Reward: -20.00, Epsilon: 0.656\n",
      "Episode 85, Total Reward: -20.00, Epsilon: 0.653\n",
      "Episode 86, Total Reward: -60.00, Epsilon: 0.650\n",
      "Episode 87, Total Reward: -40.00, Epsilon: 0.647\n",
      "Episode 88, Total Reward: -90.00, Epsilon: 0.643\n",
      "Episode 89, Total Reward: -10.00, Epsilon: 0.640\n",
      "Episode 90, Total Reward: -20.00, Epsilon: 0.637\n",
      "Episode 91, Total Reward: -90.00, Epsilon: 0.634\n",
      "Episode 92, Total Reward: -10.00, Epsilon: 0.631\n",
      "Episode 93, Total Reward: -50.00, Epsilon: 0.627\n",
      "Episode 94, Total Reward: -30.00, Epsilon: 0.624\n",
      "Episode 95, Total Reward: -70.00, Epsilon: 0.621\n",
      "Episode 96, Total Reward: -20.00, Epsilon: 0.618\n",
      "Episode 97, Total Reward: -60.00, Epsilon: 0.615\n",
      "Episode 98, Total Reward: -20.00, Epsilon: 0.612\n",
      "Episode 99, Total Reward: -10.00, Epsilon: 0.609\n",
      "Episode 100, Total Reward: -30.00, Epsilon: 0.606\n",
      "Episode 101, Total Reward: -50.00, Epsilon: 0.603\n",
      "Episode 102, Total Reward: -20.00, Epsilon: 0.600\n",
      "Episode 103, Total Reward: -40.00, Epsilon: 0.597\n",
      "Episode 104, Total Reward: -20.00, Epsilon: 0.594\n",
      "Episode 105, Total Reward: -40.00, Epsilon: 0.591\n",
      "Episode 106, Total Reward: -50.00, Epsilon: 0.588\n",
      "Episode 107, Total Reward: -60.00, Epsilon: 0.585\n",
      "Episode 108, Total Reward: -70.00, Epsilon: 0.582\n",
      "Episode 109, Total Reward: -10.00, Epsilon: 0.579\n",
      "Episode 110, Total Reward: -50.00, Epsilon: 0.576\n",
      "Episode 111, Total Reward: -30.00, Epsilon: 0.573\n",
      "Episode 112, Total Reward: -70.00, Epsilon: 0.570\n",
      "Episode 113, Total Reward: -40.00, Epsilon: 0.568\n",
      "Episode 114, Total Reward: -30.00, Epsilon: 0.565\n",
      "Episode 115, Total Reward: -10.00, Epsilon: 0.562\n",
      "Episode 116, Total Reward: -20.00, Epsilon: 0.559\n",
      "Episode 117, Total Reward: -40.00, Epsilon: 0.556\n",
      "Episode 118, Total Reward: -40.00, Epsilon: 0.554\n",
      "Episode 119, Total Reward: -50.00, Epsilon: 0.551\n",
      "Episode 120, Total Reward: -90.00, Epsilon: 0.548\n",
      "Episode 121, Total Reward: -10.00, Epsilon: 0.545\n",
      "Episode 122, Total Reward: -40.00, Epsilon: 0.543\n",
      "Episode 123, Total Reward: -110.00, Epsilon: 0.540\n",
      "Episode 124, Total Reward: -10.00, Epsilon: 0.537\n",
      "Episode 125, Total Reward: -30.00, Epsilon: 0.534\n",
      "Episode 126, Total Reward: -30.00, Epsilon: 0.532\n",
      "Episode 127, Total Reward: -20.00, Epsilon: 0.529\n",
      "Episode 128, Total Reward: -10.00, Epsilon: 0.526\n",
      "Episode 129, Total Reward: -20.00, Epsilon: 0.524\n",
      "Episode 130, Total Reward: -40.00, Epsilon: 0.521\n",
      "Episode 131, Total Reward: -10.00, Epsilon: 0.519\n",
      "Episode 132, Total Reward: -10.00, Epsilon: 0.516\n",
      "Episode 133, Total Reward: -40.00, Epsilon: 0.513\n",
      "Episode 134, Total Reward: -20.00, Epsilon: 0.511\n",
      "Episode 135, Total Reward: -30.00, Epsilon: 0.508\n",
      "Episode 136, Total Reward: -20.00, Epsilon: 0.506\n",
      "Episode 137, Total Reward: -10.00, Epsilon: 0.503\n",
      "Episode 138, Total Reward: -50.00, Epsilon: 0.501\n",
      "Episode 139, Total Reward: -10.00, Epsilon: 0.498\n",
      "Episode 140, Total Reward: -20.00, Epsilon: 0.496\n",
      "Episode 141, Total Reward: -10.00, Epsilon: 0.493\n",
      "Episode 142, Total Reward: -50.00, Epsilon: 0.491\n",
      "Episode 143, Total Reward: -10.00, Epsilon: 0.488\n",
      "Episode 144, Total Reward: 60.00, Epsilon: 0.486\n",
      "Episode 145, Total Reward: -10.00, Epsilon: 0.483\n",
      "Episode 146, Total Reward: -20.00, Epsilon: 0.481\n",
      "Episode 147, Total Reward: -30.00, Epsilon: 0.479\n",
      "Episode 148, Total Reward: -30.00, Epsilon: 0.476\n",
      "Episode 149, Total Reward: -10.00, Epsilon: 0.474\n",
      "Episode 150, Total Reward: -10.00, Epsilon: 0.471\n",
      "Episode 151, Total Reward: -20.00, Epsilon: 0.469\n",
      "Episode 152, Total Reward: -10.00, Epsilon: 0.467\n",
      "Episode 153, Total Reward: -10.00, Epsilon: 0.464\n",
      "Episode 154, Total Reward: -100.00, Epsilon: 0.462\n",
      "Episode 155, Total Reward: -70.00, Epsilon: 0.460\n",
      "Episode 156, Total Reward: -40.00, Epsilon: 0.458\n",
      "Episode 157, Total Reward: 30.00, Epsilon: 0.455\n",
      "Episode 158, Total Reward: -60.00, Epsilon: 0.453\n",
      "Episode 159, Total Reward: 50.00, Epsilon: 0.451\n",
      "Episode 160, Total Reward: -10.00, Epsilon: 0.448\n",
      "Episode 161, Total Reward: -20.00, Epsilon: 0.446\n",
      "Episode 162, Total Reward: 170.00, Epsilon: 0.444\n",
      "Episode 163, Total Reward: -30.00, Epsilon: 0.442\n",
      "Episode 164, Total Reward: -50.00, Epsilon: 0.440\n",
      "Episode 165, Total Reward: -10.00, Epsilon: 0.437\n",
      "Episode 166, Total Reward: -40.00, Epsilon: 0.435\n",
      "Episode 167, Total Reward: -30.00, Epsilon: 0.433\n",
      "Episode 168, Total Reward: -110.00, Epsilon: 0.431\n",
      "Episode 169, Total Reward: -10.00, Epsilon: 0.429\n",
      "Episode 170, Total Reward: -10.00, Epsilon: 0.427\n",
      "Episode 171, Total Reward: 40.00, Epsilon: 0.424\n",
      "Episode 172, Total Reward: -10.00, Epsilon: 0.422\n",
      "Episode 173, Total Reward: -30.00, Epsilon: 0.420\n",
      "Episode 174, Total Reward: -10.00, Epsilon: 0.418\n",
      "Episode 175, Total Reward: -20.00, Epsilon: 0.416\n",
      "Episode 176, Total Reward: -30.00, Epsilon: 0.414\n",
      "Episode 177, Total Reward: -10.00, Epsilon: 0.412\n",
      "Episode 178, Total Reward: -30.00, Epsilon: 0.410\n",
      "Episode 179, Total Reward: 80.00, Epsilon: 0.408\n",
      "Episode 180, Total Reward: -10.00, Epsilon: 0.406\n",
      "Episode 181, Total Reward: 150.00, Epsilon: 0.404\n",
      "Episode 182, Total Reward: -10.00, Epsilon: 0.402\n",
      "Episode 183, Total Reward: -10.00, Epsilon: 0.400\n",
      "Episode 184, Total Reward: -40.00, Epsilon: 0.398\n",
      "Episode 185, Total Reward: -50.00, Epsilon: 0.396\n",
      "Episode 186, Total Reward: 70.00, Epsilon: 0.394\n",
      "Episode 187, Total Reward: -10.00, Epsilon: 0.392\n",
      "Episode 188, Total Reward: -20.00, Epsilon: 0.390\n",
      "Episode 189, Total Reward: -10.00, Epsilon: 0.388\n",
      "Episode 190, Total Reward: 40.00, Epsilon: 0.386\n",
      "Episode 191, Total Reward: 60.00, Epsilon: 0.384\n",
      "Episode 192, Total Reward: -20.00, Epsilon: 0.382\n",
      "Episode 193, Total Reward: -10.00, Epsilon: 0.380\n",
      "Episode 194, Total Reward: -10.00, Epsilon: 0.378\n",
      "Episode 195, Total Reward: 150.00, Epsilon: 0.376\n",
      "Episode 196, Total Reward: 70.00, Epsilon: 0.374\n",
      "Episode 197, Total Reward: -40.00, Epsilon: 0.373\n",
      "Episode 198, Total Reward: -20.00, Epsilon: 0.371\n",
      "Episode 199, Total Reward: -10.00, Epsilon: 0.369\n",
      "Episode 200, Total Reward: -40.00, Epsilon: 0.367\n",
      "Episode 201, Total Reward: 30.00, Epsilon: 0.365\n",
      "Episode 202, Total Reward: -10.00, Epsilon: 0.363\n",
      "Episode 203, Total Reward: -10.00, Epsilon: 0.361\n",
      "Episode 204, Total Reward: 60.00, Epsilon: 0.360\n",
      "Episode 205, Total Reward: 30.00, Epsilon: 0.358\n",
      "Episode 206, Total Reward: 40.00, Epsilon: 0.356\n",
      "Episode 207, Total Reward: 230.00, Epsilon: 0.354\n",
      "Episode 208, Total Reward: -10.00, Epsilon: 0.353\n",
      "Episode 209, Total Reward: 180.00, Epsilon: 0.351\n",
      "Episode 210, Total Reward: -40.00, Epsilon: 0.349\n",
      "Episode 211, Total Reward: 60.00, Epsilon: 0.347\n",
      "Episode 212, Total Reward: 30.00, Epsilon: 0.346\n",
      "Episode 213, Total Reward: -20.00, Epsilon: 0.344\n",
      "Episode 214, Total Reward: -10.00, Epsilon: 0.342\n",
      "Episode 215, Total Reward: -10.00, Epsilon: 0.340\n",
      "Episode 216, Total Reward: 150.00, Epsilon: 0.339\n",
      "Episode 217, Total Reward: 20.00, Epsilon: 0.337\n",
      "Episode 218, Total Reward: 170.00, Epsilon: 0.335\n",
      "Episode 219, Total Reward: -20.00, Epsilon: 0.334\n",
      "Episode 220, Total Reward: 180.00, Epsilon: 0.332\n",
      "Episode 221, Total Reward: -10.00, Epsilon: 0.330\n",
      "Episode 222, Total Reward: 250.00, Epsilon: 0.329\n",
      "Episode 223, Total Reward: 120.00, Epsilon: 0.327\n",
      "Episode 224, Total Reward: 40.00, Epsilon: 0.325\n",
      "Episode 225, Total Reward: 190.00, Epsilon: 0.324\n",
      "Episode 226, Total Reward: -10.00, Epsilon: 0.322\n",
      "Episode 227, Total Reward: -10.00, Epsilon: 0.321\n",
      "Episode 228, Total Reward: -10.00, Epsilon: 0.319\n",
      "Episode 229, Total Reward: 80.00, Epsilon: 0.317\n",
      "Episode 230, Total Reward: -10.00, Epsilon: 0.316\n",
      "Episode 231, Total Reward: 10.00, Epsilon: 0.314\n",
      "Episode 232, Total Reward: 120.00, Epsilon: 0.313\n",
      "Episode 233, Total Reward: 50.00, Epsilon: 0.311\n",
      "Episode 234, Total Reward: 180.00, Epsilon: 0.309\n",
      "Episode 235, Total Reward: 60.00, Epsilon: 0.308\n",
      "Episode 236, Total Reward: 110.00, Epsilon: 0.306\n",
      "Episode 237, Total Reward: -10.00, Epsilon: 0.305\n",
      "Episode 238, Total Reward: -10.00, Epsilon: 0.303\n",
      "Episode 239, Total Reward: 220.00, Epsilon: 0.302\n",
      "Episode 240, Total Reward: 10.00, Epsilon: 0.300\n",
      "Episode 241, Total Reward: -10.00, Epsilon: 0.299\n",
      "Episode 242, Total Reward: 140.00, Epsilon: 0.297\n",
      "Episode 243, Total Reward: 90.00, Epsilon: 0.296\n",
      "Episode 244, Total Reward: 90.00, Epsilon: 0.294\n",
      "Episode 245, Total Reward: -10.00, Epsilon: 0.293\n",
      "Episode 246, Total Reward: 130.00, Epsilon: 0.291\n",
      "Episode 247, Total Reward: 50.00, Epsilon: 0.290\n",
      "Episode 248, Total Reward: 20.00, Epsilon: 0.288\n",
      "Episode 249, Total Reward: 220.00, Epsilon: 0.287\n",
      "Episode 250, Total Reward: -30.00, Epsilon: 0.286\n",
      "Episode 251, Total Reward: -10.00, Epsilon: 0.284\n",
      "Episode 252, Total Reward: 10.00, Epsilon: 0.283\n",
      "Episode 253, Total Reward: 60.00, Epsilon: 0.281\n",
      "Episode 254, Total Reward: 110.00, Epsilon: 0.280\n",
      "Episode 255, Total Reward: -10.00, Epsilon: 0.279\n",
      "Episode 256, Total Reward: -10.00, Epsilon: 0.277\n",
      "Episode 257, Total Reward: 170.00, Epsilon: 0.276\n",
      "Episode 258, Total Reward: -10.00, Epsilon: 0.274\n",
      "Episode 259, Total Reward: 200.00, Epsilon: 0.273\n",
      "Episode 260, Total Reward: -10.00, Epsilon: 0.272\n",
      "Episode 261, Total Reward: -10.00, Epsilon: 0.270\n",
      "Episode 262, Total Reward: -10.00, Epsilon: 0.269\n",
      "Episode 263, Total Reward: 170.00, Epsilon: 0.268\n",
      "Episode 264, Total Reward: -10.00, Epsilon: 0.266\n",
      "Episode 265, Total Reward: 10.00, Epsilon: 0.265\n",
      "Episode 266, Total Reward: 190.00, Epsilon: 0.264\n",
      "Episode 267, Total Reward: 180.00, Epsilon: 0.262\n",
      "Episode 268, Total Reward: -10.00, Epsilon: 0.261\n",
      "Episode 269, Total Reward: -10.00, Epsilon: 0.260\n",
      "Episode 270, Total Reward: 150.00, Epsilon: 0.258\n",
      "Episode 271, Total Reward: 70.00, Epsilon: 0.257\n",
      "Episode 272, Total Reward: 40.00, Epsilon: 0.256\n",
      "Episode 273, Total Reward: 200.00, Epsilon: 0.255\n",
      "Episode 274, Total Reward: 90.00, Epsilon: 0.253\n",
      "Episode 275, Total Reward: 160.00, Epsilon: 0.252\n",
      "Episode 276, Total Reward: -10.00, Epsilon: 0.251\n",
      "Episode 277, Total Reward: 170.00, Epsilon: 0.249\n",
      "Episode 278, Total Reward: -10.00, Epsilon: 0.248\n",
      "Episode 279, Total Reward: -10.00, Epsilon: 0.247\n",
      "Episode 280, Total Reward: 50.00, Epsilon: 0.246\n",
      "Episode 281, Total Reward: 240.00, Epsilon: 0.245\n",
      "Episode 282, Total Reward: 210.00, Epsilon: 0.243\n",
      "Episode 283, Total Reward: 60.00, Epsilon: 0.242\n",
      "Episode 284, Total Reward: -10.00, Epsilon: 0.241\n",
      "Episode 285, Total Reward: -10.00, Epsilon: 0.240\n",
      "Episode 286, Total Reward: -20.00, Epsilon: 0.238\n",
      "Episode 287, Total Reward: 180.00, Epsilon: 0.237\n",
      "Episode 288, Total Reward: -10.00, Epsilon: 0.236\n",
      "Episode 289, Total Reward: 50.00, Epsilon: 0.235\n",
      "Episode 290, Total Reward: 180.00, Epsilon: 0.234\n",
      "Episode 291, Total Reward: 110.00, Epsilon: 0.233\n",
      "Episode 292, Total Reward: 180.00, Epsilon: 0.231\n",
      "Episode 293, Total Reward: 250.00, Epsilon: 0.230\n",
      "Episode 294, Total Reward: 200.00, Epsilon: 0.229\n",
      "Episode 295, Total Reward: 150.00, Epsilon: 0.228\n",
      "Episode 296, Total Reward: 270.00, Epsilon: 0.227\n",
      "Episode 297, Total Reward: 190.00, Epsilon: 0.226\n",
      "Episode 298, Total Reward: 220.00, Epsilon: 0.225\n",
      "Episode 299, Total Reward: 240.00, Epsilon: 0.223\n",
      "Episode 300, Total Reward: 200.00, Epsilon: 0.222\n",
      "Episode 301, Total Reward: 200.00, Epsilon: 0.221\n",
      "Episode 302, Total Reward: 210.00, Epsilon: 0.220\n",
      "Episode 303, Total Reward: 150.00, Epsilon: 0.219\n",
      "Episode 304, Total Reward: 230.00, Epsilon: 0.218\n",
      "Episode 305, Total Reward: 140.00, Epsilon: 0.217\n",
      "Episode 306, Total Reward: 200.00, Epsilon: 0.216\n",
      "Episode 307, Total Reward: 200.00, Epsilon: 0.215\n",
      "Episode 308, Total Reward: 240.00, Epsilon: 0.214\n",
      "Episode 309, Total Reward: 260.00, Epsilon: 0.212\n",
      "Episode 310, Total Reward: 190.00, Epsilon: 0.211\n",
      "Episode 311, Total Reward: 220.00, Epsilon: 0.210\n",
      "Episode 312, Total Reward: 240.00, Epsilon: 0.209\n",
      "Episode 313, Total Reward: 240.00, Epsilon: 0.208\n",
      "Episode 314, Total Reward: 240.00, Epsilon: 0.207\n",
      "Episode 315, Total Reward: 240.00, Epsilon: 0.206\n",
      "Episode 316, Total Reward: 300.00, Epsilon: 0.205\n",
      "Episode 317, Total Reward: 270.00, Epsilon: 0.204\n",
      "Episode 318, Total Reward: 310.00, Epsilon: 0.203\n",
      "Episode 319, Total Reward: 180.00, Epsilon: 0.202\n",
      "Episode 320, Total Reward: 240.00, Epsilon: 0.201\n",
      "Episode 321, Total Reward: 210.00, Epsilon: 0.200\n",
      "Episode 322, Total Reward: 180.00, Epsilon: 0.199\n",
      "Episode 323, Total Reward: 280.00, Epsilon: 0.198\n",
      "Episode 324, Total Reward: 210.00, Epsilon: 0.197\n",
      "Episode 325, Total Reward: 270.00, Epsilon: 0.196\n",
      "Episode 326, Total Reward: 40.00, Epsilon: 0.195\n",
      "Episode 327, Total Reward: -10.00, Epsilon: 0.194\n",
      "Episode 328, Total Reward: 250.00, Epsilon: 0.193\n",
      "Episode 329, Total Reward: 280.00, Epsilon: 0.192\n",
      "Episode 330, Total Reward: 170.00, Epsilon: 0.191\n",
      "Episode 331, Total Reward: 250.00, Epsilon: 0.190\n",
      "Episode 332, Total Reward: 280.00, Epsilon: 0.189\n",
      "Episode 333, Total Reward: 270.00, Epsilon: 0.188\n",
      "Episode 334, Total Reward: 190.00, Epsilon: 0.187\n",
      "Episode 335, Total Reward: 20.00, Epsilon: 0.187\n",
      "Episode 336, Total Reward: 230.00, Epsilon: 0.186\n",
      "Episode 337, Total Reward: 240.00, Epsilon: 0.185\n",
      "Episode 338, Total Reward: 270.00, Epsilon: 0.184\n",
      "Episode 339, Total Reward: 290.00, Epsilon: 0.183\n",
      "Episode 340, Total Reward: 250.00, Epsilon: 0.182\n",
      "Episode 341, Total Reward: 240.00, Epsilon: 0.181\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 287\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Total Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Epsilon: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepsilon\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 287\u001b[0m     train()\n",
      "Cell \u001b[1;32mIn[1], line 259\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;66;03m# Update each agent's policy network\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m--> 259\u001b[0m     agents[i]\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_render:\n\u001b[0;32m    262\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender(screen)\n",
      "Cell \u001b[1;32mIn[1], line 195\u001b[0m, in \u001b[0;36mDQNAgent.update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    193\u001b[0m batch_action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(batch_action)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    194\u001b[0m batch_reward \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(batch_reward)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 195\u001b[0m batch_next_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(batch_next_state)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    196\u001b[0m batch_done \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(batch_done)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    198\u001b[0m current_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_net(batch_state)\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, batch_action)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pygame\n",
    "\n",
    "# Constants for gridworld and visualization\n",
    "GRID_SIZE = 10\n",
    "CELL_SIZE = 50  # pixels per cell\n",
    "WINDOW_SIZE = GRID_SIZE * CELL_SIZE\n",
    "NUM_OBSTACLES = 10\n",
    "\n",
    "# ----------------------------\n",
    "# Environment (Gridworld)\n",
    "# ----------------------------\n",
    "class Gridworld:\n",
    "    def __init__(self):\n",
    "        self.grid_size = GRID_SIZE\n",
    "        self.cell_size = CELL_SIZE\n",
    "        # Define goals: agent 0's goal is bottom-right, agent 1's goal is bottom-left\n",
    "        self.goals = [(GRID_SIZE - 1, GRID_SIZE - 1), (0, GRID_SIZE - 1)]\n",
    "        self.obstacles = self.generate_obstacles()\n",
    "        # Both agents start at top-left (0,0)\n",
    "        self.agents = [(0, 0), (0, 0)]\n",
    "        # Penalty if agents are too close (Euclidean distance < 2)\n",
    "        self.proximity_penalty = -10\n",
    "\n",
    "    def generate_obstacles(self):\n",
    "        obstacles = set()\n",
    "        # Generate random fixed obstacles while avoiding start and goal positions\n",
    "        while len(obstacles) < NUM_OBSTACLES:\n",
    "            x = random.randint(0, self.grid_size - 1)\n",
    "            y = random.randint(0, self.grid_size - 1)\n",
    "            if (x, y) != (0, 0) and (x, y) not in self.goals:\n",
    "                obstacles.add((x, y))\n",
    "        return list(obstacles)\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset agents to starting positions; obstacles remain fixed.\n",
    "        self.agents = [(0, 0), (0, 0)]\n",
    "        return self.get_states()\n",
    "\n",
    "    def get_states(self):\n",
    "        # Each agent's state: [agent_x, agent_y, goal_x, goal_y]\n",
    "        states = []\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            goal = self.goals[idx]\n",
    "            states.append([pos[0], pos[1], goal[0], goal[1]])\n",
    "        return states\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Expects a list of actions (one per agent: 0: up, 1: down, 2: left, 3: right).\n",
    "        Updates agent positions and returns:\n",
    "          - next_states: list of states for each agent,\n",
    "          - reward: a scalar reward (includes a penalty for proximity),\n",
    "          - done: True if both agents have reached their goals.\n",
    "        \"\"\"\n",
    "        new_positions = []\n",
    "        for idx, (x, y) in enumerate(self.agents):\n",
    "            dx, dy = 0, 0\n",
    "            action = actions[idx]\n",
    "            if action == 0:  # up\n",
    "                dy = -1\n",
    "            elif action == 1:  # down\n",
    "                dy = 1\n",
    "            elif action == 2:  # left\n",
    "                dx = -1\n",
    "            elif action == 3:  # right\n",
    "                dx = 1\n",
    "            new_x, new_y = x + dx, y + dy\n",
    "            # Check boundaries; invalid moves result in staying in place.\n",
    "            if new_x < 0 or new_x >= self.grid_size or new_y < 0 or new_y >= self.grid_size:\n",
    "                new_x, new_y = x, y\n",
    "            # Check obstacles; if moving into an obstacle, agent remains in place.\n",
    "            if (new_x, new_y) in self.obstacles:\n",
    "                new_x, new_y = x, y\n",
    "            new_positions.append((new_x, new_y))\n",
    "        self.agents = new_positions\n",
    "\n",
    "        # Compute reward and termination condition\n",
    "        reward = 0\n",
    "        done = False\n",
    "        # Penalty if agents are too close (Euclidean distance < 2)\n",
    "        dist = math.sqrt((self.agents[0][0] - self.agents[1][0])**2 +\n",
    "                         (self.agents[0][1] - self.agents[1][1])**2)\n",
    "        if dist < 2:\n",
    "            reward += self.proximity_penalty\n",
    "        # Reward for each agent reaching its goal\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            if pos == self.goals[idx]:\n",
    "                reward += 10\n",
    "\n",
    "        if self.agents[0] == self.goals[0] and self.agents[1] == self.goals[1]:\n",
    "            done = True\n",
    "\n",
    "        next_states = self.get_states()\n",
    "        return next_states, reward, done\n",
    "\n",
    "    def render(self, screen):\n",
    "        # Clear screen\n",
    "        screen.fill((255, 255, 255))\n",
    "        # Draw grid lines\n",
    "        for x in range(0, WINDOW_SIZE, self.cell_size):\n",
    "            pygame.draw.line(screen, (200, 200, 200), (x, 0), (x, WINDOW_SIZE))\n",
    "        for y in range(0, WINDOW_SIZE, self.cell_size):\n",
    "            pygame.draw.line(screen, (200, 200, 200), (0, y), (WINDOW_SIZE, y))\n",
    "        # Draw obstacles as black rectangles\n",
    "        for obs in self.obstacles:\n",
    "            rect = pygame.Rect(obs[0]*self.cell_size, obs[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (0, 0, 0), rect)\n",
    "        # Draw goals as green rectangles\n",
    "        for goal in self.goals:\n",
    "            rect = pygame.Rect(goal[0]*self.cell_size, goal[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (0, 255, 0), rect)\n",
    "        # Draw agents as circles (agent 0 in red, agent 1 in blue)\n",
    "        colors = [(255, 0, 0), (0, 0, 255)]\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            center = (pos[0]*self.cell_size + self.cell_size//2,\n",
    "                      pos[1]*self.cell_size + self.cell_size//2)\n",
    "            pygame.draw.circle(screen, colors[idx], center, self.cell_size//3)\n",
    "        pygame.display.flip()\n",
    "\n",
    "# ----------------------------\n",
    "# Replay Buffer for DQN\n",
    "# ----------------------------\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# ----------------------------\n",
    "# DQN Network and Agent\n",
    "# ----------------------------\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_dim, output_dim, lr=1e-3, gamma=0.99,\n",
    "                 buffer_capacity=10000, batch_size=32):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def select_action(self, state, epsilon):\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(4)  # 4 discrete actions\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "        return q_values.argmax().item()\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.replay_buffer.sample(self.batch_size)\n",
    "        batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*transitions)\n",
    "        \n",
    "        batch_state = torch.FloatTensor(batch_state).to(self.device)\n",
    "        batch_action = torch.LongTensor(batch_action).unsqueeze(1).to(self.device)\n",
    "        batch_reward = torch.FloatTensor(batch_reward).unsqueeze(1).to(self.device)\n",
    "        batch_next_state = torch.FloatTensor(batch_next_state).to(self.device)\n",
    "        batch_done = torch.FloatTensor(batch_done).unsqueeze(1).to(self.device)\n",
    "        \n",
    "        current_q = self.policy_net(batch_state).gather(1, batch_action)\n",
    "        next_q = self.target_net(batch_next_state).max(1)[0].unsqueeze(1)\n",
    "        target_q = batch_reward + self.gamma * next_q * (1 - batch_done)\n",
    "        \n",
    "        loss = nn.MSELoss()(current_q, target_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "# ----------------------------\n",
    "# Training Loop with Milestone Rendering\n",
    "# ----------------------------\n",
    "def train():\n",
    "    num_episodes = 500\n",
    "    max_steps = 50\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.1\n",
    "    epsilon_decay = 0.995\n",
    "    target_update_freq = 10\n",
    "    # Define milestone episodes (25%, 50%, 75%, and 100% of training)\n",
    "    milestones = {int(num_episodes * 0.25), int(num_episodes * 0.5), int(num_episodes * 0.75), num_episodes}\n",
    "    \n",
    "    env = Gridworld()\n",
    "    \n",
    "    # Each agent's state has 4 elements: [agent_x, agent_y, goal_x, goal_y]\n",
    "    state_dim = 4\n",
    "    action_dim = 4  # up, down, left, right\n",
    "    agents = [DQNAgent(state_dim, action_dim), DQNAgent(state_dim, action_dim)]\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        states = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        # Check if this is a milestone episode (episode indices are 0-based)\n",
    "        do_render = (episode + 1) in milestones\n",
    "        if do_render:\n",
    "            pygame.init()\n",
    "            screen = pygame.display.set_mode((WINDOW_SIZE, WINDOW_SIZE))\n",
    "            pygame.display.set_caption(f\"Training Episode {episode + 1}\")\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            actions = []\n",
    "            # Each agent selects an action using its own DQN policy\n",
    "            for i in range(2):\n",
    "                action = agents[i].select_action(states[i], epsilon)\n",
    "                actions.append(action)\n",
    "            next_states, reward, done = env.step(actions)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Store experiences for each agent\n",
    "            for i in range(2):\n",
    "                agents[i].replay_buffer.push(states[i], actions[i], reward, next_states[i], done)\n",
    "            \n",
    "            states = next_states\n",
    "            \n",
    "            # Update each agent's policy network\n",
    "            for i in range(2):\n",
    "                agents[i].update()\n",
    "            \n",
    "            if do_render:\n",
    "                env.render(screen)\n",
    "                # Process quit events in visualization mode\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.QUIT:\n",
    "                        pygame.quit()\n",
    "                        return\n",
    "                pygame.time.wait(200)\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        if do_render:\n",
    "            # Pause to allow viewing the final state of the milestone episode\n",
    "            pygame.time.wait(1000)\n",
    "            pygame.quit()\n",
    "        \n",
    "        # Update target networks periodically\n",
    "        if (episode + 1) % target_update_freq == 0:\n",
    "            for agent in agents:\n",
    "                agent.update_target()\n",
    "        \n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        print(f\"Episode {episode + 1}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a49002d-0c9d-4774-8d34-b0128d4534da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Total Reward: -800.00, Epsilon: 0.995\n",
      "Episode 2, Total Reward: -420.00, Epsilon: 0.990\n",
      "Episode 3, Total Reward: -770.00, Epsilon: 0.985\n",
      "Episode 4, Total Reward: -550.00, Epsilon: 0.980\n",
      "Episode 5, Total Reward: -690.00, Epsilon: 0.975\n",
      "Episode 6, Total Reward: -630.00, Epsilon: 0.970\n",
      "Episode 7, Total Reward: -690.00, Epsilon: 0.966\n",
      "Episode 8, Total Reward: -240.00, Epsilon: 0.961\n",
      "Episode 9, Total Reward: -390.00, Epsilon: 0.956\n",
      "Episode 10, Total Reward: -620.00, Epsilon: 0.951\n",
      "Episode 11, Total Reward: -800.00, Epsilon: 0.946\n",
      "Episode 12, Total Reward: -340.00, Epsilon: 0.942\n",
      "Episode 13, Total Reward: -660.00, Epsilon: 0.937\n",
      "Episode 14, Total Reward: -210.00, Epsilon: 0.932\n",
      "Episode 15, Total Reward: -580.00, Epsilon: 0.928\n",
      "Episode 16, Total Reward: -530.00, Epsilon: 0.923\n",
      "Episode 17, Total Reward: -610.00, Epsilon: 0.918\n",
      "Episode 18, Total Reward: -770.00, Epsilon: 0.914\n",
      "Episode 19, Total Reward: -380.00, Epsilon: 0.909\n",
      "Episode 20, Total Reward: -320.00, Epsilon: 0.905\n",
      "Episode 21, Total Reward: -790.00, Epsilon: 0.900\n",
      "Episode 22, Total Reward: -620.00, Epsilon: 0.896\n",
      "Episode 23, Total Reward: -380.00, Epsilon: 0.891\n",
      "Episode 24, Total Reward: -670.00, Epsilon: 0.887\n",
      "Episode 25, Total Reward: -700.00, Epsilon: 0.882\n",
      "Episode 26, Total Reward: -420.00, Epsilon: 0.878\n",
      "Episode 27, Total Reward: -420.00, Epsilon: 0.873\n",
      "Episode 28, Total Reward: -290.00, Epsilon: 0.869\n",
      "Episode 29, Total Reward: -380.00, Epsilon: 0.865\n",
      "Episode 30, Total Reward: -840.00, Epsilon: 0.860\n",
      "Episode 31, Total Reward: -480.00, Epsilon: 0.856\n",
      "Episode 32, Total Reward: -520.00, Epsilon: 0.852\n",
      "Episode 33, Total Reward: -510.00, Epsilon: 0.848\n",
      "Episode 34, Total Reward: -380.00, Epsilon: 0.843\n",
      "Episode 35, Total Reward: -280.00, Epsilon: 0.839\n",
      "Episode 36, Total Reward: -700.00, Epsilon: 0.835\n",
      "Episode 37, Total Reward: -410.00, Epsilon: 0.831\n",
      "Episode 38, Total Reward: -550.00, Epsilon: 0.827\n",
      "Episode 39, Total Reward: -560.00, Epsilon: 0.822\n",
      "Episode 40, Total Reward: -700.00, Epsilon: 0.818\n",
      "Episode 41, Total Reward: -310.00, Epsilon: 0.814\n",
      "Episode 42, Total Reward: -560.00, Epsilon: 0.810\n",
      "Episode 43, Total Reward: -780.00, Epsilon: 0.806\n",
      "Episode 44, Total Reward: -330.00, Epsilon: 0.802\n",
      "Episode 45, Total Reward: -730.00, Epsilon: 0.798\n",
      "Episode 46, Total Reward: -460.00, Epsilon: 0.794\n",
      "Episode 47, Total Reward: -470.00, Epsilon: 0.790\n",
      "Episode 48, Total Reward: -360.00, Epsilon: 0.786\n",
      "Episode 49, Total Reward: -270.00, Epsilon: 0.782\n",
      "Episode 50, Total Reward: -460.00, Epsilon: 0.778\n",
      "Episode 51, Total Reward: -320.00, Epsilon: 0.774\n",
      "Episode 52, Total Reward: -480.00, Epsilon: 0.771\n",
      "Episode 53, Total Reward: -360.00, Epsilon: 0.767\n",
      "Episode 54, Total Reward: -400.00, Epsilon: 0.763\n",
      "Episode 55, Total Reward: -390.00, Epsilon: 0.759\n",
      "Episode 56, Total Reward: -250.00, Epsilon: 0.755\n",
      "Episode 57, Total Reward: -260.00, Epsilon: 0.751\n",
      "Episode 58, Total Reward: -420.00, Epsilon: 0.748\n",
      "Episode 59, Total Reward: -510.00, Epsilon: 0.744\n",
      "Episode 60, Total Reward: -260.00, Epsilon: 0.740\n",
      "Episode 61, Total Reward: -510.00, Epsilon: 0.737\n",
      "Episode 62, Total Reward: -260.00, Epsilon: 0.733\n",
      "Episode 63, Total Reward: -450.00, Epsilon: 0.729\n",
      "Episode 64, Total Reward: -220.00, Epsilon: 0.726\n",
      "Episode 65, Total Reward: -460.00, Epsilon: 0.722\n",
      "Episode 66, Total Reward: -400.00, Epsilon: 0.718\n",
      "Episode 67, Total Reward: -430.00, Epsilon: 0.715\n",
      "Episode 68, Total Reward: -230.00, Epsilon: 0.711\n",
      "Episode 69, Total Reward: -350.00, Epsilon: 0.708\n",
      "Episode 70, Total Reward: -400.00, Epsilon: 0.704\n",
      "Episode 71, Total Reward: -390.00, Epsilon: 0.701\n",
      "Episode 72, Total Reward: -520.00, Epsilon: 0.697\n",
      "Episode 73, Total Reward: -390.00, Epsilon: 0.694\n",
      "Episode 74, Total Reward: -380.00, Epsilon: 0.690\n",
      "Episode 75, Total Reward: -320.00, Epsilon: 0.687\n",
      "Episode 76, Total Reward: -300.00, Epsilon: 0.683\n",
      "Episode 77, Total Reward: -400.00, Epsilon: 0.680\n",
      "Episode 78, Total Reward: -370.00, Epsilon: 0.676\n",
      "Episode 79, Total Reward: -300.00, Epsilon: 0.673\n",
      "Episode 80, Total Reward: -270.00, Epsilon: 0.670\n",
      "Episode 81, Total Reward: -390.00, Epsilon: 0.666\n",
      "Episode 82, Total Reward: -390.00, Epsilon: 0.663\n",
      "Episode 83, Total Reward: -420.00, Epsilon: 0.660\n",
      "Episode 84, Total Reward: -310.00, Epsilon: 0.656\n",
      "Episode 85, Total Reward: -330.00, Epsilon: 0.653\n",
      "Episode 86, Total Reward: -290.00, Epsilon: 0.650\n",
      "Episode 87, Total Reward: -280.00, Epsilon: 0.647\n",
      "Episode 88, Total Reward: -310.00, Epsilon: 0.643\n",
      "Episode 89, Total Reward: -340.00, Epsilon: 0.640\n",
      "Episode 90, Total Reward: -310.00, Epsilon: 0.637\n",
      "Episode 91, Total Reward: -420.00, Epsilon: 0.634\n",
      "Episode 92, Total Reward: -330.00, Epsilon: 0.631\n",
      "Episode 93, Total Reward: -520.00, Epsilon: 0.627\n",
      "Episode 94, Total Reward: -300.00, Epsilon: 0.624\n",
      "Episode 95, Total Reward: -270.00, Epsilon: 0.621\n",
      "Episode 96, Total Reward: -460.00, Epsilon: 0.618\n",
      "Episode 97, Total Reward: -390.00, Epsilon: 0.615\n",
      "Episode 98, Total Reward: -520.00, Epsilon: 0.612\n",
      "Episode 99, Total Reward: -310.00, Epsilon: 0.609\n",
      "Episode 100, Total Reward: -350.00, Epsilon: 0.606\n",
      "Episode 101, Total Reward: -450.00, Epsilon: 0.603\n",
      "Episode 102, Total Reward: -420.00, Epsilon: 0.600\n",
      "Episode 103, Total Reward: -370.00, Epsilon: 0.597\n",
      "Episode 104, Total Reward: -400.00, Epsilon: 0.594\n",
      "Episode 105, Total Reward: -310.00, Epsilon: 0.591\n",
      "Episode 106, Total Reward: -550.00, Epsilon: 0.588\n",
      "Episode 107, Total Reward: -370.00, Epsilon: 0.585\n",
      "Episode 108, Total Reward: -430.00, Epsilon: 0.582\n",
      "Episode 109, Total Reward: -160.00, Epsilon: 0.579\n",
      "Episode 110, Total Reward: -250.00, Epsilon: 0.576\n",
      "Episode 111, Total Reward: -350.00, Epsilon: 0.573\n",
      "Episode 112, Total Reward: -370.00, Epsilon: 0.570\n",
      "Episode 113, Total Reward: -300.00, Epsilon: 0.568\n",
      "Episode 114, Total Reward: -350.00, Epsilon: 0.565\n",
      "Episode 115, Total Reward: -340.00, Epsilon: 0.562\n",
      "Episode 116, Total Reward: -380.00, Epsilon: 0.559\n",
      "Episode 117, Total Reward: -580.00, Epsilon: 0.556\n",
      "Episode 118, Total Reward: -520.00, Epsilon: 0.554\n",
      "Episode 119, Total Reward: -480.00, Epsilon: 0.551\n",
      "Episode 120, Total Reward: -410.00, Epsilon: 0.548\n",
      "Episode 121, Total Reward: -290.00, Epsilon: 0.545\n",
      "Episode 122, Total Reward: -160.00, Epsilon: 0.543\n",
      "Episode 123, Total Reward: -320.00, Epsilon: 0.540\n",
      "Episode 124, Total Reward: -120.00, Epsilon: 0.537\n",
      "Episode 125, Total Reward: -410.00, Epsilon: 0.534\n",
      "Episode 126, Total Reward: -140.00, Epsilon: 0.532\n",
      "Episode 127, Total Reward: -300.00, Epsilon: 0.529\n",
      "Episode 128, Total Reward: -240.00, Epsilon: 0.526\n",
      "Episode 129, Total Reward: -280.00, Epsilon: 0.524\n",
      "Episode 130, Total Reward: -460.00, Epsilon: 0.521\n",
      "Episode 131, Total Reward: -220.00, Epsilon: 0.519\n",
      "Episode 132, Total Reward: -210.00, Epsilon: 0.516\n",
      "Episode 133, Total Reward: -220.00, Epsilon: 0.513\n",
      "Episode 134, Total Reward: -220.00, Epsilon: 0.511\n",
      "Episode 135, Total Reward: -320.00, Epsilon: 0.508\n",
      "Episode 136, Total Reward: -500.00, Epsilon: 0.506\n",
      "Episode 137, Total Reward: -370.00, Epsilon: 0.503\n",
      "Episode 138, Total Reward: -450.00, Epsilon: 0.501\n",
      "Episode 139, Total Reward: -220.00, Epsilon: 0.498\n",
      "Episode 140, Total Reward: -230.00, Epsilon: 0.496\n",
      "Episode 141, Total Reward: -290.00, Epsilon: 0.493\n",
      "Episode 142, Total Reward: -200.00, Epsilon: 0.491\n",
      "Episode 143, Total Reward: -210.00, Epsilon: 0.488\n",
      "Episode 144, Total Reward: -170.00, Epsilon: 0.486\n",
      "Episode 145, Total Reward: -130.00, Epsilon: 0.483\n",
      "Episode 146, Total Reward: -210.00, Epsilon: 0.481\n",
      "Episode 147, Total Reward: -330.00, Epsilon: 0.479\n",
      "Episode 148, Total Reward: -220.00, Epsilon: 0.476\n",
      "Episode 149, Total Reward: -350.00, Epsilon: 0.474\n",
      "Episode 150, Total Reward: -170.00, Epsilon: 0.471\n",
      "Episode 151, Total Reward: -310.00, Epsilon: 0.469\n",
      "Episode 152, Total Reward: -200.00, Epsilon: 0.467\n",
      "Episode 153, Total Reward: -260.00, Epsilon: 0.464\n",
      "Episode 154, Total Reward: -160.00, Epsilon: 0.462\n",
      "Episode 155, Total Reward: -270.00, Epsilon: 0.460\n",
      "Episode 156, Total Reward: -140.00, Epsilon: 0.458\n",
      "Episode 157, Total Reward: -270.00, Epsilon: 0.455\n",
      "Episode 158, Total Reward: -170.00, Epsilon: 0.453\n",
      "Episode 159, Total Reward: -220.00, Epsilon: 0.451\n",
      "Episode 160, Total Reward: -260.00, Epsilon: 0.448\n",
      "Episode 161, Total Reward: -170.00, Epsilon: 0.446\n",
      "Episode 162, Total Reward: -160.00, Epsilon: 0.444\n",
      "Episode 163, Total Reward: -160.00, Epsilon: 0.442\n",
      "Episode 164, Total Reward: -130.00, Epsilon: 0.440\n",
      "Episode 165, Total Reward: -170.00, Epsilon: 0.437\n",
      "Episode 166, Total Reward: -160.00, Epsilon: 0.435\n",
      "Episode 167, Total Reward: -330.00, Epsilon: 0.433\n",
      "Episode 168, Total Reward: -240.00, Epsilon: 0.431\n",
      "Episode 169, Total Reward: -270.00, Epsilon: 0.429\n",
      "Episode 170, Total Reward: -270.00, Epsilon: 0.427\n",
      "Episode 171, Total Reward: -200.00, Epsilon: 0.424\n",
      "Episode 172, Total Reward: -210.00, Epsilon: 0.422\n",
      "Episode 173, Total Reward: -160.00, Epsilon: 0.420\n",
      "Episode 174, Total Reward: -150.00, Epsilon: 0.418\n",
      "Episode 175, Total Reward: -250.00, Epsilon: 0.416\n",
      "Episode 176, Total Reward: -270.00, Epsilon: 0.414\n",
      "Episode 177, Total Reward: -110.00, Epsilon: 0.412\n",
      "Episode 178, Total Reward: -360.00, Epsilon: 0.410\n",
      "Episode 179, Total Reward: -230.00, Epsilon: 0.408\n",
      "Episode 180, Total Reward: -240.00, Epsilon: 0.406\n",
      "Episode 181, Total Reward: -230.00, Epsilon: 0.404\n",
      "Episode 182, Total Reward: -260.00, Epsilon: 0.402\n",
      "Episode 183, Total Reward: -350.00, Epsilon: 0.400\n",
      "Episode 184, Total Reward: -220.00, Epsilon: 0.398\n",
      "Episode 185, Total Reward: -160.00, Epsilon: 0.396\n",
      "Episode 186, Total Reward: -190.00, Epsilon: 0.394\n",
      "Episode 187, Total Reward: -210.00, Epsilon: 0.392\n",
      "Episode 188, Total Reward: -110.00, Epsilon: 0.390\n",
      "Episode 189, Total Reward: -170.00, Epsilon: 0.388\n",
      "Episode 190, Total Reward: -160.00, Epsilon: 0.386\n",
      "Episode 191, Total Reward: -160.00, Epsilon: 0.384\n",
      "Episode 192, Total Reward: -110.00, Epsilon: 0.382\n",
      "Episode 193, Total Reward: -110.00, Epsilon: 0.380\n",
      "Episode 194, Total Reward: -180.00, Epsilon: 0.378\n",
      "Episode 195, Total Reward: -140.00, Epsilon: 0.376\n",
      "Episode 196, Total Reward: -140.00, Epsilon: 0.374\n",
      "Episode 197, Total Reward: -110.00, Epsilon: 0.373\n",
      "Episode 198, Total Reward: -120.00, Epsilon: 0.371\n",
      "Episode 199, Total Reward: -160.00, Epsilon: 0.369\n",
      "Episode 200, Total Reward: -110.00, Epsilon: 0.367\n",
      "Episode 201, Total Reward: -120.00, Epsilon: 0.365\n",
      "Episode 202, Total Reward: -110.00, Epsilon: 0.363\n",
      "Episode 203, Total Reward: -210.00, Epsilon: 0.361\n",
      "Episode 204, Total Reward: -120.00, Epsilon: 0.360\n",
      "Episode 205, Total Reward: -110.00, Epsilon: 0.358\n",
      "Episode 206, Total Reward: -200.00, Epsilon: 0.356\n",
      "Episode 207, Total Reward: -170.00, Epsilon: 0.354\n",
      "Episode 208, Total Reward: -210.00, Epsilon: 0.353\n",
      "Episode 209, Total Reward: -110.00, Epsilon: 0.351\n",
      "Episode 210, Total Reward: -180.00, Epsilon: 0.349\n",
      "Episode 211, Total Reward: -180.00, Epsilon: 0.347\n",
      "Episode 212, Total Reward: -140.00, Epsilon: 0.346\n",
      "Episode 213, Total Reward: -110.00, Epsilon: 0.344\n",
      "Episode 214, Total Reward: -110.00, Epsilon: 0.342\n",
      "Episode 215, Total Reward: -120.00, Epsilon: 0.340\n",
      "Episode 216, Total Reward: -140.00, Epsilon: 0.339\n",
      "Episode 217, Total Reward: -100.00, Epsilon: 0.337\n",
      "Episode 218, Total Reward: -330.00, Epsilon: 0.335\n",
      "Episode 219, Total Reward: -110.00, Epsilon: 0.334\n",
      "Episode 220, Total Reward: -140.00, Epsilon: 0.332\n",
      "Episode 221, Total Reward: -100.00, Epsilon: 0.330\n",
      "Episode 222, Total Reward: -110.00, Epsilon: 0.329\n",
      "Episode 223, Total Reward: -130.00, Epsilon: 0.327\n",
      "Episode 224, Total Reward: -100.00, Epsilon: 0.325\n",
      "Episode 225, Total Reward: -110.00, Epsilon: 0.324\n",
      "Episode 226, Total Reward: -120.00, Epsilon: 0.322\n",
      "Episode 227, Total Reward: -210.00, Epsilon: 0.321\n",
      "Episode 228, Total Reward: -100.00, Epsilon: 0.319\n",
      "Episode 229, Total Reward: -110.00, Epsilon: 0.317\n",
      "Episode 230, Total Reward: -120.00, Epsilon: 0.316\n",
      "Episode 231, Total Reward: -130.00, Epsilon: 0.314\n",
      "Episode 232, Total Reward: -120.00, Epsilon: 0.313\n",
      "Episode 233, Total Reward: -110.00, Epsilon: 0.311\n",
      "Episode 234, Total Reward: -120.00, Epsilon: 0.309\n",
      "Episode 235, Total Reward: -160.00, Epsilon: 0.308\n",
      "Episode 236, Total Reward: -110.00, Epsilon: 0.306\n",
      "Episode 237, Total Reward: -120.00, Epsilon: 0.305\n",
      "Episode 238, Total Reward: -110.00, Epsilon: 0.303\n",
      "Episode 239, Total Reward: -120.00, Epsilon: 0.302\n",
      "Episode 240, Total Reward: -140.00, Epsilon: 0.300\n",
      "Episode 241, Total Reward: -110.00, Epsilon: 0.299\n",
      "Episode 242, Total Reward: -110.00, Epsilon: 0.297\n",
      "Episode 243, Total Reward: -120.00, Epsilon: 0.296\n",
      "Episode 244, Total Reward: -180.00, Epsilon: 0.294\n",
      "Episode 245, Total Reward: -130.00, Epsilon: 0.293\n",
      "Episode 246, Total Reward: -100.00, Epsilon: 0.291\n",
      "Episode 247, Total Reward: -110.00, Epsilon: 0.290\n",
      "Episode 248, Total Reward: -130.00, Epsilon: 0.288\n",
      "Episode 249, Total Reward: -160.00, Epsilon: 0.287\n",
      "Episode 250, Total Reward: -110.00, Epsilon: 0.286\n",
      "Episode 251, Total Reward: -110.00, Epsilon: 0.284\n",
      "Episode 252, Total Reward: -130.00, Epsilon: 0.283\n",
      "Episode 253, Total Reward: -120.00, Epsilon: 0.281\n",
      "Episode 254, Total Reward: -110.00, Epsilon: 0.280\n",
      "Episode 255, Total Reward: -110.00, Epsilon: 0.279\n",
      "Episode 256, Total Reward: -120.00, Epsilon: 0.277\n",
      "Episode 257, Total Reward: -120.00, Epsilon: 0.276\n",
      "Episode 258, Total Reward: -110.00, Epsilon: 0.274\n",
      "Episode 259, Total Reward: -120.00, Epsilon: 0.273\n",
      "Episode 260, Total Reward: -120.00, Epsilon: 0.272\n",
      "Episode 261, Total Reward: -110.00, Epsilon: 0.270\n",
      "Episode 262, Total Reward: -160.00, Epsilon: 0.269\n",
      "Episode 263, Total Reward: -130.00, Epsilon: 0.268\n",
      "Episode 264, Total Reward: -110.00, Epsilon: 0.266\n",
      "Episode 265, Total Reward: -110.00, Epsilon: 0.265\n",
      "Episode 266, Total Reward: -110.00, Epsilon: 0.264\n",
      "Episode 267, Total Reward: -120.00, Epsilon: 0.262\n",
      "Episode 268, Total Reward: -110.00, Epsilon: 0.261\n",
      "Episode 269, Total Reward: -110.00, Epsilon: 0.260\n",
      "Episode 270, Total Reward: -110.00, Epsilon: 0.258\n",
      "Episode 271, Total Reward: -130.00, Epsilon: 0.257\n",
      "Episode 272, Total Reward: -110.00, Epsilon: 0.256\n",
      "Episode 273, Total Reward: -120.00, Epsilon: 0.255\n",
      "Episode 274, Total Reward: -160.00, Epsilon: 0.253\n",
      "Episode 275, Total Reward: -110.00, Epsilon: 0.252\n",
      "Episode 276, Total Reward: -180.00, Epsilon: 0.251\n",
      "Episode 277, Total Reward: -110.00, Epsilon: 0.249\n",
      "Episode 278, Total Reward: -110.00, Epsilon: 0.248\n",
      "Episode 279, Total Reward: -130.00, Epsilon: 0.247\n",
      "Episode 280, Total Reward: -120.00, Epsilon: 0.246\n",
      "Episode 281, Total Reward: -130.00, Epsilon: 0.245\n",
      "Episode 282, Total Reward: -140.00, Epsilon: 0.243\n",
      "Episode 283, Total Reward: -130.00, Epsilon: 0.242\n",
      "Episode 284, Total Reward: -110.00, Epsilon: 0.241\n",
      "Episode 285, Total Reward: -110.00, Epsilon: 0.240\n",
      "Episode 286, Total Reward: -110.00, Epsilon: 0.238\n",
      "Episode 287, Total Reward: -110.00, Epsilon: 0.237\n",
      "Episode 288, Total Reward: -110.00, Epsilon: 0.236\n",
      "Episode 289, Total Reward: -130.00, Epsilon: 0.235\n",
      "Episode 290, Total Reward: -110.00, Epsilon: 0.234\n",
      "Episode 291, Total Reward: -190.00, Epsilon: 0.233\n",
      "Episode 292, Total Reward: -120.00, Epsilon: 0.231\n",
      "Episode 293, Total Reward: -110.00, Epsilon: 0.230\n",
      "Episode 294, Total Reward: -120.00, Epsilon: 0.229\n",
      "Episode 295, Total Reward: -110.00, Epsilon: 0.228\n",
      "Episode 296, Total Reward: -110.00, Epsilon: 0.227\n",
      "Episode 297, Total Reward: -100.00, Epsilon: 0.226\n",
      "Episode 298, Total Reward: -120.00, Epsilon: 0.225\n",
      "Episode 299, Total Reward: -110.00, Epsilon: 0.223\n",
      "Episode 300, Total Reward: -110.00, Epsilon: 0.222\n",
      "Episode 301, Total Reward: -110.00, Epsilon: 0.221\n",
      "Episode 302, Total Reward: -110.00, Epsilon: 0.220\n",
      "Episode 303, Total Reward: -210.00, Epsilon: 0.219\n",
      "Episode 304, Total Reward: -110.00, Epsilon: 0.218\n",
      "Episode 305, Total Reward: -110.00, Epsilon: 0.217\n",
      "Episode 306, Total Reward: -120.00, Epsilon: 0.216\n",
      "Episode 307, Total Reward: -170.00, Epsilon: 0.215\n",
      "Episode 308, Total Reward: -110.00, Epsilon: 0.214\n",
      "Episode 309, Total Reward: -110.00, Epsilon: 0.212\n",
      "Episode 310, Total Reward: -110.00, Epsilon: 0.211\n",
      "Episode 311, Total Reward: -100.00, Epsilon: 0.210\n",
      "Episode 312, Total Reward: -120.00, Epsilon: 0.209\n",
      "Episode 313, Total Reward: -110.00, Epsilon: 0.208\n",
      "Episode 314, Total Reward: -120.00, Epsilon: 0.207\n",
      "Episode 315, Total Reward: -110.00, Epsilon: 0.206\n",
      "Episode 316, Total Reward: -110.00, Epsilon: 0.205\n",
      "Episode 317, Total Reward: -130.00, Epsilon: 0.204\n",
      "Episode 318, Total Reward: -110.00, Epsilon: 0.203\n",
      "Episode 319, Total Reward: -110.00, Epsilon: 0.202\n",
      "Episode 320, Total Reward: -110.00, Epsilon: 0.201\n",
      "Episode 321, Total Reward: -110.00, Epsilon: 0.200\n",
      "Episode 322, Total Reward: -110.00, Epsilon: 0.199\n",
      "Episode 323, Total Reward: -110.00, Epsilon: 0.198\n",
      "Episode 324, Total Reward: -140.00, Epsilon: 0.197\n",
      "Episode 325, Total Reward: -110.00, Epsilon: 0.196\n",
      "Episode 326, Total Reward: -110.00, Epsilon: 0.195\n",
      "Episode 327, Total Reward: -110.00, Epsilon: 0.194\n",
      "Episode 328, Total Reward: -130.00, Epsilon: 0.193\n",
      "Episode 329, Total Reward: -110.00, Epsilon: 0.192\n",
      "Episode 330, Total Reward: -110.00, Epsilon: 0.191\n",
      "Episode 331, Total Reward: -100.00, Epsilon: 0.190\n",
      "Episode 332, Total Reward: -110.00, Epsilon: 0.189\n",
      "Episode 333, Total Reward: -110.00, Epsilon: 0.188\n",
      "Episode 334, Total Reward: -110.00, Epsilon: 0.187\n",
      "Episode 335, Total Reward: -110.00, Epsilon: 0.187\n",
      "Episode 336, Total Reward: -110.00, Epsilon: 0.186\n",
      "Episode 337, Total Reward: -110.00, Epsilon: 0.185\n",
      "Episode 338, Total Reward: -110.00, Epsilon: 0.184\n",
      "Episode 339, Total Reward: -110.00, Epsilon: 0.183\n",
      "Episode 340, Total Reward: -110.00, Epsilon: 0.182\n",
      "Episode 341, Total Reward: -960.00, Epsilon: 0.181\n",
      "Episode 342, Total Reward: -120.00, Epsilon: 0.180\n",
      "Episode 343, Total Reward: -110.00, Epsilon: 0.179\n",
      "Episode 344, Total Reward: -120.00, Epsilon: 0.178\n",
      "Episode 345, Total Reward: -120.00, Epsilon: 0.177\n",
      "Episode 346, Total Reward: -120.00, Epsilon: 0.177\n",
      "Episode 347, Total Reward: -110.00, Epsilon: 0.176\n",
      "Episode 348, Total Reward: -100.00, Epsilon: 0.175\n",
      "Episode 349, Total Reward: -110.00, Epsilon: 0.174\n",
      "Episode 350, Total Reward: -110.00, Epsilon: 0.173\n",
      "Episode 351, Total Reward: -110.00, Epsilon: 0.172\n",
      "Episode 352, Total Reward: -110.00, Epsilon: 0.171\n",
      "Episode 353, Total Reward: -110.00, Epsilon: 0.170\n",
      "Episode 354, Total Reward: -110.00, Epsilon: 0.170\n",
      "Episode 355, Total Reward: -120.00, Epsilon: 0.169\n",
      "Episode 356, Total Reward: -110.00, Epsilon: 0.168\n",
      "Episode 357, Total Reward: -100.00, Epsilon: 0.167\n",
      "Episode 358, Total Reward: -110.00, Epsilon: 0.166\n",
      "Episode 359, Total Reward: -110.00, Epsilon: 0.165\n",
      "Episode 360, Total Reward: -110.00, Epsilon: 0.165\n",
      "Episode 361, Total Reward: -100.00, Epsilon: 0.164\n",
      "Episode 362, Total Reward: -100.00, Epsilon: 0.163\n",
      "Episode 363, Total Reward: -100.00, Epsilon: 0.162\n",
      "Episode 364, Total Reward: -100.00, Epsilon: 0.161\n",
      "Episode 365, Total Reward: -130.00, Epsilon: 0.160\n",
      "Episode 366, Total Reward: -100.00, Epsilon: 0.160\n",
      "Episode 367, Total Reward: -110.00, Epsilon: 0.159\n",
      "Episode 368, Total Reward: -110.00, Epsilon: 0.158\n",
      "Episode 369, Total Reward: -100.00, Epsilon: 0.157\n",
      "Episode 370, Total Reward: -150.00, Epsilon: 0.157\n",
      "Episode 371, Total Reward: -160.00, Epsilon: 0.156\n",
      "Episode 372, Total Reward: -110.00, Epsilon: 0.155\n",
      "Episode 373, Total Reward: -110.00, Epsilon: 0.154\n",
      "Episode 374, Total Reward: -160.00, Epsilon: 0.153\n",
      "Episode 375, Total Reward: -100.00, Epsilon: 0.153\n",
      "Episode 376, Total Reward: -100.00, Epsilon: 0.152\n",
      "Episode 377, Total Reward: -110.00, Epsilon: 0.151\n",
      "Episode 378, Total Reward: -120.00, Epsilon: 0.150\n",
      "Episode 379, Total Reward: -110.00, Epsilon: 0.150\n",
      "Episode 380, Total Reward: -110.00, Epsilon: 0.149\n",
      "Episode 381, Total Reward: -110.00, Epsilon: 0.148\n",
      "Episode 382, Total Reward: -110.00, Epsilon: 0.147\n",
      "Episode 383, Total Reward: -170.00, Epsilon: 0.147\n",
      "Episode 384, Total Reward: -110.00, Epsilon: 0.146\n",
      "Episode 385, Total Reward: -110.00, Epsilon: 0.145\n",
      "Episode 386, Total Reward: -130.00, Epsilon: 0.144\n",
      "Episode 387, Total Reward: -110.00, Epsilon: 0.144\n",
      "Episode 388, Total Reward: -110.00, Epsilon: 0.143\n",
      "Episode 389, Total Reward: -110.00, Epsilon: 0.142\n",
      "Episode 390, Total Reward: -110.00, Epsilon: 0.142\n",
      "Episode 391, Total Reward: -100.00, Epsilon: 0.141\n",
      "Episode 392, Total Reward: -100.00, Epsilon: 0.140\n",
      "Episode 393, Total Reward: -100.00, Epsilon: 0.139\n",
      "Episode 394, Total Reward: -150.00, Epsilon: 0.139\n",
      "Episode 395, Total Reward: -160.00, Epsilon: 0.138\n",
      "Episode 396, Total Reward: -100.00, Epsilon: 0.137\n",
      "Episode 397, Total Reward: -110.00, Epsilon: 0.137\n",
      "Episode 398, Total Reward: -200.00, Epsilon: 0.136\n",
      "Episode 399, Total Reward: -150.00, Epsilon: 0.135\n",
      "Episode 400, Total Reward: -610.00, Epsilon: 0.135\n",
      "Episode 401, Total Reward: -190.00, Epsilon: 0.134\n",
      "Episode 402, Total Reward: -100.00, Epsilon: 0.133\n",
      "Episode 403, Total Reward: -100.00, Epsilon: 0.133\n",
      "Episode 404, Total Reward: -170.00, Epsilon: 0.132\n",
      "Episode 405, Total Reward: -150.00, Epsilon: 0.131\n",
      "Episode 406, Total Reward: -130.00, Epsilon: 0.131\n",
      "Episode 407, Total Reward: -150.00, Epsilon: 0.130\n",
      "Episode 408, Total Reward: -120.00, Epsilon: 0.129\n",
      "Episode 409, Total Reward: -150.00, Epsilon: 0.129\n",
      "Episode 410, Total Reward: -150.00, Epsilon: 0.128\n",
      "Episode 411, Total Reward: -100.00, Epsilon: 0.127\n",
      "Episode 412, Total Reward: -110.00, Epsilon: 0.127\n",
      "Episode 413, Total Reward: -160.00, Epsilon: 0.126\n",
      "Episode 414, Total Reward: -130.00, Epsilon: 0.126\n",
      "Episode 415, Total Reward: -110.00, Epsilon: 0.125\n",
      "Episode 416, Total Reward: -110.00, Epsilon: 0.124\n",
      "Episode 417, Total Reward: -110.00, Epsilon: 0.124\n",
      "Episode 418, Total Reward: -130.00, Epsilon: 0.123\n",
      "Episode 419, Total Reward: -100.00, Epsilon: 0.122\n",
      "Episode 420, Total Reward: -110.00, Epsilon: 0.122\n",
      "Episode 421, Total Reward: -190.00, Epsilon: 0.121\n",
      "Episode 422, Total Reward: -150.00, Epsilon: 0.121\n",
      "Episode 423, Total Reward: -110.00, Epsilon: 0.120\n",
      "Episode 424, Total Reward: -110.00, Epsilon: 0.119\n",
      "Episode 425, Total Reward: -120.00, Epsilon: 0.119\n",
      "Episode 426, Total Reward: -110.00, Epsilon: 0.118\n",
      "Episode 427, Total Reward: -120.00, Epsilon: 0.118\n",
      "Episode 428, Total Reward: -130.00, Epsilon: 0.117\n",
      "Episode 429, Total Reward: -110.00, Epsilon: 0.116\n",
      "Episode 430, Total Reward: -110.00, Epsilon: 0.116\n",
      "Episode 431, Total Reward: -110.00, Epsilon: 0.115\n",
      "Episode 432, Total Reward: -270.00, Epsilon: 0.115\n",
      "Episode 433, Total Reward: -120.00, Epsilon: 0.114\n",
      "Episode 434, Total Reward: -100.00, Epsilon: 0.114\n",
      "Episode 435, Total Reward: -140.00, Epsilon: 0.113\n",
      "Episode 436, Total Reward: -120.00, Epsilon: 0.112\n",
      "Episode 437, Total Reward: -110.00, Epsilon: 0.112\n",
      "Episode 438, Total Reward: -130.00, Epsilon: 0.111\n",
      "Episode 439, Total Reward: -210.00, Epsilon: 0.111\n",
      "Episode 440, Total Reward: -130.00, Epsilon: 0.110\n",
      "Episode 441, Total Reward: -170.00, Epsilon: 0.110\n",
      "Episode 442, Total Reward: -110.00, Epsilon: 0.109\n",
      "Episode 443, Total Reward: -110.00, Epsilon: 0.109\n",
      "Episode 444, Total Reward: -110.00, Epsilon: 0.108\n",
      "Episode 445, Total Reward: -160.00, Epsilon: 0.107\n",
      "Episode 446, Total Reward: -110.00, Epsilon: 0.107\n",
      "Episode 447, Total Reward: -170.00, Epsilon: 0.106\n",
      "Episode 448, Total Reward: -350.00, Epsilon: 0.106\n",
      "Episode 449, Total Reward: -120.00, Epsilon: 0.105\n",
      "Episode 450, Total Reward: -170.00, Epsilon: 0.105\n",
      "Episode 451, Total Reward: -110.00, Epsilon: 0.104\n",
      "Episode 452, Total Reward: -110.00, Epsilon: 0.104\n",
      "Episode 453, Total Reward: -130.00, Epsilon: 0.103\n",
      "Episode 454, Total Reward: -110.00, Epsilon: 0.103\n",
      "Episode 455, Total Reward: -150.00, Epsilon: 0.102\n",
      "Episode 456, Total Reward: -100.00, Epsilon: 0.102\n",
      "Episode 457, Total Reward: -100.00, Epsilon: 0.101\n",
      "Episode 458, Total Reward: -130.00, Epsilon: 0.101\n",
      "Episode 459, Total Reward: -220.00, Epsilon: 0.100\n",
      "Episode 460, Total Reward: -110.00, Epsilon: 0.100\n",
      "Episode 461, Total Reward: -100.00, Epsilon: 0.100\n",
      "Episode 462, Total Reward: -120.00, Epsilon: 0.100\n",
      "Episode 463, Total Reward: -130.00, Epsilon: 0.100\n",
      "Episode 464, Total Reward: -110.00, Epsilon: 0.100\n",
      "Episode 465, Total Reward: -110.00, Epsilon: 0.100\n",
      "Episode 466, Total Reward: -110.00, Epsilon: 0.100\n",
      "Episode 467, Total Reward: -110.00, Epsilon: 0.100\n",
      "Episode 468, Total Reward: -120.00, Epsilon: 0.100\n",
      "Episode 469, Total Reward: -120.00, Epsilon: 0.100\n",
      "Episode 470, Total Reward: -150.00, Epsilon: 0.100\n",
      "Episode 471, Total Reward: -120.00, Epsilon: 0.100\n",
      "Episode 472, Total Reward: -110.00, Epsilon: 0.100\n",
      "Episode 473, Total Reward: -120.00, Epsilon: 0.100\n",
      "Episode 474, Total Reward: -150.00, Epsilon: 0.100\n",
      "Episode 475, Total Reward: -100.00, Epsilon: 0.100\n",
      "Episode 476, Total Reward: -110.00, Epsilon: 0.100\n",
      "Episode 477, Total Reward: -120.00, Epsilon: 0.100\n",
      "Episode 478, Total Reward: -120.00, Epsilon: 0.100\n",
      "Episode 479, Total Reward: -110.00, Epsilon: 0.100\n",
      "Episode 480, Total Reward: -120.00, Epsilon: 0.100\n",
      "Episode 481, Total Reward: -100.00, Epsilon: 0.100\n",
      "Episode 482, Total Reward: -120.00, Epsilon: 0.100\n",
      "Episode 483, Total Reward: -130.00, Epsilon: 0.100\n",
      "Episode 484, Total Reward: -110.00, Epsilon: 0.100\n",
      "Episode 485, Total Reward: -170.00, Epsilon: 0.100\n",
      "Episode 486, Total Reward: -110.00, Epsilon: 0.100\n",
      "Episode 487, Total Reward: -120.00, Epsilon: 0.100\n",
      "Episode 488, Total Reward: -110.00, Epsilon: 0.100\n",
      "Episode 489, Total Reward: -110.00, Epsilon: 0.100\n",
      "Episode 490, Total Reward: -110.00, Epsilon: 0.100\n",
      "Episode 491, Total Reward: -130.00, Epsilon: 0.100\n",
      "Episode 492, Total Reward: -110.00, Epsilon: 0.100\n",
      "Episode 493, Total Reward: -110.00, Epsilon: 0.100\n",
      "Episode 494, Total Reward: -110.00, Epsilon: 0.100\n",
      "Episode 495, Total Reward: -120.00, Epsilon: 0.100\n",
      "Episode 496, Total Reward: -110.00, Epsilon: 0.100\n",
      "Episode 497, Total Reward: -110.00, Epsilon: 0.100\n",
      "Episode 498, Total Reward: -110.00, Epsilon: 0.100\n",
      "Episode 499, Total Reward: -110.00, Epsilon: 0.100\n",
      "Episode 500, Total Reward: -120.00, Epsilon: 0.100\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pygame\n",
    "\n",
    "# Constants for gridworld and visualization\n",
    "GRID_SIZE = 10\n",
    "CELL_SIZE = 50   # pixels per cell\n",
    "WINDOW_SIZE = GRID_SIZE * CELL_SIZE\n",
    "NUM_OBSTACLES = 10\n",
    "\n",
    "# ----------------------------\n",
    "# Environment (Gridworld)\n",
    "# ----------------------------\n",
    "class Gridworld:\n",
    "    def __init__(self):\n",
    "        self.grid_size = GRID_SIZE\n",
    "        self.cell_size = CELL_SIZE\n",
    "        self.vision_radius = 5  # drones can sense obstacles within 5 cells\n",
    "        # Define goals: agent 0's goal is bottom-right, agent 1's goal is bottom-left\n",
    "        self.goals = [(GRID_SIZE - 1, GRID_SIZE - 1), (0, GRID_SIZE - 1)]\n",
    "        self.obstacles = self.generate_obstacles()\n",
    "        # Both agents start at top-left (0,0)\n",
    "        self.agents = [(0, 0), (0, 0)]\n",
    "        # Penalty values\n",
    "        self.movement_penalty = -1       # per move per agent\n",
    "        self.closeness_penalty = -10     # if Manhattan distance < 2\n",
    "        self.collision_penalty = -50     # if agent attempts to move into an obstacle\n",
    "\n",
    "    def generate_obstacles(self):\n",
    "        obstacles = set()\n",
    "        # Generate random fixed obstacles while avoiding start and goal positions\n",
    "        while len(obstacles) < NUM_OBSTACLES:\n",
    "            x = random.randint(0, self.grid_size - 1)\n",
    "            y = random.randint(0, self.grid_size - 1)\n",
    "            if (x, y) != (0, 0) and (x, y) not in self.goals:\n",
    "                obstacles.add((x, y))\n",
    "        return list(obstacles)\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset agents to starting positions; obstacles remain fixed.\n",
    "        self.agents = [(0, 0), (0, 0)]\n",
    "        return self.get_states()\n",
    "\n",
    "    def get_states(self):\n",
    "        # Each agent's state: [agent_x, agent_y, goal_x, goal_y]\n",
    "        # (Extend this in the future with vision info if desired)\n",
    "        states = []\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            goal = self.goals[idx]\n",
    "            states.append([pos[0], pos[1], goal[0], goal[1]])\n",
    "        return states\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Expects a list of actions (one per agent: 0: up, 1: down, 2: left, 3: right).\n",
    "        Updates agent positions and returns:\n",
    "          - next_states: list of states for each agent,\n",
    "          - reward: a scalar reward that includes:\n",
    "                * movement penalty,\n",
    "                * collision penalty (if agent attempts to move into an obstacle),\n",
    "                * closeness penalty (if Manhattan distance < 2),\n",
    "                * goal achievement rewards.\n",
    "          - done: True if both agents have reached their goals.\n",
    "        \"\"\"\n",
    "        reward = 0\n",
    "        # Apply movement penalty for each agent regardless of move validity.\n",
    "        reward += self.movement_penalty * len(self.agents)\n",
    "        \n",
    "        new_positions = []\n",
    "        for idx, (x, y) in enumerate(self.agents):\n",
    "            dx, dy = 0, 0\n",
    "            action = actions[idx]\n",
    "            if action == 0:  # up\n",
    "                dy = -1\n",
    "            elif action == 1:  # down\n",
    "                dy = 1\n",
    "            elif action == 2:  # left\n",
    "                dx = -1\n",
    "            elif action == 3:  # right\n",
    "                dx = 1\n",
    "            new_x, new_y = x + dx, y + dy\n",
    "            # Check boundaries; invalid moves result in staying in place.\n",
    "            if new_x < 0 or new_x >= self.grid_size or new_y < 0 or new_y >= self.grid_size:\n",
    "                new_x, new_y = x, y\n",
    "            # Check obstacles; if moving into an obstacle, apply collision penalty and stay in place.\n",
    "            if (new_x, new_y) in self.obstacles:\n",
    "                reward += self.collision_penalty\n",
    "                new_x, new_y = x, y\n",
    "            new_positions.append((new_x, new_y))\n",
    "        self.agents = new_positions\n",
    "\n",
    "        # Compute closeness penalty based on Manhattan distance between agents.\n",
    "        manhattan_distance = abs(self.agents[0][0] - self.agents[1][0]) + abs(self.agents[0][1] - self.agents[1][1])\n",
    "        if manhattan_distance < 2:\n",
    "            reward += self.closeness_penalty\n",
    "\n",
    "        # Reward for each agent reaching its goal.\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            if pos == self.goals[idx]:\n",
    "                reward += 10  # goal reward\n",
    "\n",
    "        # Episode ends if both agents have reached their goals.\n",
    "        done = (self.agents[0] == self.goals[0] and self.agents[1] == self.goals[1])\n",
    "        next_states = self.get_states()\n",
    "        return next_states, reward, done\n",
    "\n",
    "    def render(self, screen):\n",
    "        # Clear screen\n",
    "        screen.fill((255, 255, 255))\n",
    "        # Draw grid lines\n",
    "        for x in range(0, WINDOW_SIZE, self.cell_size):\n",
    "            pygame.draw.line(screen, (200, 200, 200), (x, 0), (x, WINDOW_SIZE))\n",
    "        for y in range(0, WINDOW_SIZE, self.cell_size):\n",
    "            pygame.draw.line(screen, (200, 200, 200), (0, y), (WINDOW_SIZE, y))\n",
    "        # Draw obstacles as black rectangles\n",
    "        for obs in self.obstacles:\n",
    "            rect = pygame.Rect(obs[0]*self.cell_size, obs[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (0, 0, 0), rect)\n",
    "        # Draw goals as green rectangles\n",
    "        for goal in self.goals:\n",
    "            rect = pygame.Rect(goal[0]*self.cell_size, goal[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (0, 255, 0), rect)\n",
    "        # Draw agents as circles (agent 0 in red, agent 1 in blue)\n",
    "        colors = [(255, 0, 0), (0, 0, 255)]\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            center = (pos[0]*self.cell_size + self.cell_size//2,\n",
    "                      pos[1]*self.cell_size + self.cell_size//2)\n",
    "            pygame.draw.circle(screen, colors[idx], center, self.cell_size//3)\n",
    "        pygame.display.flip()\n",
    "\n",
    "# ----------------------------\n",
    "# Replay Buffer for DQN\n",
    "# ----------------------------\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# ----------------------------\n",
    "# DQN Network and Agent\n",
    "# ----------------------------\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_dim, output_dim, lr=1e-3, gamma=0.99,\n",
    "                 buffer_capacity=10000, batch_size=32):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def select_action(self, state, epsilon):\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(4)  # 4 discrete actions: up, down, left, right\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "        return q_values.argmax().item()\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.replay_buffer.sample(self.batch_size)\n",
    "        batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*transitions)\n",
    "        \n",
    "        batch_state = torch.FloatTensor(batch_state).to(self.device)\n",
    "        batch_action = torch.LongTensor(batch_action).unsqueeze(1).to(self.device)\n",
    "        batch_reward = torch.FloatTensor(batch_reward).unsqueeze(1).to(self.device)\n",
    "        batch_next_state = torch.FloatTensor(batch_next_state).to(self.device)\n",
    "        batch_done = torch.FloatTensor(batch_done).unsqueeze(1).to(self.device)\n",
    "        \n",
    "        current_q = self.policy_net(batch_state).gather(1, batch_action)\n",
    "        next_q = self.target_net(batch_next_state).max(1)[0].unsqueeze(1)\n",
    "        target_q = batch_reward + self.gamma * next_q * (1 - batch_done)\n",
    "        \n",
    "        loss = nn.MSELoss()(current_q, target_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "# ----------------------------\n",
    "# Training Loop with Milestone Rendering\n",
    "# ----------------------------\n",
    "def train():\n",
    "    num_episodes = 500\n",
    "    max_steps = 50\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.1\n",
    "    epsilon_decay = 0.995\n",
    "    target_update_freq = 10\n",
    "    # Define milestone episodes (25%, 50%, 75%, and 100% of training)\n",
    "    milestones = {int(num_episodes * 0.25), int(num_episodes * 0.5), int(num_episodes * 0.75), num_episodes}\n",
    "    \n",
    "    env = Gridworld()\n",
    "    \n",
    "    # Each agent's state has 4 elements: [agent_x, agent_y, goal_x, goal_y]\n",
    "    state_dim = 4\n",
    "    action_dim = 4  # up, down, left, right\n",
    "    agents = [DQNAgent(state_dim, action_dim), DQNAgent(state_dim, action_dim)]\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        states = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        # Check if this is a milestone episode (episode indices are 0-based)\n",
    "        do_render = (episode + 1) in milestones\n",
    "        if do_render:\n",
    "            pygame.init()\n",
    "            screen = pygame.display.set_mode((WINDOW_SIZE, WINDOW_SIZE))\n",
    "            pygame.display.set_caption(f\"Training Episode {episode + 1}\")\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            actions = []\n",
    "            # Each agent selects an action using its own DQN policy\n",
    "            for i in range(2):\n",
    "                action = agents[i].select_action(states[i], epsilon)\n",
    "                actions.append(action)\n",
    "            next_states, reward, done = env.step(actions)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Store experiences for each agent\n",
    "            for i in range(2):\n",
    "                agents[i].replay_buffer.push(states[i], actions[i], reward, next_states[i], done)\n",
    "            \n",
    "            states = next_states\n",
    "            \n",
    "            # Update each agent's policy network\n",
    "            for i in range(2):\n",
    "                agents[i].update()\n",
    "            \n",
    "            if do_render:\n",
    "                env.render(screen)\n",
    "                # Process quit events in visualization mode\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.QUIT:\n",
    "                        pygame.quit()\n",
    "                        return\n",
    "                pygame.time.wait(200)\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        if do_render:\n",
    "            # Pause to allow viewing the final state of the milestone episode\n",
    "            pygame.time.wait(1000)\n",
    "            pygame.quit()\n",
    "        \n",
    "        # Update target networks periodically\n",
    "        if (episode + 1) % target_update_freq == 0:\n",
    "            for agent in agents:\n",
    "                agent.update_target()\n",
    "        \n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        print(f\"Episode {episode + 1}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "913de779-d8fc-4bd5-9d7a-7e744d0e0c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Total Reward: -2706.50, Epsilon: 1.000\n",
      "Episode 2, Total Reward: -2118.00, Epsilon: 0.999\n",
      "Episode 3, Total Reward: -1733.00, Epsilon: 0.999\n",
      "Episode 4, Total Reward: -1460.00, Epsilon: 0.998\n",
      "Episode 5, Total Reward: -1259.00, Epsilon: 0.998\n",
      "Episode 6, Total Reward: -1526.50, Epsilon: 0.997\n",
      "Episode 7, Total Reward: -1250.00, Epsilon: 0.997\n",
      "Episode 8, Total Reward: -2258.50, Epsilon: 0.996\n",
      "Episode 9, Total Reward: -1134.00, Epsilon: 0.996\n",
      "Episode 10, Total Reward: -1380.00, Epsilon: 0.995\n",
      "Episode 11, Total Reward: -1535.00, Epsilon: 0.995\n",
      "Episode 12, Total Reward: -2342.00, Epsilon: 0.994\n",
      "Episode 13, Total Reward: -2122.00, Epsilon: 0.994\n",
      "Episode 14, Total Reward: -1156.00, Epsilon: 0.993\n",
      "Episode 15, Total Reward: -2175.00, Epsilon: 0.993\n",
      "Episode 16, Total Reward: -1365.00, Epsilon: 0.992\n",
      "Episode 17, Total Reward: -3869.50, Epsilon: 0.992\n",
      "Episode 18, Total Reward: -1144.50, Epsilon: 0.991\n",
      "Episode 19, Total Reward: -1268.50, Epsilon: 0.991\n",
      "Episode 20, Total Reward: -492.00, Epsilon: 0.990\n",
      "Episode 21, Total Reward: -1735.50, Epsilon: 0.990\n",
      "Episode 22, Total Reward: -1320.00, Epsilon: 0.989\n",
      "Episode 23, Total Reward: -2279.50, Epsilon: 0.989\n",
      "Episode 24, Total Reward: -562.00, Epsilon: 0.988\n",
      "Episode 25, Total Reward: -1553.50, Epsilon: 0.988\n",
      "Episode 26, Total Reward: -2103.00, Epsilon: 0.987\n",
      "Episode 27, Total Reward: -1608.50, Epsilon: 0.987\n",
      "Episode 28, Total Reward: -1234.00, Epsilon: 0.986\n",
      "Episode 29, Total Reward: -1315.00, Epsilon: 0.986\n",
      "Episode 30, Total Reward: -1356.00, Epsilon: 0.985\n",
      "Episode 31, Total Reward: -2354.50, Epsilon: 0.985\n",
      "Episode 32, Total Reward: -2251.00, Epsilon: 0.984\n",
      "Episode 33, Total Reward: -3606.00, Epsilon: 0.984\n",
      "Episode 34, Total Reward: -2003.00, Epsilon: 0.983\n",
      "Episode 35, Total Reward: -1643.00, Epsilon: 0.983\n",
      "Episode 36, Total Reward: -1316.00, Epsilon: 0.982\n",
      "Episode 37, Total Reward: -1710.00, Epsilon: 0.982\n",
      "Episode 38, Total Reward: -993.00, Epsilon: 0.981\n",
      "Episode 39, Total Reward: -1357.00, Epsilon: 0.981\n",
      "Episode 40, Total Reward: -1847.00, Epsilon: 0.980\n",
      "Episode 41, Total Reward: -1790.00, Epsilon: 0.980\n",
      "Episode 42, Total Reward: -1795.00, Epsilon: 0.979\n",
      "Episode 43, Total Reward: -2553.00, Epsilon: 0.979\n",
      "Episode 44, Total Reward: -1704.50, Epsilon: 0.978\n",
      "Episode 45, Total Reward: -3209.00, Epsilon: 0.978\n",
      "Episode 46, Total Reward: -1339.50, Epsilon: 0.977\n",
      "Episode 47, Total Reward: -1496.50, Epsilon: 0.977\n",
      "Episode 48, Total Reward: -3136.00, Epsilon: 0.976\n",
      "Episode 49, Total Reward: -2176.50, Epsilon: 0.976\n",
      "Episode 50, Total Reward: -1735.50, Epsilon: 0.975\n",
      "Episode 51, Total Reward: -1602.00, Epsilon: 0.975\n",
      "Episode 52, Total Reward: -1929.50, Epsilon: 0.974\n",
      "Episode 53, Total Reward: -2976.00, Epsilon: 0.974\n",
      "Episode 54, Total Reward: -1327.50, Epsilon: 0.973\n",
      "Episode 55, Total Reward: -1528.00, Epsilon: 0.973\n",
      "Episode 56, Total Reward: -1162.00, Epsilon: 0.972\n",
      "Episode 57, Total Reward: -1297.00, Epsilon: 0.972\n",
      "Episode 58, Total Reward: -1216.00, Epsilon: 0.971\n",
      "Episode 59, Total Reward: -1305.50, Epsilon: 0.971\n",
      "Episode 60, Total Reward: -683.50, Epsilon: 0.970\n",
      "Episode 61, Total Reward: -354.00, Epsilon: 0.970\n",
      "Episode 62, Total Reward: -913.50, Epsilon: 0.969\n",
      "Episode 63, Total Reward: -2478.50, Epsilon: 0.969\n",
      "Episode 64, Total Reward: -1952.50, Epsilon: 0.968\n",
      "Episode 65, Total Reward: -2679.00, Epsilon: 0.968\n",
      "Episode 66, Total Reward: -44.00, Epsilon: 0.968\n",
      "Episode 67, Total Reward: -1754.00, Epsilon: 0.967\n",
      "Episode 68, Total Reward: -2213.00, Epsilon: 0.967\n",
      "Episode 69, Total Reward: -1265.00, Epsilon: 0.966\n",
      "Episode 70, Total Reward: -1283.50, Epsilon: 0.966\n",
      "Episode 71, Total Reward: -1063.00, Epsilon: 0.965\n",
      "Episode 72, Total Reward: -2013.50, Epsilon: 0.965\n",
      "Episode 73, Total Reward: -1065.00, Epsilon: 0.964\n",
      "Episode 74, Total Reward: -2188.00, Epsilon: 0.964\n",
      "Episode 75, Total Reward: -1787.50, Epsilon: 0.963\n",
      "Episode 76, Total Reward: -1735.00, Epsilon: 0.963\n",
      "Episode 77, Total Reward: -1381.50, Epsilon: 0.962\n",
      "Episode 78, Total Reward: -1131.50, Epsilon: 0.962\n",
      "Episode 79, Total Reward: -830.50, Epsilon: 0.961\n",
      "Episode 80, Total Reward: -2397.50, Epsilon: 0.961\n",
      "Episode 81, Total Reward: -1768.00, Epsilon: 0.960\n",
      "Episode 82, Total Reward: -1316.50, Epsilon: 0.960\n",
      "Episode 83, Total Reward: -985.50, Epsilon: 0.959\n",
      "Episode 84, Total Reward: -945.00, Epsilon: 0.959\n",
      "Episode 85, Total Reward: -2371.00, Epsilon: 0.958\n",
      "Episode 86, Total Reward: -1087.00, Epsilon: 0.958\n",
      "Episode 87, Total Reward: -957.00, Epsilon: 0.957\n",
      "Episode 88, Total Reward: -2448.00, Epsilon: 0.957\n",
      "Episode 89, Total Reward: -1573.50, Epsilon: 0.956\n",
      "Episode 90, Total Reward: -2158.00, Epsilon: 0.956\n",
      "Episode 91, Total Reward: -1454.00, Epsilon: 0.956\n",
      "Episode 92, Total Reward: -2120.50, Epsilon: 0.955\n",
      "Episode 93, Total Reward: -903.00, Epsilon: 0.955\n",
      "Episode 94, Total Reward: -2140.00, Epsilon: 0.954\n",
      "Episode 95, Total Reward: -1037.00, Epsilon: 0.954\n",
      "Episode 96, Total Reward: -1937.00, Epsilon: 0.953\n",
      "Episode 97, Total Reward: -3081.00, Epsilon: 0.953\n",
      "Episode 98, Total Reward: -1448.50, Epsilon: 0.952\n",
      "Episode 99, Total Reward: -1377.00, Epsilon: 0.952\n",
      "Episode 100, Total Reward: -1342.50, Epsilon: 0.951\n",
      "Episode 101, Total Reward: -2143.00, Epsilon: 0.951\n",
      "Episode 102, Total Reward: -1385.50, Epsilon: 0.950\n",
      "Episode 103, Total Reward: -2398.50, Epsilon: 0.950\n",
      "Episode 104, Total Reward: -1464.50, Epsilon: 0.949\n",
      "Episode 105, Total Reward: -1731.00, Epsilon: 0.949\n",
      "Episode 106, Total Reward: -1078.00, Epsilon: 0.948\n",
      "Episode 107, Total Reward: -1274.50, Epsilon: 0.948\n",
      "Episode 108, Total Reward: -2083.50, Epsilon: 0.947\n",
      "Episode 109, Total Reward: -2478.50, Epsilon: 0.947\n",
      "Episode 110, Total Reward: -1755.00, Epsilon: 0.946\n",
      "Episode 111, Total Reward: 251.50, Epsilon: 0.946\n",
      "Episode 112, Total Reward: -2253.50, Epsilon: 0.946\n",
      "Episode 113, Total Reward: -687.50, Epsilon: 0.945\n",
      "Episode 114, Total Reward: -1201.50, Epsilon: 0.945\n",
      "Episode 115, Total Reward: -814.00, Epsilon: 0.944\n",
      "Episode 116, Total Reward: -1485.00, Epsilon: 0.944\n",
      "Episode 117, Total Reward: 857.50, Epsilon: 0.943\n",
      "Episode 118, Total Reward: -3072.50, Epsilon: 0.943\n",
      "Episode 119, Total Reward: 399.50, Epsilon: 0.942\n",
      "Episode 120, Total Reward: -1617.00, Epsilon: 0.942\n",
      "Episode 121, Total Reward: -1289.50, Epsilon: 0.941\n",
      "Episode 122, Total Reward: -1938.50, Epsilon: 0.941\n",
      "Episode 123, Total Reward: -283.50, Epsilon: 0.940\n",
      "Episode 124, Total Reward: -2749.00, Epsilon: 0.940\n",
      "Episode 125, Total Reward: -3275.50, Epsilon: 0.939\n",
      "Episode 126, Total Reward: -1535.00, Epsilon: 0.939\n",
      "Episode 127, Total Reward: -1783.50, Epsilon: 0.938\n",
      "Episode 128, Total Reward: -1613.00, Epsilon: 0.938\n",
      "Episode 129, Total Reward: -243.50, Epsilon: 0.938\n",
      "Episode 130, Total Reward: -1184.50, Epsilon: 0.937\n",
      "Episode 131, Total Reward: -1437.50, Epsilon: 0.937\n",
      "Episode 132, Total Reward: -1903.50, Epsilon: 0.936\n",
      "Episode 133, Total Reward: -20.00, Epsilon: 0.936\n",
      "Episode 134, Total Reward: -201.50, Epsilon: 0.935\n",
      "Episode 135, Total Reward: -789.50, Epsilon: 0.935\n",
      "Episode 136, Total Reward: -710.00, Epsilon: 0.934\n",
      "Episode 137, Total Reward: 645.00, Epsilon: 0.934\n",
      "Episode 138, Total Reward: -1974.00, Epsilon: 0.933\n",
      "Episode 139, Total Reward: -1474.50, Epsilon: 0.933\n",
      "Episode 140, Total Reward: -1188.50, Epsilon: 0.932\n",
      "Episode 141, Total Reward: -1102.00, Epsilon: 0.932\n",
      "Episode 142, Total Reward: -868.50, Epsilon: 0.931\n",
      "Episode 143, Total Reward: -1522.00, Epsilon: 0.931\n",
      "Episode 144, Total Reward: -805.50, Epsilon: 0.931\n",
      "Episode 145, Total Reward: -2265.00, Epsilon: 0.930\n",
      "Episode 146, Total Reward: -2604.00, Epsilon: 0.930\n",
      "Episode 147, Total Reward: -1134.50, Epsilon: 0.929\n",
      "Episode 148, Total Reward: -1077.00, Epsilon: 0.929\n",
      "Episode 149, Total Reward: 649.00, Epsilon: 0.928\n",
      "Episode 150, Total Reward: -1297.50, Epsilon: 0.928\n",
      "Episode 151, Total Reward: -1953.50, Epsilon: 0.927\n",
      "Episode 152, Total Reward: -790.50, Epsilon: 0.927\n",
      "Episode 153, Total Reward: -2154.50, Epsilon: 0.926\n",
      "Episode 154, Total Reward: -3837.50, Epsilon: 0.926\n",
      "Episode 155, Total Reward: -801.00, Epsilon: 0.925\n",
      "Episode 156, Total Reward: -620.00, Epsilon: 0.925\n",
      "Episode 157, Total Reward: -1851.50, Epsilon: 0.924\n",
      "Episode 158, Total Reward: -1815.00, Epsilon: 0.924\n",
      "Episode 159, Total Reward: -680.00, Epsilon: 0.924\n",
      "Episode 160, Total Reward: -2181.50, Epsilon: 0.923\n",
      "Episode 161, Total Reward: -1025.00, Epsilon: 0.923\n",
      "Episode 162, Total Reward: -264.00, Epsilon: 0.922\n",
      "Episode 163, Total Reward: -2101.00, Epsilon: 0.922\n",
      "Episode 164, Total Reward: -1451.00, Epsilon: 0.921\n",
      "Episode 165, Total Reward: -1114.00, Epsilon: 0.921\n",
      "Episode 166, Total Reward: -2211.50, Epsilon: 0.920\n",
      "Episode 167, Total Reward: -1620.50, Epsilon: 0.920\n",
      "Episode 168, Total Reward: -2812.50, Epsilon: 0.919\n",
      "Episode 169, Total Reward: -1579.00, Epsilon: 0.919\n",
      "Episode 170, Total Reward: -2054.50, Epsilon: 0.918\n",
      "Episode 171, Total Reward: -713.00, Epsilon: 0.918\n",
      "Episode 172, Total Reward: -1102.00, Epsilon: 0.918\n",
      "Episode 173, Total Reward: -762.00, Epsilon: 0.917\n",
      "Episode 174, Total Reward: -669.50, Epsilon: 0.917\n",
      "Episode 175, Total Reward: -1401.50, Epsilon: 0.916\n",
      "Episode 176, Total Reward: 37.50, Epsilon: 0.916\n",
      "Episode 177, Total Reward: 300.50, Epsilon: 0.915\n",
      "Episode 178, Total Reward: 389.50, Epsilon: 0.915\n",
      "Episode 179, Total Reward: -1290.00, Epsilon: 0.914\n",
      "Episode 180, Total Reward: -971.00, Epsilon: 0.914\n",
      "Episode 181, Total Reward: -802.50, Epsilon: 0.913\n",
      "Episode 182, Total Reward: 113.50, Epsilon: 0.913\n",
      "Episode 183, Total Reward: -3012.50, Epsilon: 0.913\n",
      "Episode 184, Total Reward: -1273.50, Epsilon: 0.912\n",
      "Episode 185, Total Reward: -732.50, Epsilon: 0.912\n",
      "Episode 186, Total Reward: -1290.50, Epsilon: 0.911\n",
      "Episode 187, Total Reward: -1522.50, Epsilon: 0.911\n",
      "Episode 188, Total Reward: -1383.00, Epsilon: 0.910\n",
      "Episode 189, Total Reward: -126.50, Epsilon: 0.910\n",
      "Episode 190, Total Reward: -942.50, Epsilon: 0.909\n",
      "Episode 191, Total Reward: -516.00, Epsilon: 0.909\n",
      "Episode 192, Total Reward: 523.50, Epsilon: 0.908\n",
      "Episode 193, Total Reward: -895.00, Epsilon: 0.908\n",
      "Episode 194, Total Reward: -258.50, Epsilon: 0.908\n",
      "Episode 195, Total Reward: -1532.50, Epsilon: 0.907\n",
      "Episode 196, Total Reward: -1164.50, Epsilon: 0.907\n",
      "Episode 197, Total Reward: -236.50, Epsilon: 0.906\n",
      "Episode 198, Total Reward: -1299.50, Epsilon: 0.906\n",
      "Episode 199, Total Reward: -1439.50, Epsilon: 0.905\n",
      "Episode 200, Total Reward: -1040.50, Epsilon: 0.905\n",
      "Episode 201, Total Reward: -1375.50, Epsilon: 0.904\n",
      "Episode 202, Total Reward: -1804.00, Epsilon: 0.904\n",
      "Episode 203, Total Reward: 442.00, Epsilon: 0.903\n",
      "Episode 204, Total Reward: -587.00, Epsilon: 0.903\n",
      "Episode 205, Total Reward: -668.50, Epsilon: 0.903\n",
      "Episode 206, Total Reward: -536.50, Epsilon: 0.902\n",
      "Episode 207, Total Reward: -3.50, Epsilon: 0.902\n",
      "Episode 208, Total Reward: 242.00, Epsilon: 0.901\n",
      "Episode 209, Total Reward: -1500.00, Epsilon: 0.901\n",
      "Episode 210, Total Reward: -802.00, Epsilon: 0.900\n",
      "Episode 211, Total Reward: -1014.00, Epsilon: 0.900\n",
      "Episode 212, Total Reward: -658.00, Epsilon: 0.899\n",
      "Episode 213, Total Reward: -1411.50, Epsilon: 0.899\n",
      "Episode 214, Total Reward: -1409.00, Epsilon: 0.899\n",
      "Episode 215, Total Reward: -2800.50, Epsilon: 0.898\n",
      "Episode 216, Total Reward: -634.50, Epsilon: 0.898\n",
      "Episode 217, Total Reward: -1748.50, Epsilon: 0.897\n",
      "Episode 218, Total Reward: -1111.50, Epsilon: 0.897\n",
      "Episode 219, Total Reward: -1320.50, Epsilon: 0.896\n",
      "Episode 220, Total Reward: -56.50, Epsilon: 0.896\n",
      "Episode 221, Total Reward: -1246.50, Epsilon: 0.895\n",
      "Episode 222, Total Reward: 209.50, Epsilon: 0.895\n",
      "Episode 223, Total Reward: -1230.50, Epsilon: 0.894\n",
      "Episode 224, Total Reward: -342.50, Epsilon: 0.894\n",
      "Episode 225, Total Reward: -696.50, Epsilon: 0.894\n",
      "Episode 226, Total Reward: -586.50, Epsilon: 0.893\n",
      "Episode 227, Total Reward: -654.50, Epsilon: 0.893\n",
      "Episode 228, Total Reward: 367.50, Epsilon: 0.892\n",
      "Episode 229, Total Reward: 1251.50, Epsilon: 0.892\n",
      "Episode 230, Total Reward: -490.00, Epsilon: 0.891\n",
      "Episode 231, Total Reward: -2236.00, Epsilon: 0.891\n",
      "Episode 232, Total Reward: 1207.00, Epsilon: 0.890\n",
      "Episode 233, Total Reward: -750.50, Epsilon: 0.890\n",
      "Episode 234, Total Reward: 871.00, Epsilon: 0.890\n",
      "Episode 235, Total Reward: 1142.00, Epsilon: 0.889\n",
      "Episode 236, Total Reward: -1964.50, Epsilon: 0.889\n",
      "Episode 237, Total Reward: -929.00, Epsilon: 0.888\n",
      "Episode 238, Total Reward: -1591.00, Epsilon: 0.888\n",
      "Episode 239, Total Reward: 183.50, Epsilon: 0.887\n",
      "Episode 240, Total Reward: -727.00, Epsilon: 0.887\n",
      "Episode 241, Total Reward: 445.50, Epsilon: 0.886\n",
      "Episode 242, Total Reward: -1550.50, Epsilon: 0.886\n",
      "Episode 243, Total Reward: -454.50, Epsilon: 0.886\n",
      "Episode 244, Total Reward: -980.50, Epsilon: 0.885\n",
      "Episode 245, Total Reward: -1493.00, Epsilon: 0.885\n",
      "Episode 246, Total Reward: -948.50, Epsilon: 0.884\n",
      "Episode 247, Total Reward: -449.00, Epsilon: 0.884\n",
      "Episode 248, Total Reward: -1021.00, Epsilon: 0.883\n",
      "Episode 249, Total Reward: 120.00, Epsilon: 0.883\n",
      "Episode 250, Total Reward: 889.00, Epsilon: 0.882\n",
      "Episode 251, Total Reward: -1366.00, Epsilon: 0.882\n",
      "Episode 252, Total Reward: -1808.50, Epsilon: 0.882\n",
      "Episode 253, Total Reward: 150.00, Epsilon: 0.881\n",
      "Episode 254, Total Reward: -220.50, Epsilon: 0.881\n",
      "Episode 255, Total Reward: 577.00, Epsilon: 0.880\n",
      "Episode 256, Total Reward: -478.00, Epsilon: 0.880\n",
      "Episode 257, Total Reward: 329.00, Epsilon: 0.879\n",
      "Episode 258, Total Reward: 875.50, Epsilon: 0.879\n",
      "Episode 259, Total Reward: -564.50, Epsilon: 0.879\n",
      "Episode 260, Total Reward: -496.50, Epsilon: 0.878\n",
      "Episode 261, Total Reward: -133.00, Epsilon: 0.878\n",
      "Episode 262, Total Reward: -472.50, Epsilon: 0.877\n",
      "Episode 263, Total Reward: 629.50, Epsilon: 0.877\n",
      "Episode 264, Total Reward: 302.00, Epsilon: 0.876\n",
      "Episode 265, Total Reward: -402.50, Epsilon: 0.876\n",
      "Episode 266, Total Reward: -708.50, Epsilon: 0.875\n",
      "Episode 267, Total Reward: -950.00, Epsilon: 0.875\n",
      "Episode 268, Total Reward: -1355.00, Epsilon: 0.875\n",
      "Episode 269, Total Reward: -209.00, Epsilon: 0.874\n",
      "Episode 270, Total Reward: -1041.00, Epsilon: 0.874\n",
      "Episode 271, Total Reward: 1070.00, Epsilon: 0.873\n",
      "Episode 272, Total Reward: 55.50, Epsilon: 0.873\n",
      "Episode 273, Total Reward: -380.50, Epsilon: 0.872\n",
      "Episode 274, Total Reward: -49.50, Epsilon: 0.872\n",
      "Episode 275, Total Reward: -752.50, Epsilon: 0.872\n",
      "Episode 276, Total Reward: -1242.50, Epsilon: 0.871\n",
      "Episode 277, Total Reward: 498.50, Epsilon: 0.871\n",
      "Episode 278, Total Reward: -1570.50, Epsilon: 0.870\n",
      "Episode 279, Total Reward: -1384.50, Epsilon: 0.870\n",
      "Episode 280, Total Reward: 624.00, Epsilon: 0.869\n",
      "Episode 281, Total Reward: -324.50, Epsilon: 0.869\n",
      "Episode 282, Total Reward: -1429.50, Epsilon: 0.868\n",
      "Episode 283, Total Reward: -458.50, Epsilon: 0.868\n",
      "Episode 284, Total Reward: -628.50, Epsilon: 0.868\n",
      "Episode 285, Total Reward: -420.50, Epsilon: 0.867\n",
      "Episode 286, Total Reward: -532.00, Epsilon: 0.867\n",
      "Episode 287, Total Reward: -1907.50, Epsilon: 0.866\n",
      "Episode 288, Total Reward: -1678.50, Epsilon: 0.866\n",
      "Episode 289, Total Reward: -277.00, Epsilon: 0.865\n",
      "Episode 290, Total Reward: -88.50, Epsilon: 0.865\n",
      "Episode 291, Total Reward: -440.50, Epsilon: 0.865\n",
      "Episode 292, Total Reward: -410.50, Epsilon: 0.864\n",
      "Episode 293, Total Reward: -1402.00, Epsilon: 0.864\n",
      "Episode 294, Total Reward: 1543.50, Epsilon: 0.863\n",
      "Episode 295, Total Reward: -1193.50, Epsilon: 0.863\n",
      "Episode 296, Total Reward: -1951.00, Epsilon: 0.862\n",
      "Episode 297, Total Reward: -619.00, Epsilon: 0.862\n",
      "Episode 298, Total Reward: 351.50, Epsilon: 0.862\n",
      "Episode 299, Total Reward: 691.50, Epsilon: 0.861\n",
      "Episode 300, Total Reward: -1460.50, Epsilon: 0.861\n",
      "Episode 301, Total Reward: 1503.00, Epsilon: 0.860\n",
      "Episode 302, Total Reward: -24.50, Epsilon: 0.860\n",
      "Episode 303, Total Reward: 189.50, Epsilon: 0.859\n",
      "Episode 304, Total Reward: -766.50, Epsilon: 0.859\n",
      "Episode 305, Total Reward: 323.50, Epsilon: 0.859\n",
      "Episode 306, Total Reward: -716.50, Epsilon: 0.858\n",
      "Episode 307, Total Reward: -408.50, Epsilon: 0.858\n",
      "Episode 308, Total Reward: 77.50, Epsilon: 0.857\n",
      "Episode 309, Total Reward: -1222.50, Epsilon: 0.857\n",
      "Episode 310, Total Reward: -288.50, Epsilon: 0.856\n",
      "Episode 311, Total Reward: 151.50, Epsilon: 0.856\n",
      "Episode 312, Total Reward: -1911.00, Epsilon: 0.856\n",
      "Episode 313, Total Reward: -487.00, Epsilon: 0.855\n",
      "Episode 314, Total Reward: -1237.50, Epsilon: 0.855\n",
      "Episode 315, Total Reward: 167.00, Epsilon: 0.854\n",
      "Episode 316, Total Reward: -300.50, Epsilon: 0.854\n",
      "Episode 317, Total Reward: -584.50, Epsilon: 0.853\n",
      "Episode 318, Total Reward: -562.50, Epsilon: 0.853\n",
      "Episode 319, Total Reward: -1240.50, Epsilon: 0.853\n",
      "Episode 320, Total Reward: -584.50, Epsilon: 0.852\n",
      "Episode 321, Total Reward: -271.50, Epsilon: 0.852\n",
      "Episode 322, Total Reward: 365.50, Epsilon: 0.851\n",
      "Episode 323, Total Reward: 435.50, Epsilon: 0.851\n",
      "Episode 324, Total Reward: 491.50, Epsilon: 0.850\n",
      "Episode 325, Total Reward: 928.50, Epsilon: 0.850\n",
      "Episode 326, Total Reward: -772.50, Epsilon: 0.850\n",
      "Episode 327, Total Reward: 968.00, Epsilon: 0.849\n",
      "Episode 328, Total Reward: -1912.50, Epsilon: 0.849\n",
      "Episode 329, Total Reward: -596.50, Epsilon: 0.848\n",
      "Episode 330, Total Reward: 265.50, Epsilon: 0.848\n",
      "Episode 331, Total Reward: -640.50, Epsilon: 0.847\n",
      "Episode 332, Total Reward: -323.00, Epsilon: 0.847\n",
      "Episode 333, Total Reward: 45.50, Epsilon: 0.847\n",
      "Episode 334, Total Reward: -1002.00, Epsilon: 0.846\n",
      "Episode 335, Total Reward: -32.00, Epsilon: 0.846\n",
      "Episode 336, Total Reward: 55.50, Epsilon: 0.845\n",
      "Episode 337, Total Reward: -196.50, Epsilon: 0.845\n",
      "Episode 338, Total Reward: 600.00, Epsilon: 0.844\n",
      "Episode 339, Total Reward: -419.50, Epsilon: 0.844\n",
      "Episode 340, Total Reward: 795.50, Epsilon: 0.844\n",
      "Episode 341, Total Reward: -1214.50, Epsilon: 0.843\n",
      "Episode 342, Total Reward: -63.00, Epsilon: 0.843\n",
      "Episode 343, Total Reward: -131.00, Epsilon: 0.842\n",
      "Episode 344, Total Reward: -918.00, Epsilon: 0.842\n",
      "Episode 345, Total Reward: 862.00, Epsilon: 0.842\n",
      "Episode 346, Total Reward: -50.50, Epsilon: 0.841\n",
      "Episode 347, Total Reward: 311.50, Epsilon: 0.841\n",
      "Episode 348, Total Reward: -1140.50, Epsilon: 0.840\n",
      "Episode 349, Total Reward: -360.50, Epsilon: 0.840\n",
      "Episode 350, Total Reward: -889.00, Epsilon: 0.839\n",
      "Episode 351, Total Reward: 369.50, Epsilon: 0.839\n",
      "Episode 352, Total Reward: -831.50, Epsilon: 0.839\n",
      "Episode 353, Total Reward: -196.50, Epsilon: 0.838\n",
      "Episode 354, Total Reward: -248.00, Epsilon: 0.838\n",
      "Episode 355, Total Reward: -308.00, Epsilon: 0.837\n",
      "Episode 356, Total Reward: -880.50, Epsilon: 0.837\n",
      "Episode 357, Total Reward: 273.50, Epsilon: 0.836\n",
      "Episode 358, Total Reward: -1988.50, Epsilon: 0.836\n",
      "Episode 359, Total Reward: -296.50, Epsilon: 0.836\n",
      "Episode 360, Total Reward: 340.00, Epsilon: 0.835\n",
      "Episode 361, Total Reward: -858.00, Epsilon: 0.835\n",
      "Episode 362, Total Reward: 123.50, Epsilon: 0.834\n",
      "Episode 363, Total Reward: 241.50, Epsilon: 0.834\n",
      "Episode 364, Total Reward: -426.50, Epsilon: 0.834\n",
      "Episode 365, Total Reward: 39.50, Epsilon: 0.833\n",
      "Episode 366, Total Reward: 358.50, Epsilon: 0.833\n",
      "Episode 367, Total Reward: 108.50, Epsilon: 0.832\n",
      "Episode 368, Total Reward: 167.50, Epsilon: 0.832\n",
      "Episode 369, Total Reward: 1247.50, Epsilon: 0.831\n",
      "Episode 370, Total Reward: 522.50, Epsilon: 0.831\n",
      "Episode 371, Total Reward: 29.50, Epsilon: 0.831\n",
      "Episode 372, Total Reward: -884.50, Epsilon: 0.830\n",
      "Episode 373, Total Reward: 1469.00, Epsilon: 0.830\n",
      "Episode 374, Total Reward: -384.50, Epsilon: 0.829\n",
      "Episode 375, Total Reward: -366.50, Epsilon: 0.829\n",
      "Episode 376, Total Reward: 589.00, Epsilon: 0.829\n",
      "Episode 377, Total Reward: -462.50, Epsilon: 0.828\n",
      "Episode 378, Total Reward: -2110.50, Epsilon: 0.828\n",
      "Episode 379, Total Reward: 700.00, Epsilon: 0.827\n",
      "Episode 380, Total Reward: -34.50, Epsilon: 0.827\n",
      "Episode 381, Total Reward: -100.50, Epsilon: 0.827\n",
      "Episode 382, Total Reward: -408.50, Epsilon: 0.826\n",
      "Episode 383, Total Reward: -230.50, Epsilon: 0.826\n",
      "Episode 384, Total Reward: -166.50, Epsilon: 0.825\n",
      "Episode 385, Total Reward: -29.00, Epsilon: 0.825\n",
      "Episode 386, Total Reward: -402.50, Epsilon: 0.824\n",
      "Episode 387, Total Reward: -682.50, Epsilon: 0.824\n",
      "Episode 388, Total Reward: -543.00, Epsilon: 0.824\n",
      "Episode 389, Total Reward: -883.50, Epsilon: 0.823\n",
      "Episode 390, Total Reward: 489.50, Epsilon: 0.823\n",
      "Episode 391, Total Reward: 1359.00, Epsilon: 0.822\n",
      "Episode 392, Total Reward: 397.50, Epsilon: 0.822\n",
      "Episode 393, Total Reward: 469.00, Epsilon: 0.822\n",
      "Episode 394, Total Reward: -756.50, Epsilon: 0.821\n",
      "Episode 395, Total Reward: 726.00, Epsilon: 0.821\n",
      "Episode 396, Total Reward: 205.50, Epsilon: 0.820\n",
      "Episode 397, Total Reward: 1179.00, Epsilon: 0.820\n",
      "Episode 398, Total Reward: 637.50, Epsilon: 0.820\n",
      "Episode 399, Total Reward: 947.50, Epsilon: 0.819\n",
      "Episode 400, Total Reward: 923.50, Epsilon: 0.819\n",
      "Episode 401, Total Reward: 183.50, Epsilon: 0.818\n",
      "Episode 402, Total Reward: 953.50, Epsilon: 0.818\n",
      "Episode 403, Total Reward: -606.50, Epsilon: 0.817\n",
      "Episode 404, Total Reward: -294.50, Epsilon: 0.817\n",
      "Episode 405, Total Reward: -426.50, Epsilon: 0.817\n",
      "Episode 406, Total Reward: 733.50, Epsilon: 0.816\n",
      "Episode 407, Total Reward: -62.50, Epsilon: 0.816\n",
      "Episode 408, Total Reward: 363.50, Epsilon: 0.815\n",
      "Episode 409, Total Reward: 467.50, Epsilon: 0.815\n",
      "Episode 410, Total Reward: 8.00, Epsilon: 0.815\n",
      "Episode 411, Total Reward: -520.50, Epsilon: 0.814\n",
      "Episode 412, Total Reward: -48.50, Epsilon: 0.814\n",
      "Episode 413, Total Reward: 249.50, Epsilon: 0.813\n",
      "Episode 414, Total Reward: -924.50, Epsilon: 0.813\n",
      "Episode 415, Total Reward: 643.50, Epsilon: 0.813\n",
      "Episode 416, Total Reward: 333.50, Epsilon: 0.812\n",
      "Episode 417, Total Reward: 331.50, Epsilon: 0.812\n",
      "Episode 418, Total Reward: 495.50, Epsilon: 0.811\n",
      "Episode 419, Total Reward: 405.50, Epsilon: 0.811\n",
      "Episode 420, Total Reward: 1155.50, Epsilon: 0.811\n",
      "Episode 421, Total Reward: -222.50, Epsilon: 0.810\n",
      "Episode 422, Total Reward: -656.50, Epsilon: 0.810\n",
      "Episode 423, Total Reward: -996.50, Epsilon: 0.809\n",
      "Episode 424, Total Reward: 842.50, Epsilon: 0.809\n",
      "Episode 425, Total Reward: 21.50, Epsilon: 0.809\n",
      "Episode 426, Total Reward: -1918.50, Epsilon: 0.808\n",
      "Episode 427, Total Reward: -163.50, Epsilon: 0.808\n",
      "Episode 428, Total Reward: -622.50, Epsilon: 0.807\n",
      "Episode 429, Total Reward: -378.00, Epsilon: 0.807\n",
      "Episode 430, Total Reward: 839.50, Epsilon: 0.806\n",
      "Episode 431, Total Reward: -11.00, Epsilon: 0.806\n",
      "Episode 432, Total Reward: -74.50, Epsilon: 0.806\n",
      "Episode 433, Total Reward: -1348.00, Epsilon: 0.805\n",
      "Episode 434, Total Reward: 562.00, Epsilon: 0.805\n",
      "Episode 435, Total Reward: -1521.00, Epsilon: 0.804\n",
      "Episode 436, Total Reward: 290.00, Epsilon: 0.804\n",
      "Episode 437, Total Reward: 289.50, Epsilon: 0.804\n",
      "Episode 438, Total Reward: 599.50, Epsilon: 0.803\n",
      "Episode 439, Total Reward: 802.00, Epsilon: 0.803\n",
      "Episode 440, Total Reward: -276.50, Epsilon: 0.802\n",
      "Episode 441, Total Reward: 695.50, Epsilon: 0.802\n",
      "Episode 442, Total Reward: 1442.50, Epsilon: 0.802\n",
      "Episode 443, Total Reward: -110.00, Epsilon: 0.801\n",
      "Episode 444, Total Reward: -1580.50, Epsilon: 0.801\n",
      "Episode 445, Total Reward: 282.00, Epsilon: 0.800\n",
      "Episode 446, Total Reward: 131.50, Epsilon: 0.800\n",
      "Episode 447, Total Reward: 1911.00, Epsilon: 0.800\n",
      "Episode 448, Total Reward: 1991.50, Epsilon: 0.799\n",
      "Episode 449, Total Reward: -1528.00, Epsilon: 0.799\n",
      "Episode 450, Total Reward: 421.00, Epsilon: 0.798\n",
      "Episode 451, Total Reward: -1361.00, Epsilon: 0.798\n",
      "Episode 452, Total Reward: 1622.50, Epsilon: 0.798\n",
      "Episode 453, Total Reward: 992.50, Epsilon: 0.797\n",
      "Episode 454, Total Reward: -517.50, Epsilon: 0.797\n",
      "Episode 455, Total Reward: 1151.50, Epsilon: 0.796\n",
      "Episode 456, Total Reward: -50.50, Epsilon: 0.796\n",
      "Episode 457, Total Reward: -482.00, Epsilon: 0.796\n",
      "Episode 458, Total Reward: 1311.50, Epsilon: 0.795\n",
      "Episode 459, Total Reward: -540.00, Epsilon: 0.795\n",
      "Episode 460, Total Reward: 1067.50, Epsilon: 0.794\n",
      "Episode 461, Total Reward: 740.00, Epsilon: 0.794\n",
      "Episode 462, Total Reward: -54.50, Epsilon: 0.794\n",
      "Episode 463, Total Reward: 1258.50, Epsilon: 0.793\n",
      "Episode 464, Total Reward: 259.50, Epsilon: 0.793\n",
      "Episode 465, Total Reward: 991.50, Epsilon: 0.793\n",
      "Episode 466, Total Reward: 52.50, Epsilon: 0.792\n",
      "Episode 467, Total Reward: 2559.50, Epsilon: 0.792\n",
      "Episode 468, Total Reward: -464.50, Epsilon: 0.791\n",
      "Episode 469, Total Reward: 351.00, Epsilon: 0.791\n",
      "Episode 470, Total Reward: 1700.00, Epsilon: 0.791\n",
      "Episode 471, Total Reward: 1597.00, Epsilon: 0.790\n",
      "Episode 472, Total Reward: -631.50, Epsilon: 0.790\n",
      "Episode 473, Total Reward: 701.00, Epsilon: 0.789\n",
      "Episode 474, Total Reward: 1145.50, Epsilon: 0.789\n",
      "Episode 475, Total Reward: -2162.00, Epsilon: 0.789\n",
      "Episode 476, Total Reward: -1392.00, Epsilon: 0.788\n",
      "Episode 477, Total Reward: 1945.50, Epsilon: 0.788\n",
      "Episode 478, Total Reward: 2100.00, Epsilon: 0.787\n",
      "Episode 479, Total Reward: 2081.50, Epsilon: 0.787\n",
      "Episode 480, Total Reward: 217.50, Epsilon: 0.787\n",
      "Episode 481, Total Reward: 1777.50, Epsilon: 0.786\n",
      "Episode 482, Total Reward: 240.50, Epsilon: 0.786\n",
      "Episode 483, Total Reward: -359.00, Epsilon: 0.785\n",
      "Episode 484, Total Reward: 1231.00, Epsilon: 0.785\n",
      "Episode 485, Total Reward: 416.00, Epsilon: 0.785\n",
      "Episode 486, Total Reward: -349.00, Epsilon: 0.784\n",
      "Episode 487, Total Reward: 1259.50, Epsilon: 0.784\n",
      "Episode 488, Total Reward: 2688.50, Epsilon: 0.783\n",
      "Episode 489, Total Reward: 1081.50, Epsilon: 0.783\n",
      "Episode 490, Total Reward: 112.00, Epsilon: 0.783\n",
      "Episode 491, Total Reward: -1129.00, Epsilon: 0.782\n",
      "Episode 492, Total Reward: 980.00, Epsilon: 0.782\n",
      "Episode 493, Total Reward: 871.00, Epsilon: 0.781\n",
      "Episode 494, Total Reward: -286.00, Epsilon: 0.781\n",
      "Episode 495, Total Reward: 911.00, Epsilon: 0.781\n",
      "Episode 496, Total Reward: 799.50, Epsilon: 0.780\n",
      "Episode 497, Total Reward: -159.50, Epsilon: 0.780\n",
      "Episode 498, Total Reward: 1679.50, Epsilon: 0.780\n",
      "Episode 499, Total Reward: 489.50, Epsilon: 0.779\n",
      "Episode 500, Total Reward: -560.00, Epsilon: 0.779\n",
      "Episode 501, Total Reward: 709.00, Epsilon: 0.778\n",
      "Episode 502, Total Reward: 3130.00, Epsilon: 0.778\n",
      "Episode 503, Total Reward: 3277.50, Epsilon: 0.778\n",
      "Episode 504, Total Reward: 3055.50, Epsilon: 0.777\n",
      "Episode 505, Total Reward: -531.50, Epsilon: 0.777\n",
      "Episode 506, Total Reward: 1100.50, Epsilon: 0.776\n",
      "Episode 507, Total Reward: 480.50, Epsilon: 0.776\n",
      "Episode 508, Total Reward: 1310.50, Epsilon: 0.776\n",
      "Episode 509, Total Reward: 369.50, Epsilon: 0.775\n",
      "Episode 510, Total Reward: 630.00, Epsilon: 0.775\n",
      "Episode 511, Total Reward: -450.00, Epsilon: 0.774\n",
      "Episode 512, Total Reward: -731.00, Epsilon: 0.774\n",
      "Episode 513, Total Reward: -802.50, Epsilon: 0.774\n",
      "Episode 514, Total Reward: 80.00, Epsilon: 0.773\n",
      "Episode 515, Total Reward: -1141.00, Epsilon: 0.773\n",
      "Episode 516, Total Reward: -1150.50, Epsilon: 0.773\n",
      "Episode 517, Total Reward: 2040.00, Epsilon: 0.772\n",
      "Episode 518, Total Reward: 310.00, Epsilon: 0.772\n",
      "Episode 519, Total Reward: -418.50, Epsilon: 0.771\n",
      "Episode 520, Total Reward: 1899.00, Epsilon: 0.771\n",
      "Episode 521, Total Reward: 88.00, Epsilon: 0.771\n",
      "Episode 522, Total Reward: -639.00, Epsilon: 0.770\n",
      "Episode 523, Total Reward: -379.50, Epsilon: 0.770\n",
      "Episode 524, Total Reward: -299.50, Epsilon: 0.769\n",
      "Episode 525, Total Reward: 588.50, Epsilon: 0.769\n",
      "Episode 526, Total Reward: -440.50, Epsilon: 0.769\n",
      "Episode 527, Total Reward: 789.00, Epsilon: 0.768\n",
      "Episode 528, Total Reward: 341.00, Epsilon: 0.768\n",
      "Episode 529, Total Reward: 1349.00, Epsilon: 0.768\n",
      "Episode 530, Total Reward: 369.00, Epsilon: 0.767\n",
      "Episode 531, Total Reward: -742.00, Epsilon: 0.767\n",
      "Episode 532, Total Reward: 1501.50, Epsilon: 0.766\n",
      "Episode 533, Total Reward: -131.00, Epsilon: 0.766\n",
      "Episode 534, Total Reward: 321.50, Epsilon: 0.766\n",
      "Episode 535, Total Reward: -549.00, Epsilon: 0.765\n",
      "Episode 536, Total Reward: 1290.00, Epsilon: 0.765\n",
      "Episode 537, Total Reward: -1159.00, Epsilon: 0.764\n",
      "Episode 538, Total Reward: 701.50, Epsilon: 0.764\n",
      "Episode 539, Total Reward: -762.50, Epsilon: 0.764\n",
      "Episode 540, Total Reward: -720.00, Epsilon: 0.763\n",
      "Episode 541, Total Reward: 1649.50, Epsilon: 0.763\n",
      "Episode 542, Total Reward: 241.50, Epsilon: 0.763\n",
      "Episode 543, Total Reward: 1689.00, Epsilon: 0.762\n",
      "Episode 544, Total Reward: -88.50, Epsilon: 0.762\n",
      "Episode 545, Total Reward: -1399.50, Epsilon: 0.761\n",
      "Episode 546, Total Reward: -1771.50, Epsilon: 0.761\n",
      "Episode 547, Total Reward: 719.50, Epsilon: 0.761\n",
      "Episode 548, Total Reward: 3051.00, Epsilon: 0.760\n",
      "Episode 549, Total Reward: -400.00, Epsilon: 0.760\n",
      "Episode 550, Total Reward: 740.00, Epsilon: 0.760\n",
      "Episode 551, Total Reward: 129.00, Epsilon: 0.759\n",
      "Episode 552, Total Reward: -358.50, Epsilon: 0.759\n",
      "Episode 553, Total Reward: 797.50, Epsilon: 0.758\n",
      "Episode 554, Total Reward: 200.00, Epsilon: 0.758\n",
      "Episode 555, Total Reward: 798.00, Epsilon: 0.758\n",
      "Episode 556, Total Reward: -239.00, Epsilon: 0.757\n",
      "Episode 557, Total Reward: 1520.50, Epsilon: 0.757\n",
      "Episode 558, Total Reward: 943.50, Epsilon: 0.756\n",
      "Episode 559, Total Reward: 1339.00, Epsilon: 0.756\n",
      "Episode 560, Total Reward: -709.00, Epsilon: 0.756\n",
      "Episode 561, Total Reward: 1240.00, Epsilon: 0.755\n",
      "Episode 562, Total Reward: 1730.50, Epsilon: 0.755\n",
      "Episode 563, Total Reward: 2101.00, Epsilon: 0.755\n",
      "Episode 564, Total Reward: 340.00, Epsilon: 0.754\n",
      "Episode 565, Total Reward: 935.50, Epsilon: 0.754\n",
      "Episode 566, Total Reward: 2041.50, Epsilon: 0.753\n",
      "Episode 567, Total Reward: 2091.00, Epsilon: 0.753\n",
      "Episode 568, Total Reward: -1420.00, Epsilon: 0.753\n",
      "Episode 569, Total Reward: -154.50, Epsilon: 0.752\n",
      "Episode 570, Total Reward: 2201.00, Epsilon: 0.752\n",
      "Episode 571, Total Reward: 1400.50, Epsilon: 0.752\n",
      "Episode 572, Total Reward: 1028.50, Epsilon: 0.751\n",
      "Episode 573, Total Reward: -91.00, Epsilon: 0.751\n",
      "Episode 574, Total Reward: 1520.00, Epsilon: 0.750\n",
      "Episode 575, Total Reward: 1942.00, Epsilon: 0.750\n",
      "Episode 576, Total Reward: 2479.50, Epsilon: 0.750\n",
      "Episode 577, Total Reward: 1081.50, Epsilon: 0.749\n",
      "Episode 578, Total Reward: -650.50, Epsilon: 0.749\n",
      "Episode 579, Total Reward: 619.50, Epsilon: 0.749\n",
      "Episode 580, Total Reward: -701.00, Epsilon: 0.748\n",
      "Episode 581, Total Reward: 1550.50, Epsilon: 0.748\n",
      "Episode 582, Total Reward: -300.00, Epsilon: 0.747\n",
      "Episode 583, Total Reward: 3419.50, Epsilon: 0.747\n",
      "Episode 584, Total Reward: 1740.00, Epsilon: 0.747\n",
      "Episode 585, Total Reward: 730.50, Epsilon: 0.746\n",
      "Episode 586, Total Reward: 2300.50, Epsilon: 0.746\n",
      "Episode 587, Total Reward: 1921.00, Epsilon: 0.746\n",
      "Episode 588, Total Reward: 1660.50, Epsilon: 0.745\n",
      "Episode 589, Total Reward: 3162.00, Epsilon: 0.745\n",
      "Episode 590, Total Reward: 1201.50, Epsilon: 0.744\n",
      "Episode 591, Total Reward: 1253.50, Epsilon: 0.744\n",
      "Episode 592, Total Reward: -908.50, Epsilon: 0.744\n",
      "Episode 593, Total Reward: 458.50, Epsilon: 0.743\n",
      "Episode 594, Total Reward: 2441.50, Epsilon: 0.743\n",
      "Episode 595, Total Reward: 1040.50, Epsilon: 0.743\n",
      "Episode 596, Total Reward: 1410.00, Epsilon: 0.742\n",
      "Episode 597, Total Reward: 926.00, Epsilon: 0.742\n",
      "Episode 598, Total Reward: 1918.50, Epsilon: 0.742\n",
      "Episode 599, Total Reward: 548.00, Epsilon: 0.741\n",
      "Episode 600, Total Reward: 418.50, Epsilon: 0.741\n",
      "Episode 601, Total Reward: 1281.50, Epsilon: 0.740\n",
      "Episode 602, Total Reward: -1261.00, Epsilon: 0.740\n",
      "Episode 603, Total Reward: 709.00, Epsilon: 0.740\n",
      "Episode 604, Total Reward: -350.00, Epsilon: 0.739\n",
      "Episode 605, Total Reward: 589.00, Epsilon: 0.739\n",
      "Episode 606, Total Reward: 1221.50, Epsilon: 0.739\n",
      "Episode 607, Total Reward: 2421.00, Epsilon: 0.738\n",
      "Episode 608, Total Reward: 1620.50, Epsilon: 0.738\n",
      "Episode 609, Total Reward: -838.00, Epsilon: 0.737\n",
      "Episode 610, Total Reward: 1172.50, Epsilon: 0.737\n",
      "Episode 611, Total Reward: 2081.50, Epsilon: 0.737\n",
      "Episode 612, Total Reward: -429.00, Epsilon: 0.736\n",
      "Episode 613, Total Reward: 2871.00, Epsilon: 0.736\n",
      "Episode 614, Total Reward: 2338.00, Epsilon: 0.736\n",
      "Episode 615, Total Reward: 1030.00, Epsilon: 0.735\n",
      "Episode 616, Total Reward: 821.50, Epsilon: 0.735\n",
      "Episode 617, Total Reward: 3391.00, Epsilon: 0.734\n",
      "Episode 618, Total Reward: 191.00, Epsilon: 0.734\n",
      "Episode 619, Total Reward: 1401.50, Epsilon: 0.734\n",
      "Episode 620, Total Reward: 3202.50, Epsilon: 0.733\n",
      "Episode 621, Total Reward: -429.00, Epsilon: 0.733\n",
      "Episode 622, Total Reward: 1519.00, Epsilon: 0.733\n",
      "Episode 623, Total Reward: 1671.00, Epsilon: 0.732\n",
      "Episode 624, Total Reward: 2681.50, Epsilon: 0.732\n",
      "Episode 625, Total Reward: 3322.00, Epsilon: 0.732\n",
      "Episode 626, Total Reward: 1548.50, Epsilon: 0.731\n",
      "Episode 627, Total Reward: 2101.00, Epsilon: 0.731\n",
      "Episode 628, Total Reward: 1947.50, Epsilon: 0.730\n",
      "Episode 629, Total Reward: 1791.00, Epsilon: 0.730\n",
      "Episode 630, Total Reward: 3001.50, Epsilon: 0.730\n",
      "Episode 631, Total Reward: 2860.00, Epsilon: 0.729\n",
      "Episode 632, Total Reward: 2952.00, Epsilon: 0.729\n",
      "Episode 633, Total Reward: 1280.00, Epsilon: 0.729\n",
      "Episode 634, Total Reward: 3340.00, Epsilon: 0.728\n",
      "Episode 635, Total Reward: 2121.50, Epsilon: 0.728\n",
      "Episode 636, Total Reward: 4181.50, Epsilon: 0.728\n",
      "Episode 637, Total Reward: 607.50, Epsilon: 0.727\n",
      "Episode 638, Total Reward: 2007.50, Epsilon: 0.727\n",
      "Episode 639, Total Reward: 3103.50, Epsilon: 0.726\n",
      "Episode 640, Total Reward: 2970.50, Epsilon: 0.726\n",
      "Episode 641, Total Reward: 17.50, Epsilon: 0.726\n",
      "Episode 642, Total Reward: 753.50, Epsilon: 0.725\n",
      "Episode 643, Total Reward: 13.50, Epsilon: 0.725\n",
      "Episode 644, Total Reward: 2922.00, Epsilon: 0.725\n",
      "Episode 645, Total Reward: 173.50, Epsilon: 0.724\n",
      "Episode 646, Total Reward: 1460.00, Epsilon: 0.724\n",
      "Episode 647, Total Reward: 1547.50, Epsilon: 0.724\n",
      "Episode 648, Total Reward: 3432.50, Epsilon: 0.723\n",
      "Episode 649, Total Reward: 1429.50, Epsilon: 0.723\n",
      "Episode 650, Total Reward: 337.00, Epsilon: 0.722\n",
      "Episode 651, Total Reward: 1561.00, Epsilon: 0.722\n",
      "Episode 652, Total Reward: 3331.50, Epsilon: 0.722\n",
      "Episode 653, Total Reward: 1401.00, Epsilon: 0.721\n",
      "Episode 654, Total Reward: 2581.00, Epsilon: 0.721\n",
      "Episode 655, Total Reward: 960.50, Epsilon: 0.721\n",
      "Episode 656, Total Reward: 1070.50, Epsilon: 0.720\n",
      "Episode 657, Total Reward: 1547.50, Epsilon: 0.720\n",
      "Episode 658, Total Reward: 2000.50, Epsilon: 0.720\n",
      "Episode 659, Total Reward: 1900.50, Epsilon: 0.719\n",
      "Episode 660, Total Reward: 1085.50, Epsilon: 0.719\n",
      "Episode 661, Total Reward: 2642.00, Epsilon: 0.719\n",
      "Episode 662, Total Reward: -334.50, Epsilon: 0.718\n",
      "Episode 663, Total Reward: 2448.50, Epsilon: 0.718\n",
      "Episode 664, Total Reward: 875.50, Epsilon: 0.717\n",
      "Episode 665, Total Reward: 3200.50, Epsilon: 0.717\n",
      "Episode 666, Total Reward: 1203.00, Epsilon: 0.717\n",
      "Episode 667, Total Reward: 3311.50, Epsilon: 0.716\n",
      "Episode 668, Total Reward: 575.50, Epsilon: 0.716\n",
      "Episode 669, Total Reward: 3578.50, Epsilon: 0.716\n",
      "Episode 670, Total Reward: 849.50, Epsilon: 0.715\n",
      "Episode 671, Total Reward: 3372.00, Epsilon: 0.715\n",
      "Episode 672, Total Reward: 1520.50, Epsilon: 0.715\n",
      "Episode 673, Total Reward: -84.50, Epsilon: 0.714\n",
      "Episode 674, Total Reward: 1551.50, Epsilon: 0.714\n",
      "Episode 675, Total Reward: 2641.50, Epsilon: 0.713\n",
      "Episode 676, Total Reward: 4171.50, Epsilon: 0.713\n",
      "Episode 677, Total Reward: 1690.00, Epsilon: 0.713\n",
      "Episode 678, Total Reward: 3441.00, Epsilon: 0.712\n",
      "Episode 679, Total Reward: 2901.50, Epsilon: 0.712\n",
      "Episode 680, Total Reward: -778.50, Epsilon: 0.712\n",
      "Episode 681, Total Reward: 3220.00, Epsilon: 0.711\n",
      "Episode 682, Total Reward: 2821.50, Epsilon: 0.711\n",
      "Episode 683, Total Reward: 1375.50, Epsilon: 0.711\n",
      "Episode 684, Total Reward: 439.50, Epsilon: 0.710\n",
      "Episode 685, Total Reward: -124.50, Epsilon: 0.710\n",
      "Episode 686, Total Reward: 221.50, Epsilon: 0.710\n",
      "Episode 687, Total Reward: 913.50, Epsilon: 0.709\n",
      "Episode 688, Total Reward: 703.50, Epsilon: 0.709\n",
      "Episode 689, Total Reward: 1737.50, Epsilon: 0.709\n",
      "Episode 690, Total Reward: 2405.50, Epsilon: 0.708\n",
      "Episode 691, Total Reward: 689.50, Epsilon: 0.708\n",
      "Episode 692, Total Reward: 1103.50, Epsilon: 0.707\n",
      "Episode 693, Total Reward: 2322.00, Epsilon: 0.707\n",
      "Episode 694, Total Reward: 2119.50, Epsilon: 0.707\n",
      "Episode 695, Total Reward: 2501.00, Epsilon: 0.706\n",
      "Episode 696, Total Reward: 629.50, Epsilon: 0.706\n",
      "Episode 697, Total Reward: 1157.50, Epsilon: 0.706\n",
      "Episode 698, Total Reward: 1722.00, Epsilon: 0.705\n",
      "Episode 699, Total Reward: 940.00, Epsilon: 0.705\n",
      "Episode 700, Total Reward: 1936.00, Epsilon: 0.705\n",
      "Episode 701, Total Reward: 2101.50, Epsilon: 0.704\n",
      "Episode 702, Total Reward: 1991.50, Epsilon: 0.704\n",
      "Episode 703, Total Reward: -704.50, Epsilon: 0.704\n",
      "Episode 704, Total Reward: 1021.50, Epsilon: 0.703\n",
      "Episode 705, Total Reward: 5101.50, Epsilon: 0.703\n",
      "Episode 706, Total Reward: -182.50, Epsilon: 0.703\n",
      "Episode 707, Total Reward: 511.50, Epsilon: 0.702\n",
      "Episode 708, Total Reward: 287.50, Epsilon: 0.702\n",
      "Episode 709, Total Reward: 281.50, Epsilon: 0.701\n",
      "Episode 710, Total Reward: 3680.50, Epsilon: 0.701\n",
      "Episode 711, Total Reward: 2072.00, Epsilon: 0.701\n",
      "Episode 712, Total Reward: 517.50, Epsilon: 0.700\n",
      "Episode 713, Total Reward: 1061.50, Epsilon: 0.700\n",
      "Episode 714, Total Reward: 1607.50, Epsilon: 0.700\n",
      "Episode 715, Total Reward: 929.50, Epsilon: 0.699\n",
      "Episode 716, Total Reward: 1261.50, Epsilon: 0.699\n",
      "Episode 717, Total Reward: 2269.50, Epsilon: 0.699\n",
      "Episode 718, Total Reward: 3819.50, Epsilon: 0.698\n",
      "Episode 719, Total Reward: 941.50, Epsilon: 0.698\n",
      "Episode 720, Total Reward: 421.50, Epsilon: 0.698\n",
      "Episode 721, Total Reward: 1783.50, Epsilon: 0.697\n",
      "Episode 722, Total Reward: 481.50, Epsilon: 0.697\n",
      "Episode 723, Total Reward: 1001.50, Epsilon: 0.697\n",
      "Episode 724, Total Reward: 587.50, Epsilon: 0.696\n",
      "Episode 725, Total Reward: 1151.00, Epsilon: 0.696\n",
      "Episode 726, Total Reward: 285.50, Epsilon: 0.696\n",
      "Episode 727, Total Reward: 1441.00, Epsilon: 0.695\n",
      "Episode 728, Total Reward: 2339.50, Epsilon: 0.695\n",
      "Episode 729, Total Reward: 2269.50, Epsilon: 0.694\n",
      "Episode 730, Total Reward: 4551.50, Epsilon: 0.694\n",
      "Episode 731, Total Reward: 465.50, Epsilon: 0.694\n",
      "Episode 732, Total Reward: 2502.00, Epsilon: 0.693\n",
      "Episode 733, Total Reward: 4011.50, Epsilon: 0.693\n",
      "Episode 734, Total Reward: 3041.50, Epsilon: 0.693\n",
      "Episode 735, Total Reward: 779.50, Epsilon: 0.692\n",
      "Episode 736, Total Reward: 2801.50, Epsilon: 0.692\n",
      "Episode 737, Total Reward: 711.50, Epsilon: 0.692\n",
      "Episode 738, Total Reward: 3592.00, Epsilon: 0.691\n",
      "Episode 739, Total Reward: 2160.00, Epsilon: 0.691\n",
      "Episode 740, Total Reward: 291.50, Epsilon: 0.691\n",
      "Episode 741, Total Reward: 1922.00, Epsilon: 0.690\n",
      "Episode 742, Total Reward: 2298.00, Epsilon: 0.690\n",
      "Episode 743, Total Reward: -146.50, Epsilon: 0.690\n",
      "Episode 744, Total Reward: 1612.50, Epsilon: 0.689\n",
      "Episode 745, Total Reward: 1461.50, Epsilon: 0.689\n",
      "Episode 746, Total Reward: 1361.50, Epsilon: 0.689\n",
      "Episode 747, Total Reward: 543.50, Epsilon: 0.688\n",
      "Episode 748, Total Reward: 449.50, Epsilon: 0.688\n",
      "Episode 749, Total Reward: 2760.50, Epsilon: 0.688\n",
      "Episode 750, Total Reward: 2557.50, Epsilon: 0.687\n",
      "Episode 751, Total Reward: 1481.50, Epsilon: 0.687\n",
      "Episode 752, Total Reward: 307.50, Epsilon: 0.687\n",
      "Episode 753, Total Reward: 2053.50, Epsilon: 0.686\n",
      "Episode 754, Total Reward: 879.00, Epsilon: 0.686\n",
      "Episode 755, Total Reward: 4101.00, Epsilon: 0.686\n",
      "Episode 756, Total Reward: 3310.50, Epsilon: 0.685\n",
      "Episode 757, Total Reward: 963.50, Epsilon: 0.685\n",
      "Episode 758, Total Reward: -320.50, Epsilon: 0.684\n",
      "Episode 759, Total Reward: 2549.00, Epsilon: 0.684\n",
      "Episode 760, Total Reward: 2419.50, Epsilon: 0.684\n",
      "Episode 761, Total Reward: 2702.00, Epsilon: 0.683\n",
      "Episode 762, Total Reward: 335.50, Epsilon: 0.683\n",
      "Episode 763, Total Reward: 1750.50, Epsilon: 0.683\n",
      "Episode 764, Total Reward: 1923.50, Epsilon: 0.682\n",
      "Episode 765, Total Reward: 1079.50, Epsilon: 0.682\n",
      "Episode 766, Total Reward: 2340.00, Epsilon: 0.682\n",
      "Episode 767, Total Reward: 2327.50, Epsilon: 0.681\n",
      "Episode 768, Total Reward: 433.50, Epsilon: 0.681\n",
      "Episode 769, Total Reward: 171.50, Epsilon: 0.681\n",
      "Episode 770, Total Reward: 1191.50, Epsilon: 0.680\n",
      "Episode 771, Total Reward: 3349.50, Epsilon: 0.680\n",
      "Episode 772, Total Reward: 3211.00, Epsilon: 0.680\n",
      "Episode 773, Total Reward: 2880.00, Epsilon: 0.679\n",
      "Episode 774, Total Reward: 3362.00, Epsilon: 0.679\n",
      "Episode 775, Total Reward: 2760.50, Epsilon: 0.679\n",
      "Episode 776, Total Reward: 1782.50, Epsilon: 0.678\n",
      "Episode 777, Total Reward: 445.50, Epsilon: 0.678\n",
      "Episode 778, Total Reward: 2561.00, Epsilon: 0.678\n",
      "Episode 779, Total Reward: 3242.50, Epsilon: 0.677\n",
      "Episode 780, Total Reward: 2081.00, Epsilon: 0.677\n",
      "Episode 781, Total Reward: 3292.50, Epsilon: 0.677\n",
      "Episode 782, Total Reward: 739.50, Epsilon: 0.676\n",
      "Episode 783, Total Reward: 1252.00, Epsilon: 0.676\n",
      "Episode 784, Total Reward: 1890.50, Epsilon: 0.676\n",
      "Episode 785, Total Reward: 2470.50, Epsilon: 0.675\n",
      "Episode 786, Total Reward: 1099.50, Epsilon: 0.675\n",
      "Episode 787, Total Reward: 2959.00, Epsilon: 0.675\n",
      "Episode 788, Total Reward: 1340.50, Epsilon: 0.674\n",
      "Episode 789, Total Reward: 2592.00, Epsilon: 0.674\n",
      "Episode 790, Total Reward: 1127.50, Epsilon: 0.674\n",
      "Episode 791, Total Reward: 3091.50, Epsilon: 0.673\n",
      "Episode 792, Total Reward: 4240.50, Epsilon: 0.673\n",
      "Episode 793, Total Reward: 3199.50, Epsilon: 0.673\n",
      "Episode 794, Total Reward: 3300.00, Epsilon: 0.672\n",
      "Episode 795, Total Reward: -629.50, Epsilon: 0.672\n",
      "Episode 796, Total Reward: 2939.00, Epsilon: 0.672\n",
      "Episode 797, Total Reward: 2951.00, Epsilon: 0.671\n",
      "Episode 798, Total Reward: 3599.00, Epsilon: 0.671\n",
      "Episode 799, Total Reward: 1900.00, Epsilon: 0.671\n",
      "Episode 800, Total Reward: 3759.00, Epsilon: 0.670\n",
      "Episode 801, Total Reward: 4065.50, Epsilon: 0.670\n",
      "Episode 802, Total Reward: 2690.50, Epsilon: 0.670\n",
      "Episode 803, Total Reward: 3461.00, Epsilon: 0.669\n",
      "Episode 804, Total Reward: 2001.00, Epsilon: 0.669\n",
      "Episode 805, Total Reward: 1219.50, Epsilon: 0.669\n",
      "Episode 806, Total Reward: 1086.00, Epsilon: 0.668\n",
      "Episode 807, Total Reward: 4040.00, Epsilon: 0.668\n",
      "Episode 808, Total Reward: 1940.00, Epsilon: 0.668\n",
      "Episode 809, Total Reward: 2158.00, Epsilon: 0.667\n",
      "Episode 810, Total Reward: 999.50, Epsilon: 0.667\n",
      "Episode 811, Total Reward: 2290.00, Epsilon: 0.667\n",
      "Episode 812, Total Reward: 2979.50, Epsilon: 0.666\n",
      "Episode 813, Total Reward: 4179.00, Epsilon: 0.666\n",
      "Episode 814, Total Reward: 183.50, Epsilon: 0.666\n",
      "Episode 815, Total Reward: 4101.50, Epsilon: 0.665\n",
      "Episode 816, Total Reward: 1713.50, Epsilon: 0.665\n",
      "Episode 817, Total Reward: 2073.50, Epsilon: 0.665\n",
      "Episode 818, Total Reward: 3021.00, Epsilon: 0.664\n",
      "Episode 819, Total Reward: 3071.00, Epsilon: 0.664\n",
      "Episode 820, Total Reward: 2102.00, Epsilon: 0.664\n",
      "Episode 821, Total Reward: 1870.00, Epsilon: 0.663\n",
      "Episode 822, Total Reward: 1730.50, Epsilon: 0.663\n",
      "Episode 823, Total Reward: 1990.50, Epsilon: 0.663\n",
      "Episode 824, Total Reward: 1389.50, Epsilon: 0.662\n",
      "Episode 825, Total Reward: 2481.50, Epsilon: 0.662\n",
      "Episode 826, Total Reward: 3220.50, Epsilon: 0.662\n",
      "Episode 827, Total Reward: 2270.50, Epsilon: 0.661\n",
      "Episode 828, Total Reward: 3010.50, Epsilon: 0.661\n",
      "Episode 829, Total Reward: 1703.50, Epsilon: 0.661\n",
      "Episode 830, Total Reward: 2281.50, Epsilon: 0.660\n",
      "Episode 831, Total Reward: 1559.50, Epsilon: 0.660\n",
      "Episode 832, Total Reward: 2467.50, Epsilon: 0.660\n",
      "Episode 833, Total Reward: 2488.50, Epsilon: 0.659\n",
      "Episode 834, Total Reward: 1009.00, Epsilon: 0.659\n",
      "Episode 835, Total Reward: 3102.50, Epsilon: 0.659\n",
      "Episode 836, Total Reward: 2659.50, Epsilon: 0.658\n",
      "Episode 837, Total Reward: 1467.50, Epsilon: 0.658\n",
      "Episode 838, Total Reward: 995.50, Epsilon: 0.658\n",
      "Episode 839, Total Reward: 103.50, Epsilon: 0.657\n",
      "Episode 840, Total Reward: 1809.50, Epsilon: 0.657\n",
      "Episode 841, Total Reward: 2982.50, Epsilon: 0.657\n",
      "Episode 842, Total Reward: 2690.00, Epsilon: 0.656\n",
      "Episode 843, Total Reward: 1520.50, Epsilon: 0.656\n",
      "Episode 844, Total Reward: 1297.50, Epsilon: 0.656\n",
      "Episode 845, Total Reward: 277.50, Epsilon: 0.655\n",
      "Episode 846, Total Reward: 785.50, Epsilon: 0.655\n",
      "Episode 847, Total Reward: 1877.50, Epsilon: 0.655\n",
      "Episode 848, Total Reward: 1041.50, Epsilon: 0.654\n",
      "Episode 849, Total Reward: 3059.50, Epsilon: 0.654\n",
      "Episode 850, Total Reward: 1791.50, Epsilon: 0.654\n",
      "Episode 851, Total Reward: 4201.50, Epsilon: 0.653\n",
      "Episode 852, Total Reward: 159.50, Epsilon: 0.653\n",
      "Episode 853, Total Reward: 2851.50, Epsilon: 0.653\n",
      "Episode 854, Total Reward: 771.50, Epsilon: 0.652\n",
      "Episode 855, Total Reward: -134.50, Epsilon: 0.652\n",
      "Episode 856, Total Reward: 2007.50, Epsilon: 0.652\n",
      "Episode 857, Total Reward: 1443.50, Epsilon: 0.651\n",
      "Episode 858, Total Reward: 495.50, Epsilon: 0.651\n",
      "Episode 859, Total Reward: 2738.50, Epsilon: 0.651\n",
      "Episode 860, Total Reward: 2753.50, Epsilon: 0.650\n",
      "Episode 861, Total Reward: 1199.50, Epsilon: 0.650\n",
      "Episode 862, Total Reward: 3332.00, Epsilon: 0.650\n",
      "Episode 863, Total Reward: 2370.50, Epsilon: 0.649\n",
      "Episode 864, Total Reward: 3132.50, Epsilon: 0.649\n",
      "Episode 865, Total Reward: 1623.50, Epsilon: 0.649\n",
      "Episode 866, Total Reward: 2571.50, Epsilon: 0.648\n",
      "Episode 867, Total Reward: 961.50, Epsilon: 0.648\n",
      "Episode 868, Total Reward: 563.50, Epsilon: 0.648\n",
      "Episode 869, Total Reward: 2809.00, Epsilon: 0.648\n",
      "Episode 870, Total Reward: 940.50, Epsilon: 0.647\n",
      "Episode 871, Total Reward: 1571.50, Epsilon: 0.647\n",
      "Episode 872, Total Reward: 1602.50, Epsilon: 0.647\n",
      "Episode 873, Total Reward: 1189.50, Epsilon: 0.646\n",
      "Episode 874, Total Reward: 315.50, Epsilon: 0.646\n",
      "Episode 875, Total Reward: 2791.00, Epsilon: 0.646\n",
      "Episode 876, Total Reward: 3272.50, Epsilon: 0.645\n",
      "Episode 877, Total Reward: 2811.00, Epsilon: 0.645\n",
      "Episode 878, Total Reward: 4450.50, Epsilon: 0.645\n",
      "Episode 879, Total Reward: 1773.50, Epsilon: 0.644\n",
      "Episode 880, Total Reward: 1751.50, Epsilon: 0.644\n",
      "Episode 881, Total Reward: 1210.00, Epsilon: 0.644\n",
      "Episode 882, Total Reward: 3007.50, Epsilon: 0.643\n",
      "Episode 883, Total Reward: 3312.00, Epsilon: 0.643\n",
      "Episode 884, Total Reward: 3031.00, Epsilon: 0.643\n",
      "Episode 885, Total Reward: 727.50, Epsilon: 0.642\n",
      "Episode 886, Total Reward: 709.50, Epsilon: 0.642\n",
      "Episode 887, Total Reward: 4052.00, Epsilon: 0.642\n",
      "Episode 888, Total Reward: 2941.00, Epsilon: 0.641\n",
      "Episode 889, Total Reward: 1397.50, Epsilon: 0.641\n",
      "Episode 890, Total Reward: 4521.00, Epsilon: 0.641\n",
      "Episode 891, Total Reward: 2249.50, Epsilon: 0.640\n",
      "Episode 892, Total Reward: 2971.50, Epsilon: 0.640\n",
      "Episode 893, Total Reward: 193.50, Epsilon: 0.640\n",
      "Episode 894, Total Reward: 2079.50, Epsilon: 0.639\n",
      "Episode 895, Total Reward: 1839.50, Epsilon: 0.639\n",
      "Episode 896, Total Reward: 1615.50, Epsilon: 0.639\n",
      "Episode 897, Total Reward: 748.50, Epsilon: 0.639\n",
      "Episode 898, Total Reward: 2412.50, Epsilon: 0.638\n",
      "Episode 899, Total Reward: 521.50, Epsilon: 0.638\n",
      "Episode 900, Total Reward: 2935.50, Epsilon: 0.638\n",
      "Episode 901, Total Reward: 1855.50, Epsilon: 0.637\n",
      "Episode 902, Total Reward: 2517.50, Epsilon: 0.637\n",
      "Episode 903, Total Reward: 1310.50, Epsilon: 0.637\n",
      "Episode 904, Total Reward: 2881.50, Epsilon: 0.636\n",
      "Episode 905, Total Reward: 1371.50, Epsilon: 0.636\n",
      "Episode 906, Total Reward: 4098.50, Epsilon: 0.636\n",
      "Episode 907, Total Reward: 452.00, Epsilon: 0.635\n",
      "Episode 908, Total Reward: 25.50, Epsilon: 0.635\n",
      "Episode 909, Total Reward: 1229.50, Epsilon: 0.635\n",
      "Episode 910, Total Reward: 3612.00, Epsilon: 0.634\n",
      "Episode 911, Total Reward: 4102.00, Epsilon: 0.634\n",
      "Episode 912, Total Reward: 1965.50, Epsilon: 0.634\n",
      "Episode 913, Total Reward: 1721.00, Epsilon: 0.633\n",
      "Episode 914, Total Reward: 495.50, Epsilon: 0.633\n",
      "Episode 915, Total Reward: 619.50, Epsilon: 0.633\n",
      "Episode 916, Total Reward: 1347.50, Epsilon: 0.632\n",
      "Episode 917, Total Reward: 1625.50, Epsilon: 0.632\n",
      "Episode 918, Total Reward: 819.50, Epsilon: 0.632\n",
      "Episode 919, Total Reward: 2085.50, Epsilon: 0.632\n",
      "Episode 920, Total Reward: 2671.00, Epsilon: 0.631\n",
      "Episode 921, Total Reward: 4065.50, Epsilon: 0.631\n",
      "Episode 922, Total Reward: 4942.00, Epsilon: 0.631\n",
      "Episode 923, Total Reward: 2069.50, Epsilon: 0.630\n",
      "Episode 924, Total Reward: 2713.50, Epsilon: 0.630\n",
      "Episode 925, Total Reward: 5492.50, Epsilon: 0.630\n",
      "Episode 926, Total Reward: 3045.50, Epsilon: 0.629\n",
      "Episode 927, Total Reward: 1993.50, Epsilon: 0.629\n",
      "Episode 928, Total Reward: 1512.00, Epsilon: 0.629\n",
      "Episode 929, Total Reward: 3262.50, Epsilon: 0.628\n",
      "Episode 930, Total Reward: 3262.00, Epsilon: 0.628\n",
      "Episode 931, Total Reward: 419.50, Epsilon: 0.628\n",
      "Episode 932, Total Reward: 2601.00, Epsilon: 0.627\n",
      "Episode 933, Total Reward: 1691.50, Epsilon: 0.627\n",
      "Episode 934, Total Reward: 3149.00, Epsilon: 0.627\n",
      "Episode 935, Total Reward: 2051.50, Epsilon: 0.626\n",
      "Episode 936, Total Reward: 2115.50, Epsilon: 0.626\n",
      "Episode 937, Total Reward: 1745.50, Epsilon: 0.626\n",
      "Episode 938, Total Reward: 2532.50, Epsilon: 0.626\n",
      "Episode 939, Total Reward: 3771.50, Epsilon: 0.625\n",
      "Episode 940, Total Reward: 523.50, Epsilon: 0.625\n",
      "Episode 941, Total Reward: 2207.50, Epsilon: 0.625\n",
      "Episode 942, Total Reward: 833.50, Epsilon: 0.624\n",
      "Episode 943, Total Reward: 415.50, Epsilon: 0.624\n",
      "Episode 944, Total Reward: 3342.50, Epsilon: 0.624\n",
      "Episode 945, Total Reward: 3302.00, Epsilon: 0.623\n",
      "Episode 946, Total Reward: 4581.00, Epsilon: 0.623\n",
      "Episode 947, Total Reward: 2545.50, Epsilon: 0.623\n",
      "Episode 948, Total Reward: 3940.50, Epsilon: 0.622\n",
      "Episode 949, Total Reward: 4330.00, Epsilon: 0.622\n",
      "Episode 950, Total Reward: 2801.50, Epsilon: 0.622\n",
      "Episode 951, Total Reward: 2679.50, Epsilon: 0.622\n",
      "Episode 952, Total Reward: 1965.50, Epsilon: 0.621\n",
      "Episode 953, Total Reward: 2065.50, Epsilon: 0.621\n",
      "Episode 954, Total Reward: 3441.00, Epsilon: 0.621\n",
      "Episode 955, Total Reward: 925.50, Epsilon: 0.620\n",
      "Episode 956, Total Reward: 4531.00, Epsilon: 0.620\n",
      "Episode 957, Total Reward: 3951.50, Epsilon: 0.620\n",
      "Episode 958, Total Reward: 4252.00, Epsilon: 0.619\n",
      "Episode 959, Total Reward: 219.50, Epsilon: 0.619\n",
      "Episode 960, Total Reward: 3421.50, Epsilon: 0.619\n",
      "Episode 961, Total Reward: 1855.50, Epsilon: 0.618\n",
      "Episode 962, Total Reward: 681.50, Epsilon: 0.618\n",
      "Episode 963, Total Reward: 3740.50, Epsilon: 0.618\n",
      "Episode 964, Total Reward: 2655.50, Epsilon: 0.617\n",
      "Episode 965, Total Reward: 1177.50, Epsilon: 0.617\n",
      "Episode 966, Total Reward: 3321.00, Epsilon: 0.617\n",
      "Episode 967, Total Reward: 3649.50, Epsilon: 0.617\n",
      "Episode 968, Total Reward: 1421.00, Epsilon: 0.616\n",
      "Episode 969, Total Reward: 3201.00, Epsilon: 0.616\n",
      "Episode 970, Total Reward: 1401.50, Epsilon: 0.616\n",
      "Episode 971, Total Reward: 1821.50, Epsilon: 0.615\n",
      "Episode 972, Total Reward: 3652.00, Epsilon: 0.615\n",
      "Episode 973, Total Reward: 2807.50, Epsilon: 0.615\n",
      "Episode 974, Total Reward: 367.50, Epsilon: 0.614\n",
      "Episode 975, Total Reward: 1033.50, Epsilon: 0.614\n",
      "Episode 976, Total Reward: 1121.50, Epsilon: 0.614\n",
      "Episode 977, Total Reward: 2369.50, Epsilon: 0.613\n",
      "Episode 978, Total Reward: 2261.50, Epsilon: 0.613\n",
      "Episode 979, Total Reward: 4467.50, Epsilon: 0.613\n",
      "Episode 980, Total Reward: 4280.00, Epsilon: 0.613\n",
      "Episode 981, Total Reward: 803.50, Epsilon: 0.612\n",
      "Episode 982, Total Reward: 907.50, Epsilon: 0.612\n",
      "Episode 983, Total Reward: 467.50, Epsilon: 0.612\n",
      "Episode 984, Total Reward: 4130.50, Epsilon: 0.611\n",
      "Episode 985, Total Reward: 1233.50, Epsilon: 0.611\n",
      "Episode 986, Total Reward: 301.50, Epsilon: 0.611\n",
      "Episode 987, Total Reward: 4381.50, Epsilon: 0.610\n",
      "Episode 988, Total Reward: 3105.50, Epsilon: 0.610\n",
      "Episode 989, Total Reward: 1163.50, Epsilon: 0.610\n",
      "Episode 990, Total Reward: -294.50, Epsilon: 0.609\n",
      "Episode 991, Total Reward: 3879.50, Epsilon: 0.609\n",
      "Episode 992, Total Reward: 4181.50, Epsilon: 0.609\n",
      "Episode 993, Total Reward: 841.50, Epsilon: 0.609\n",
      "Episode 994, Total Reward: 1411.50, Epsilon: 0.608\n",
      "Episode 995, Total Reward: 3277.50, Epsilon: 0.608\n",
      "Episode 996, Total Reward: 699.50, Epsilon: 0.608\n",
      "Episode 997, Total Reward: 793.50, Epsilon: 0.607\n",
      "Episode 998, Total Reward: 2425.50, Epsilon: 0.607\n",
      "Episode 999, Total Reward: 4320.00, Epsilon: 0.607\n",
      "Episode 1000, Total Reward: 2317.50, Epsilon: 0.606\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pygame\n",
    "\n",
    "# Constants for gridworld and visualization\n",
    "GRID_SIZE = 10\n",
    "CELL_SIZE = 50   # pixels per cell\n",
    "WINDOW_SIZE = GRID_SIZE * CELL_SIZE\n",
    "NUM_OBSTACLES = 10\n",
    "\n",
    "# ----------------------------\n",
    "# Environment (Gridworld)\n",
    "# ----------------------------\n",
    "class Gridworld:\n",
    "    def __init__(self):\n",
    "        self.grid_size = GRID_SIZE\n",
    "        self.cell_size = CELL_SIZE\n",
    "        self.vision_radius = 5  # drones can sense obstacles within 5 cells\n",
    "        # Define goals: agent 0's goal is bottom-right, agent 1's goal is bottom-left\n",
    "        self.goals = [(GRID_SIZE - 1, GRID_SIZE - 1), (0, GRID_SIZE - 1)]\n",
    "        self.obstacles = self.generate_obstacles()\n",
    "        # Both agents start at top-left (0,0)\n",
    "        self.agents = [(0, 0), (0, 0)]\n",
    "        # Penalty and reward values\n",
    "        self.movement_penalty = -1       # per move per agent\n",
    "        self.closeness_penalty = -10     # if Manhattan distance < 2\n",
    "        self.collision_penalty = -50     # if agent attempts to move into an obstacle\n",
    "        self.goal_reward = 50            # increased goal reward to incentivize reaching the goal\n",
    "        self.shaping_factor = 0.5        # reward shaping factor for progress towards goal\n",
    "\n",
    "    def generate_obstacles(self):\n",
    "        obstacles = set()\n",
    "        # Generate random fixed obstacles while avoiding start and goal positions\n",
    "        while len(obstacles) < NUM_OBSTACLES:\n",
    "            x = random.randint(0, self.grid_size - 1)\n",
    "            y = random.randint(0, self.grid_size - 1)\n",
    "            if (x, y) != (0, 0) and (x, y) not in self.goals:\n",
    "                obstacles.add((x, y))\n",
    "        return list(obstacles)\n",
    "\n",
    "    def get_obstacle_view(self, agent_position):\n",
    "        view = []\n",
    "        ax, ay = agent_position\n",
    "        for dy in range(-self.vision_radius, self.vision_radius + 1):\n",
    "            for dx in range(-self.vision_radius, self.vision_radius + 1):\n",
    "                cx = ax + dx\n",
    "                cy = ay + dy\n",
    "                if 0 <= cx < self.grid_size and 0 <= cy < self.grid_size:\n",
    "                    if (cx, cy) in self.obstacles:\n",
    "                        view.append(1.0)\n",
    "                    else:\n",
    "                        view.append(0.0)\n",
    "                else:\n",
    "                    view.append(0.0)\n",
    "        return view\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset agents to starting positions; obstacles remain fixed.\n",
    "        self.agents = [(0, 0), (0, 0)]\n",
    "        return self.get_states()\n",
    "\n",
    "    def get_states(self):\n",
    "        # Each agent's state: [agent_x, agent_y, goal_x, goal_y] + flattened obstacle view\n",
    "        states = []\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            goal = self.goals[idx]\n",
    "            base_state = [pos[0], pos[1], goal[0], goal[1]]\n",
    "            obstacle_view = self.get_obstacle_view(pos)\n",
    "            states.append(base_state + obstacle_view)\n",
    "        return states\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Expects a list of actions (one per agent: 0: up, 1: down, 2: left, 3: right).\n",
    "        Updates agent positions and returns:\n",
    "          - next_states: list of states for each agent,\n",
    "          - reward: a scalar reward that includes:\n",
    "                * movement penalty,\n",
    "                * collision penalty (if agent attempts to move into an obstacle),\n",
    "                * reward shaping (for progress toward the goal),\n",
    "                * closeness penalty (if Manhattan distance < 2),\n",
    "                * goal achievement rewards.\n",
    "          - done: True if both agents have reached their goals.\n",
    "        \"\"\"\n",
    "        reward = 0\n",
    "        # Apply movement penalty for each agent\n",
    "        reward += self.movement_penalty * len(self.agents)\n",
    "        \n",
    "        # Compute old Manhattan distances for reward shaping\n",
    "        old_distances = []\n",
    "        for idx, (x, y) in enumerate(self.agents):\n",
    "            goal = self.goals[idx]\n",
    "            old_distance = abs(x - goal[0]) + abs(y - goal[1])\n",
    "            old_distances.append(old_distance)\n",
    "        \n",
    "        new_positions = []\n",
    "        for idx, (x, y) in enumerate(self.agents):\n",
    "            dx, dy = 0, 0\n",
    "            action = actions[idx]\n",
    "            if action == 0:  # up\n",
    "                dy = -1\n",
    "            elif action == 1:  # down\n",
    "                dy = 1\n",
    "            elif action == 2:  # left\n",
    "                dx = -1\n",
    "            elif action == 3:  # right\n",
    "                dx = 1\n",
    "            new_x, new_y = x + dx, y + dy\n",
    "            # Check boundaries; invalid moves result in staying in place.\n",
    "            if new_x < 0 or new_x >= self.grid_size or new_y < 0 or new_y >= self.grid_size:\n",
    "                new_x, new_y = x, y\n",
    "            # Check obstacles; if moving into an obstacle, apply collision penalty and stay in place.\n",
    "            if (new_x, new_y) in self.obstacles:\n",
    "                reward += self.collision_penalty\n",
    "                new_x, new_y = x, y\n",
    "            new_positions.append((new_x, new_y))\n",
    "        self.agents = new_positions\n",
    "\n",
    "        # Reward shaping: add reward proportional to progress toward goal\n",
    "        for idx, (x, y) in enumerate(self.agents):\n",
    "            goal = self.goals[idx]\n",
    "            new_distance = abs(x - goal[0]) + abs(y - goal[1])\n",
    "            # If agent gets closer, the difference (old - new) is positive.\n",
    "            shaping_reward = self.shaping_factor * (old_distances[idx] - new_distance)\n",
    "            reward += shaping_reward\n",
    "\n",
    "        # Closeness penalty: if Manhattan distance between agents < 2\n",
    "        manhattan_distance = abs(self.agents[0][0] - self.agents[1][0]) + abs(self.agents[0][1] - self.agents[1][1])\n",
    "        if manhattan_distance < 2:\n",
    "            reward += self.closeness_penalty\n",
    "\n",
    "        # Reward for reaching the goal for each agent.\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            if pos == self.goals[idx]:\n",
    "                reward += self.goal_reward\n",
    "\n",
    "        # Episode ends if both agents have reached their goals.\n",
    "        done = (self.agents[0] == self.goals[0] and self.agents[1] == self.goals[1])\n",
    "        next_states = self.get_states()\n",
    "        return next_states, reward, done\n",
    "\n",
    "    def render(self, screen):\n",
    "        # Clear screen\n",
    "        screen.fill((255, 255, 255))\n",
    "        # Draw grid lines\n",
    "        for x in range(0, WINDOW_SIZE, self.cell_size):\n",
    "            pygame.draw.line(screen, (200, 200, 200), (x, 0), (x, WINDOW_SIZE))\n",
    "        for y in range(0, WINDOW_SIZE, self.cell_size):\n",
    "            pygame.draw.line(screen, (200, 200, 200), (0, y), (WINDOW_SIZE, y))\n",
    "        # Draw obstacles as black rectangles\n",
    "        for obs in self.obstacles:\n",
    "            rect = pygame.Rect(obs[0]*self.cell_size, obs[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (0, 0, 0), rect)\n",
    "        # Draw goals as green rectangles\n",
    "        for goal in self.goals:\n",
    "            rect = pygame.Rect(goal[0]*self.cell_size, goal[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (0, 255, 0), rect)\n",
    "        # Draw agents as circles (agent 0 in red, agent 1 in blue)\n",
    "        colors = [(255, 0, 0), (0, 0, 255)]\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            center = (pos[0]*self.cell_size + self.cell_size//2,\n",
    "                      pos[1]*self.cell_size + self.cell_size//2)\n",
    "            pygame.draw.circle(screen, colors[idx], center, self.cell_size//3)\n",
    "        pygame.display.flip()\n",
    "\n",
    "# ----------------------------\n",
    "# Replay Buffer for DQN\n",
    "# ----------------------------\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# ----------------------------\n",
    "# DQN Network and Agent\n",
    "# ----------------------------\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_dim, output_dim, lr=1e-3, gamma=0.99,\n",
    "                 buffer_capacity=10000, batch_size=32):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def select_action(self, state, epsilon):\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(4)  # 4 discrete actions: up, down, left, right\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "        return q_values.argmax().item()\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.replay_buffer.sample(self.batch_size)\n",
    "        batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*transitions)\n",
    "        \n",
    "        batch_state = torch.FloatTensor(batch_state).to(self.device)\n",
    "        batch_action = torch.LongTensor(batch_action).unsqueeze(1).to(self.device)\n",
    "        batch_reward = torch.FloatTensor(batch_reward).unsqueeze(1).to(self.device)\n",
    "        batch_next_state = torch.FloatTensor(batch_next_state).to(self.device)\n",
    "        batch_done = torch.FloatTensor(batch_done).unsqueeze(1).to(self.device)\n",
    "        \n",
    "        current_q = self.policy_net(batch_state).gather(1, batch_action)\n",
    "        next_q = self.target_net(batch_next_state).max(1)[0].unsqueeze(1)\n",
    "        target_q = batch_reward + self.gamma * next_q * (1 - batch_done)\n",
    "        \n",
    "        loss = nn.MSELoss()(current_q, target_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "# ----------------------------\n",
    "# Training Loop with Milestone Rendering\n",
    "# ----------------------------\n",
    "def train():\n",
    "    num_episodes = 1000\n",
    "    max_steps = 200\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.05\n",
    "    epsilon_decay = 0.9967\n",
    "    target_update_freq = 10\n",
    "    # Define milestone episodes (25%, 50%, 75%, and 100% of training)\n",
    "    milestones = {int(num_episodes * 0.25), int(num_episodes * 0.5), int(num_episodes * 0.75), num_episodes}\n",
    "    \n",
    "    env = Gridworld()\n",
    "    # Each agent's state dimension: 4 + (2*vision_radius+1)^2.\n",
    "    state_dim = 4 + (2 * env.vision_radius + 1) ** 2\n",
    "    action_dim = 4  # up, down, left, right\n",
    "    agents = [DQNAgent(state_dim, action_dim), DQNAgent(state_dim, action_dim)]\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        states = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        # Check if this is a milestone episode (episode indices are 0-based)\n",
    "        do_render = (episode + 1) in milestones\n",
    "        if do_render:\n",
    "            pygame.init()\n",
    "            screen = pygame.display.set_mode((WINDOW_SIZE, WINDOW_SIZE))\n",
    "            pygame.display.set_caption(f\"Training Episode {episode + 1}\")\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            actions = []\n",
    "            # Each agent selects an action using its own DQN policy\n",
    "            for i in range(2):\n",
    "                action = agents[i].select_action(states[i], epsilon)\n",
    "                actions.append(action)\n",
    "            next_states, reward, done = env.step(actions)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Store experiences for each agent\n",
    "            for i in range(2):\n",
    "                agents[i].replay_buffer.push(states[i], actions[i], reward, next_states[i], done)\n",
    "            \n",
    "            states = next_states\n",
    "            \n",
    "            # Update each agent's policy network\n",
    "            for i in range(2):\n",
    "                agents[i].update()\n",
    "            \n",
    "            if do_render:\n",
    "                env.render(screen)\n",
    "                # Process quit events in visualization mode\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.QUIT:\n",
    "                        pygame.quit()\n",
    "                        return\n",
    "                pygame.time.wait(200)\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        if do_render:\n",
    "            # Pause to allow viewing the final state of the milestone episode\n",
    "            pygame.time.wait(1000)\n",
    "            pygame.quit()\n",
    "        \n",
    "        # Update target networks periodically\n",
    "        if (episode + 1) % target_update_freq == 0:\n",
    "            for agent in agents:\n",
    "                agent.update_target()\n",
    "        \n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        print(f\"Episode {episode + 1}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea773b1b-2908-43b2-a412-a6de7d9ecea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
