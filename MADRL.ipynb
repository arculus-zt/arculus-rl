{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "472bdfad-7839-4809-905d-f169d591ae22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Agent positions: [(0, 0), (0, 0)], Reward: -10\n",
      "Agent positions: [(0, 1), (0, 0)], Reward: -10\n",
      "Agent positions: [(0, 1), (0, 1)], Reward: -10\n",
      "Agent positions: [(0, 0), (0, 0)], Reward: -10\n",
      "Agent positions: [(0, 0), (0, 1)], Reward: -10\n",
      "Agent positions: [(0, 0), (0, 2)], Reward: 0\n",
      "Agent positions: [(0, 0), (1, 2)], Reward: 0\n",
      "Agent positions: [(0, 0), (2, 2)], Reward: 0\n",
      "Agent positions: [(0, 0), (2, 1)], Reward: 0\n",
      "Agent positions: [(0, 0), (2, 1)], Reward: 0\n",
      "Agent positions: [(0, 0), (1, 1)], Reward: -10\n",
      "Agent positions: [(0, 0), (0, 1)], Reward: -10\n",
      "Agent positions: [(0, 1), (0, 2)], Reward: -10\n",
      "Agent positions: [(1, 1), (0, 1)], Reward: -10\n",
      "Agent positions: [(1, 2), (0, 2)], Reward: -10\n",
      "Agent positions: [(0, 2), (0, 2)], Reward: -10\n",
      "Agent positions: [(0, 3), (0, 3)], Reward: -10\n",
      "Agent positions: [(0, 3), (0, 2)], Reward: -10\n",
      "Agent positions: [(0, 2), (0, 2)], Reward: -10\n",
      "Agent positions: [(0, 3), (0, 3)], Reward: -10\n",
      "Agent positions: [(1, 3), (0, 2)], Reward: -10\n",
      "Agent positions: [(2, 3), (0, 2)], Reward: 0\n",
      "Agent positions: [(3, 3), (0, 3)], Reward: 0\n",
      "Agent positions: [(2, 3), (0, 3)], Reward: 0\n",
      "Agent positions: [(2, 2), (0, 3)], Reward: 0\n",
      "Agent positions: [(2, 1), (0, 3)], Reward: 0\n",
      "Agent positions: [(2, 1), (1, 3)], Reward: 0\n",
      "Agent positions: [(2, 1), (1, 4)], Reward: 0\n",
      "Agent positions: [(1, 1), (1, 4)], Reward: 0\n",
      "Agent positions: [(1, 2), (2, 4)], Reward: 0\n",
      "Agent positions: [(0, 2), (1, 4)], Reward: 0\n",
      "Agent positions: [(1, 2), (1, 5)], Reward: 0\n",
      "Agent positions: [(1, 3), (1, 6)], Reward: 0\n",
      "Agent positions: [(2, 3), (1, 6)], Reward: 0\n",
      "Agent positions: [(3, 3), (0, 6)], Reward: 0\n",
      "Agent positions: [(3, 4), (0, 5)], Reward: 0\n",
      "Agent positions: [(4, 4), (0, 6)], Reward: 0\n",
      "Agent positions: [(4, 3), (0, 7)], Reward: 0\n",
      "Agent positions: [(4, 2), (0, 8)], Reward: 0\n",
      "Agent positions: [(4, 1), (0, 9)], Reward: 10\n",
      "Agent positions: [(4, 2), (0, 8)], Reward: 0\n",
      "Agent positions: [(4, 3), (1, 8)], Reward: 0\n",
      "Agent positions: [(4, 4), (0, 8)], Reward: 0\n",
      "Agent positions: [(5, 4), (0, 8)], Reward: 0\n",
      "Agent positions: [(5, 3), (0, 7)], Reward: 0\n",
      "Agent positions: [(4, 3), (0, 8)], Reward: 0\n",
      "Agent positions: [(4, 4), (0, 9)], Reward: 10\n",
      "Agent positions: [(4, 5), (0, 9)], Reward: 10\n",
      "Agent positions: [(5, 5), (0, 9)], Reward: 10\n",
      "Agent positions: [(4, 5), (1, 9)], Reward: 0\n",
      "Agent positions: [(3, 5), (1, 9)], Reward: 0\n",
      "Agent positions: [(2, 5), (1, 9)], Reward: 0\n",
      "Agent positions: [(2, 6), (1, 8)], Reward: 0\n",
      "Agent positions: [(1, 6), (1, 9)], Reward: 0\n",
      "Agent positions: [(0, 6), (1, 9)], Reward: 0\n",
      "Agent positions: [(1, 6), (2, 9)], Reward: 0\n",
      "Agent positions: [(1, 5), (1, 9)], Reward: 0\n",
      "Agent positions: [(2, 5), (1, 9)], Reward: 0\n",
      "Agent positions: [(2, 4), (1, 8)], Reward: 0\n",
      "Agent positions: [(1, 4), (1, 9)], Reward: 0\n",
      "Agent positions: [(2, 4), (1, 9)], Reward: 0\n",
      "Agent positions: [(2, 3), (1, 9)], Reward: 0\n",
      "Agent positions: [(2, 2), (1, 9)], Reward: 0\n",
      "Agent positions: [(2, 3), (2, 9)], Reward: 0\n",
      "Agent positions: [(3, 3), (2, 9)], Reward: 0\n",
      "Agent positions: [(3, 2), (1, 9)], Reward: 0\n",
      "Agent positions: [(3, 3), (1, 9)], Reward: 0\n",
      "Agent positions: [(3, 2), (0, 9)], Reward: 10\n",
      "Agent positions: [(3, 1), (0, 9)], Reward: 10\n",
      "Agent positions: [(4, 1), (0, 9)], Reward: 10\n",
      "Agent positions: [(3, 1), (0, 9)], Reward: 10\n",
      "Agent positions: [(2, 1), (1, 9)], Reward: 0\n",
      "Agent positions: [(2, 1), (0, 9)], Reward: 10\n",
      "Agent positions: [(3, 1), (1, 9)], Reward: 0\n",
      "Agent positions: [(2, 1), (2, 9)], Reward: 0\n",
      "Agent positions: [(2, 1), (1, 9)], Reward: 0\n",
      "Agent positions: [(1, 1), (1, 8)], Reward: 0\n",
      "Agent positions: [(2, 1), (0, 8)], Reward: 0\n",
      "Agent positions: [(3, 1), (0, 7)], Reward: 0\n",
      "Agent positions: [(3, 0), (0, 8)], Reward: 0\n",
      "Agent positions: [(3, 0), (0, 8)], Reward: 0\n",
      "Agent positions: [(3, 0), (0, 8)], Reward: 0\n",
      "Agent positions: [(3, 0), (1, 8)], Reward: 0\n",
      "Agent positions: [(3, 0), (1, 8)], Reward: 0\n",
      "Agent positions: [(3, 0), (2, 8)], Reward: 0\n",
      "Agent positions: [(3, 0), (2, 9)], Reward: 0\n",
      "Agent positions: [(3, 0), (2, 8)], Reward: 0\n",
      "Agent positions: [(3, 1), (3, 8)], Reward: 0\n",
      "Agent positions: [(3, 2), (4, 8)], Reward: 0\n",
      "Agent positions: [(4, 2), (4, 7)], Reward: 0\n",
      "Agent positions: [(5, 2), (5, 7)], Reward: 0\n",
      "Agent positions: [(4, 2), (5, 8)], Reward: 0\n",
      "Agent positions: [(4, 1), (5, 9)], Reward: 0\n",
      "Agent positions: [(4, 2), (5, 8)], Reward: 0\n",
      "Agent positions: [(4, 3), (5, 9)], Reward: 0\n",
      "Agent positions: [(4, 2), (5, 8)], Reward: 0\n",
      "Agent positions: [(3, 2), (6, 8)], Reward: 0\n",
      "Agent positions: [(3, 1), (6, 7)], Reward: 0\n",
      "Agent positions: [(3, 2), (5, 7)], Reward: 0\n",
      "Agent positions: [(3, 1), (6, 7)], Reward: 0\n",
      "Agent positions: [(3, 0), (5, 7)], Reward: 0\n",
      "Agent positions: [(3, 1), (4, 7)], Reward: 0\n",
      "Agent positions: [(2, 1), (4, 8)], Reward: 0\n",
      "Agent positions: [(3, 1), (4, 9)], Reward: 0\n",
      "Agent positions: [(2, 1), (3, 9)], Reward: 0\n",
      "Agent positions: [(2, 2), (4, 9)], Reward: 0\n",
      "Agent positions: [(2, 1), (5, 9)], Reward: 0\n",
      "Agent positions: [(2, 1), (4, 9)], Reward: 0\n",
      "Agent positions: [(2, 1), (4, 9)], Reward: 0\n",
      "Agent positions: [(3, 1), (5, 9)], Reward: 0\n",
      "Agent positions: [(4, 1), (4, 9)], Reward: 0\n",
      "Agent positions: [(4, 2), (5, 9)], Reward: 0\n",
      "Agent positions: [(4, 3), (5, 8)], Reward: 0\n",
      "Agent positions: [(5, 3), (4, 8)], Reward: 0\n",
      "Agent positions: [(5, 4), (3, 8)], Reward: 0\n",
      "Agent positions: [(5, 5), (3, 9)], Reward: 0\n",
      "Agent positions: [(5, 4), (3, 8)], Reward: 0\n",
      "Agent positions: [(5, 3), (4, 8)], Reward: 0\n",
      "Agent positions: [(5, 4), (4, 7)], Reward: 0\n",
      "Agent positions: [(5, 3), (4, 6)], Reward: 0\n",
      "Agent positions: [(5, 2), (3, 6)], Reward: 0\n",
      "Agent positions: [(6, 2), (4, 6)], Reward: 0\n",
      "Agent positions: [(6, 2), (4, 5)], Reward: 0\n",
      "Agent positions: [(5, 2), (4, 6)], Reward: 0\n",
      "Agent positions: [(6, 2), (5, 6)], Reward: 0\n",
      "Agent positions: [(6, 2), (4, 6)], Reward: 0\n",
      "Agent positions: [(6, 3), (4, 7)], Reward: 0\n",
      "Agent positions: [(5, 3), (4, 8)], Reward: 0\n",
      "Agent positions: [(5, 2), (4, 7)], Reward: 0\n",
      "Agent positions: [(4, 2), (4, 8)], Reward: 0\n",
      "Agent positions: [(5, 2), (4, 9)], Reward: 0\n",
      "Agent positions: [(5, 1), (4, 8)], Reward: 0\n",
      "Agent positions: [(5, 2), (5, 8)], Reward: 0\n",
      "Agent positions: [(5, 3), (6, 8)], Reward: 0\n",
      "Agent positions: [(6, 3), (5, 8)], Reward: 0\n",
      "Agent positions: [(5, 3), (5, 9)], Reward: 0\n",
      "Agent positions: [(6, 3), (5, 9)], Reward: 0\n",
      "Agent positions: [(6, 4), (5, 9)], Reward: 0\n",
      "Agent positions: [(6, 3), (4, 9)], Reward: 0\n",
      "Agent positions: [(5, 3), (4, 8)], Reward: 0\n",
      "Agent positions: [(5, 2), (4, 9)], Reward: 0\n",
      "Agent positions: [(6, 2), (4, 9)], Reward: 0\n",
      "Agent positions: [(5, 2), (5, 9)], Reward: 0\n",
      "Agent positions: [(6, 2), (5, 8)], Reward: 0\n",
      "Agent positions: [(7, 2), (6, 8)], Reward: 0\n",
      "Agent positions: [(8, 2), (6, 8)], Reward: 0\n",
      "Agent positions: [(9, 2), (6, 8)], Reward: 0\n",
      "Agent positions: [(9, 2), (6, 7)], Reward: 0\n",
      "Agent positions: [(8, 2), (6, 6)], Reward: 0\n",
      "Agent positions: [(8, 1), (6, 5)], Reward: 0\n",
      "Agent positions: [(8, 1), (6, 6)], Reward: 0\n",
      "Agent positions: [(7, 1), (7, 6)], Reward: 0\n",
      "Agent positions: [(8, 1), (7, 6)], Reward: 0\n",
      "Agent positions: [(8, 0), (7, 5)], Reward: 0\n",
      "Agent positions: [(8, 0), (6, 5)], Reward: 0\n",
      "Agent positions: [(8, 1), (6, 4)], Reward: 0\n",
      "Agent positions: [(8, 2), (5, 4)], Reward: 0\n",
      "Agent positions: [(9, 2), (5, 3)], Reward: 0\n",
      "Agent positions: [(9, 2), (6, 3)], Reward: 0\n",
      "Agent positions: [(9, 2), (5, 3)], Reward: 0\n",
      "Agent positions: [(9, 2), (5, 4)], Reward: 0\n",
      "Agent positions: [(9, 2), (5, 3)], Reward: 0\n",
      "Agent positions: [(9, 2), (5, 2)], Reward: 0\n",
      "Agent positions: [(8, 2), (5, 1)], Reward: 0\n",
      "Agent positions: [(7, 2), (5, 0)], Reward: 0\n",
      "Agent positions: [(8, 2), (5, 0)], Reward: 0\n",
      "Agent positions: [(8, 1), (5, 0)], Reward: 0\n",
      "Agent positions: [(8, 1), (6, 0)], Reward: 0\n",
      "Agent positions: [(8, 0), (6, 0)], Reward: 0\n",
      "Agent positions: [(8, 0), (6, 0)], Reward: 0\n",
      "Agent positions: [(8, 1), (5, 0)], Reward: 0\n",
      "Agent positions: [(8, 2), (5, 0)], Reward: 0\n",
      "Agent positions: [(9, 2), (6, 0)], Reward: 0\n",
      "Agent positions: [(9, 3), (6, 0)], Reward: 0\n",
      "Agent positions: [(9, 2), (6, 0)], Reward: 0\n",
      "Agent positions: [(8, 2), (6, 0)], Reward: 0\n",
      "Agent positions: [(7, 2), (6, 0)], Reward: 0\n",
      "Agent positions: [(6, 2), (6, 0)], Reward: 0\n",
      "Agent positions: [(5, 2), (6, 0)], Reward: 0\n",
      "Agent positions: [(5, 3), (5, 0)], Reward: 0\n",
      "Agent positions: [(6, 3), (5, 1)], Reward: 0\n",
      "Agent positions: [(5, 3), (5, 1)], Reward: 0\n",
      "Agent positions: [(6, 3), (5, 0)], Reward: 0\n",
      "Agent positions: [(7, 3), (5, 0)], Reward: 0\n",
      "Agent positions: [(6, 3), (6, 0)], Reward: 0\n",
      "Agent positions: [(7, 3), (6, 0)], Reward: 0\n",
      "Agent positions: [(7, 4), (5, 0)], Reward: 0\n",
      "Agent positions: [(7, 5), (6, 0)], Reward: 0\n",
      "Agent positions: [(7, 4), (6, 0)], Reward: 0\n",
      "Agent positions: [(7, 3), (6, 0)], Reward: 0\n",
      "Agent positions: [(8, 3), (6, 0)], Reward: 0\n",
      "Agent positions: [(8, 4), (6, 0)], Reward: 0\n",
      "Agent positions: [(8, 5), (6, 0)], Reward: 0\n",
      "Agent positions: [(8, 4), (6, 0)], Reward: 0\n",
      "Agent positions: [(9, 4), (6, 0)], Reward: 0\n",
      "Agent positions: [(8, 4), (6, 0)], Reward: 0\n",
      "Agent positions: [(8, 3), (6, 0)], Reward: 0\n",
      "Agent positions: [(8, 2), (6, 0)], Reward: 0\n",
      "Agent positions: [(8, 1), (5, 0)], Reward: 0\n",
      "Agent positions: [(8, 2), (5, 1)], Reward: 0\n",
      "Agent positions: [(8, 1), (5, 1)], Reward: 0\n",
      "Agent positions: [(7, 1), (5, 1)], Reward: 0\n",
      "Agent positions: [(7, 2), (5, 0)], Reward: 0\n",
      "Agent positions: [(6, 2), (5, 0)], Reward: 0\n",
      "Agent positions: [(5, 2), (5, 0)], Reward: 0\n",
      "Agent positions: [(5, 3), (5, 1)], Reward: 0\n",
      "Agent positions: [(5, 2), (5, 0)], Reward: 0\n",
      "Agent positions: [(6, 2), (6, 0)], Reward: 0\n",
      "Agent positions: [(6, 3), (6, 0)], Reward: 0\n",
      "Agent positions: [(7, 3), (5, 0)], Reward: 0\n",
      "Agent positions: [(7, 4), (5, 0)], Reward: 0\n",
      "Agent positions: [(8, 4), (5, 1)], Reward: 0\n",
      "Agent positions: [(8, 5), (5, 2)], Reward: 0\n",
      "Agent positions: [(9, 5), (5, 3)], Reward: 0\n",
      "Agent positions: [(8, 5), (5, 2)], Reward: 0\n",
      "Agent positions: [(8, 6), (6, 2)], Reward: 0\n",
      "Agent positions: [(9, 6), (7, 2)], Reward: 0\n",
      "Agent positions: [(9, 5), (7, 1)], Reward: 0\n",
      "Agent positions: [(9, 4), (7, 2)], Reward: 0\n",
      "Agent positions: [(8, 4), (8, 2)], Reward: 0\n",
      "Agent positions: [(7, 4), (8, 3)], Reward: -10\n",
      "Agent positions: [(7, 3), (7, 3)], Reward: -10\n",
      "Agent positions: [(6, 3), (7, 2)], Reward: -10\n",
      "Agent positions: [(6, 2), (7, 1)], Reward: -10\n",
      "Agent positions: [(7, 2), (7, 2)], Reward: -10\n",
      "Agent positions: [(6, 2), (6, 2)], Reward: -10\n",
      "Agent positions: [(5, 2), (6, 2)], Reward: -10\n",
      "Agent positions: [(4, 2), (5, 2)], Reward: -10\n",
      "Agent positions: [(4, 3), (6, 2)], Reward: 0\n",
      "Agent positions: [(3, 3), (6, 2)], Reward: 0\n",
      "Agent positions: [(3, 4), (6, 3)], Reward: 0\n",
      "Agent positions: [(3, 5), (6, 2)], Reward: 0\n",
      "Agent positions: [(2, 5), (7, 2)], Reward: 0\n",
      "Agent positions: [(2, 4), (8, 2)], Reward: 0\n",
      "Agent positions: [(2, 5), (8, 3)], Reward: 0\n",
      "Agent positions: [(1, 5), (7, 3)], Reward: 0\n",
      "Agent positions: [(1, 6), (7, 4)], Reward: 0\n",
      "Agent positions: [(1, 5), (6, 4)], Reward: 0\n",
      "Agent positions: [(2, 5), (5, 4)], Reward: 0\n",
      "Agent positions: [(1, 5), (5, 3)], Reward: 0\n",
      "Agent positions: [(2, 5), (5, 2)], Reward: 0\n",
      "Agent positions: [(1, 5), (5, 3)], Reward: 0\n",
      "Agent positions: [(1, 6), (6, 3)], Reward: 0\n",
      "Agent positions: [(1, 5), (7, 3)], Reward: 0\n",
      "Agent positions: [(0, 5), (7, 2)], Reward: 0\n",
      "Agent positions: [(0, 5), (7, 1)], Reward: 0\n",
      "Agent positions: [(0, 5), (7, 1)], Reward: 0\n",
      "Agent positions: [(0, 5), (7, 2)], Reward: 0\n",
      "Agent positions: [(0, 5), (6, 2)], Reward: 0\n",
      "Agent positions: [(0, 5), (7, 2)], Reward: 0\n",
      "Agent positions: [(0, 5), (7, 3)], Reward: 0\n",
      "Agent positions: [(1, 5), (6, 3)], Reward: 0\n",
      "Agent positions: [(0, 5), (7, 3)], Reward: 0\n",
      "Agent positions: [(0, 6), (7, 2)], Reward: 0\n",
      "Agent positions: [(0, 5), (7, 1)], Reward: 0\n",
      "Agent positions: [(0, 5), (7, 1)], Reward: 0\n",
      "Agent positions: [(0, 6), (7, 1)], Reward: 0\n",
      "Agent positions: [(1, 6), (7, 1)], Reward: 0\n",
      "Agent positions: [(1, 5), (7, 1)], Reward: 0\n",
      "Agent positions: [(2, 5), (7, 1)], Reward: 0\n",
      "Agent positions: [(3, 5), (7, 1)], Reward: 0\n",
      "Agent positions: [(3, 4), (7, 2)], Reward: 0\n",
      "Agent positions: [(4, 4), (7, 1)], Reward: 0\n",
      "Agent positions: [(3, 4), (7, 2)], Reward: 0\n",
      "Agent positions: [(3, 3), (7, 1)], Reward: 0\n",
      "Agent positions: [(3, 4), (7, 1)], Reward: 0\n",
      "Agent positions: [(3, 3), (7, 1)], Reward: 0\n",
      "Agent positions: [(2, 3), (7, 2)], Reward: 0\n",
      "Agent positions: [(2, 2), (7, 3)], Reward: 0\n",
      "Agent positions: [(3, 2), (6, 3)], Reward: 0\n",
      "Agent positions: [(2, 2), (6, 4)], Reward: 0\n",
      "Agent positions: [(3, 2), (7, 4)], Reward: 0\n",
      "Agent positions: [(4, 2), (6, 4)], Reward: 0\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Grid and cell configuration\n",
    "GRID_SIZE = 10\n",
    "CELL_SIZE = 50  # pixels per grid cell\n",
    "WINDOW_SIZE = GRID_SIZE * CELL_SIZE\n",
    "NUM_OBSTACLES = 10\n",
    "\n",
    "# Actions available\n",
    "ACTIONS = [\"up\", \"down\", \"left\", \"right\"]\n",
    "\n",
    "class Gridworld:\n",
    "    def __init__(self):\n",
    "        self.grid_size = GRID_SIZE\n",
    "        self.cell_size = CELL_SIZE\n",
    "        # Define goals: agent 0's goal is bottom-right, agent 1's goal is bottom-left\n",
    "        self.goals = [(GRID_SIZE - 1, GRID_SIZE - 1), (0, GRID_SIZE - 1)]\n",
    "        self.obstacles = self.generate_obstacles()\n",
    "        # Both agents start at top-left (0,0)\n",
    "        self.agents = [(0, 0), (0, 0)]\n",
    "        # Penalty for agents being too close\n",
    "        self.proximity_penalty = -10\n",
    "        \n",
    "    def generate_obstacles(self):\n",
    "        obstacles = set()\n",
    "        # Generate random fixed obstacles while avoiding start and goal positions\n",
    "        while len(obstacles) < NUM_OBSTACLES:\n",
    "            x = random.randint(0, self.grid_size - 1)\n",
    "            y = random.randint(0, self.grid_size - 1)\n",
    "            if (x, y) != (0, 0) and (x, y) not in self.goals:\n",
    "                obstacles.add((x, y))\n",
    "        return list(obstacles)\n",
    "        \n",
    "    def reset(self):\n",
    "        # Reset agents to the starting position; obstacles remain fixed\n",
    "        self.agents = [(0, 0), (0, 0)]\n",
    "    \n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Takes a list of actions (one per agent) and updates their positions.\n",
    "        Actions can be one of \"up\", \"down\", \"left\", \"right\".\n",
    "        Returns the new positions and a reward based on proximity and reaching goals.\n",
    "        \"\"\"\n",
    "        new_positions = []\n",
    "        for idx, (x, y) in enumerate(self.agents):\n",
    "            dx, dy = 0, 0\n",
    "            action = actions[idx]\n",
    "            if action == \"up\":\n",
    "                dy = -1\n",
    "            elif action == \"down\":\n",
    "                dy = 1\n",
    "            elif action == \"left\":\n",
    "                dx = -1\n",
    "            elif action == \"right\":\n",
    "                dx = 1\n",
    "            new_x, new_y = x + dx, y + dy\n",
    "            # Check boundaries\n",
    "            if new_x < 0 or new_x >= self.grid_size or new_y < 0 or new_y >= self.grid_size:\n",
    "                new_x, new_y = x, y  # invalid move, stay in place\n",
    "            # Check obstacles; cannot move into an obstacle\n",
    "            if (new_x, new_y) in self.obstacles:\n",
    "                new_x, new_y = x, y\n",
    "            new_positions.append((new_x, new_y))\n",
    "        self.agents = new_positions\n",
    "        \n",
    "        # Compute reward\n",
    "        reward = 0\n",
    "        # Penalty if agents are too close (Euclidean distance < 2)\n",
    "        dist = math.sqrt((self.agents[0][0] - self.agents[1][0])**2 +\n",
    "                         (self.agents[0][1] - self.agents[1][1])**2)\n",
    "        if dist < 2:\n",
    "            reward += self.proximity_penalty\n",
    "        # Reward for reaching the designated goal\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            if pos == self.goals[idx]:\n",
    "                reward += 10  # arbitrary reward for goal achievement\n",
    "        return self.agents, reward\n",
    "    \n",
    "    def render(self, screen):\n",
    "        # Clear screen\n",
    "        screen.fill((255, 255, 255))\n",
    "        \n",
    "        # Draw grid lines\n",
    "        for x in range(0, WINDOW_SIZE, self.cell_size):\n",
    "            pygame.draw.line(screen, (200, 200, 200), (x, 0), (x, WINDOW_SIZE))\n",
    "        for y in range(0, WINDOW_SIZE, self.cell_size):\n",
    "            pygame.draw.line(screen, (200, 200, 200), (0, y), (WINDOW_SIZE, y))\n",
    "        \n",
    "        # Draw obstacles as black rectangles\n",
    "        for obs in self.obstacles:\n",
    "            rect = pygame.Rect(obs[0]*self.cell_size, obs[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (0, 0, 0), rect)\n",
    "        \n",
    "        # Draw goals as green rectangles\n",
    "        for goal in self.goals:\n",
    "            rect = pygame.Rect(goal[0]*self.cell_size, goal[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (0, 255, 0), rect)\n",
    "        \n",
    "        # Draw agents as circles (agent 0 in red, agent 1 in blue)\n",
    "        colors = [(255, 0, 0), (0, 0, 255)]\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            center = (pos[0]*self.cell_size + self.cell_size//2, pos[1]*self.cell_size + self.cell_size//2)\n",
    "            pygame.draw.circle(screen, colors[idx], center, self.cell_size//3)\n",
    "        \n",
    "        pygame.display.flip()\n",
    "\n",
    "def main():\n",
    "    pygame.init()\n",
    "    screen = pygame.display.set_mode((WINDOW_SIZE, WINDOW_SIZE))\n",
    "    pygame.display.set_caption(\"Gridworld RL Framework\")\n",
    "    \n",
    "    gridworld = Gridworld()\n",
    "    clock = pygame.time.Clock()\n",
    "    running = True\n",
    "    \n",
    "    while running:\n",
    "        # Event handling: quit if the window is closed\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "        \n",
    "        # For demonstration, we take random actions for both agents\n",
    "        actions = [random.choice(ACTIONS) for _ in range(2)]\n",
    "        agents, reward = gridworld.step(actions)\n",
    "        print(f\"Agent positions: {agents}, Reward: {reward}\")\n",
    "        \n",
    "        gridworld.render(screen)\n",
    "        clock.tick(15)  # Run at 2 frames per second for visualization purposes\n",
    "        \n",
    "    pygame.quit()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a77cf3a9-ef77-4d71-bb40-69969a67c59f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torch' has no attribute 'fx' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\EZ\\Lib\\site-packages\\torch\\__init__.py:2222\u001b[0m\n\u001b[0;32m   2218\u001b[0m sys\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.classes\u001b[39m\u001b[38;5;124m\"\u001b[39m, classes)\n\u001b[0;32m   2220\u001b[0m \u001b[38;5;66;03m# quantization depends on torch.fx and torch.ops\u001b[39;00m\n\u001b[0;32m   2221\u001b[0m \u001b[38;5;66;03m# Import quantization\u001b[39;00m\n\u001b[1;32m-> 2222\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m quantization \u001b[38;5;28;01mas\u001b[39;00m quantization  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m   2224\u001b[0m \u001b[38;5;66;03m# Import the quasi random sampler\u001b[39;00m\n\u001b[0;32m   2225\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m quasirandom \u001b[38;5;28;01mas\u001b[39;00m quasirandom  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\EZ\\Lib\\site-packages\\torch\\quantization\\__init__.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfuser_method_mappings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mobserver\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquant_type\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization_mappings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\EZ\\Lib\\site-packages\\torch\\quantization\\qconfig.py:9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# flake8: noqa: F401\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mThis file is in the process of migration to `torch/ao/quantization`, and\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mis kept here for compatibility while the migration process is ongoing.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03mhere.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mao\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     _add_module_to_qconfig_obs_ctr,\n\u001b[0;32m     11\u001b[0m     _assert_valid_qconfig,\n\u001b[0;32m     12\u001b[0m     default_activation_only_qconfig,\n\u001b[0;32m     13\u001b[0m     default_debug_qconfig,\n\u001b[0;32m     14\u001b[0m     default_dynamic_qconfig,\n\u001b[0;32m     15\u001b[0m     default_per_channel_qconfig,\n\u001b[0;32m     16\u001b[0m     default_qat_qconfig,\n\u001b[0;32m     17\u001b[0m     default_qat_qconfig_v2,\n\u001b[0;32m     18\u001b[0m     default_qconfig,\n\u001b[0;32m     19\u001b[0m     default_weight_only_qconfig,\n\u001b[0;32m     20\u001b[0m     float16_dynamic_qconfig,\n\u001b[0;32m     21\u001b[0m     float16_static_qconfig,\n\u001b[0;32m     22\u001b[0m     float_qparams_weight_only_qconfig,\n\u001b[0;32m     23\u001b[0m     get_default_qat_qconfig,\n\u001b[0;32m     24\u001b[0m     get_default_qconfig,\n\u001b[0;32m     25\u001b[0m     per_channel_dynamic_qconfig,\n\u001b[0;32m     26\u001b[0m     QConfig,\n\u001b[0;32m     27\u001b[0m     qconfig_equals,\n\u001b[0;32m     28\u001b[0m     QConfigAny,\n\u001b[0;32m     29\u001b[0m     QConfigDynamic,\n\u001b[0;32m     30\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\EZ\\Lib\\site-packages\\torch\\ao\\quantization\\__init__.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfuser_method_mappings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mobserver\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpt2e\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_numeric_debugger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     compare_results,\n\u001b[0;32m     14\u001b[0m     CUSTOM_KEY,\n\u001b[0;32m     15\u001b[0m     extract_results_from_loggers,\n\u001b[0;32m     16\u001b[0m     generate_numeric_debug_handle,\n\u001b[0;32m     17\u001b[0m     NUMERIC_DEBUG_HANDLE_KEY,\n\u001b[0;32m     18\u001b[0m     prepare_for_propagation_comparison,\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpt2e\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     21\u001b[0m     _allow_exported_model_train_eval \u001b[38;5;28;01mas\u001b[39;00m allow_exported_model_train_eval,\n\u001b[0;32m     22\u001b[0m     _move_exported_model_to_eval \u001b[38;5;28;01mas\u001b[39;00m move_exported_model_to_eval,\n\u001b[0;32m     23\u001b[0m     _move_exported_model_to_train \u001b[38;5;28;01mas\u001b[39;00m move_exported_model_to_train,\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\EZ\\Lib\\site-packages\\torch\\ao\\quantization\\pt2e\\_numeric_debugger.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mao\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mns\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_sqnr\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mao\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpt2e\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_control_flow_submodules\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexport\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExportedProgram\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GraphModule, Node\n",
      "File \u001b[1;32m~\\.conda\\envs\\EZ\\Lib\\site-packages\\torch\\ao\\quantization\\pt2e\\graph_utils.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Node\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msource_matcher_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     check_subgraphs_connected,\n\u001b[0;32m     10\u001b[0m     get_source_partitions,\n\u001b[0;32m     11\u001b[0m     SourcePartition,\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     15\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfind_sequential_partitions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_control_flow_submodules\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_equivalent_types\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate_equivalent_types_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     20\u001b[0m ]\n\u001b[0;32m     22\u001b[0m _EQUIVALENT_TYPES: List[Set] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     23\u001b[0m     {torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mConv1d, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mconv1d},\n\u001b[0;32m     24\u001b[0m     {torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mConv2d, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mconv2d},\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     {torch\u001b[38;5;241m.\u001b[39mmul, operator\u001b[38;5;241m.\u001b[39mmul, operator\u001b[38;5;241m.\u001b[39mimul, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmul\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmul_\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     31\u001b[0m ]\n",
      "File \u001b[1;32m~\\.conda\\envs\\EZ\\Lib\\site-packages\\torch\\fx\\passes\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     graph_drawer,\n\u001b[0;32m      3\u001b[0m     graph_manipulation,\n\u001b[0;32m      4\u001b[0m     net_min_base,\n\u001b[0;32m      5\u001b[0m     operator_support,\n\u001b[0;32m      6\u001b[0m     param_fetch,\n\u001b[0;32m      7\u001b[0m     reinplace,\n\u001b[0;32m      8\u001b[0m     runtime_assert,\n\u001b[0;32m      9\u001b[0m     shape_prop,\n\u001b[0;32m     10\u001b[0m     split_module,\n\u001b[0;32m     11\u001b[0m     split_utils,\n\u001b[0;32m     12\u001b[0m     splitter_base,\n\u001b[0;32m     13\u001b[0m     tools_common,\n\u001b[0;32m     14\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\EZ\\Lib\\site-packages\\torch\\fx\\passes\\graph_drawer.py:13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnode\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _format_arg, _get_qualified_name\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moperator_schemas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m normalize_function\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshape_prop\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorMetadata\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydot\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\EZ\\Lib\\site-packages\\torch\\fx\\passes\\shape_prop.py:85\u001b[0m\n\u001b[0;32m     77\u001b[0m             qparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maxis\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mq_per_channel_axis()  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TensorMetadata(\n\u001b[0;32m     80\u001b[0m         shape, dtype, requires_grad, stride, memory_format, is_quantized, qparams\n\u001b[0;32m     81\u001b[0m     )\n\u001b[0;32m     84\u001b[0m \u001b[38;5;129m@compatibility\u001b[39m(is_backward_compatible\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mShapeProp\u001b[39;00m(torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mInterpreter):\n\u001b[0;32m     86\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124;03m    Execute an FX graph Node-by-Node and\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03m    record the shape and type of the result\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    129\u001b[0m \n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gm, fake_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'torch' has no attribute 'fx' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pygame\n",
    "\n",
    "# Constants for gridworld\n",
    "GRID_SIZE = 10\n",
    "CELL_SIZE = 50  # pixels per cell\n",
    "WINDOW_SIZE = GRID_SIZE * CELL_SIZE\n",
    "NUM_OBSTACLES = 10\n",
    "\n",
    "# ----------------------------\n",
    "# Environment (Gridworld)\n",
    "# ----------------------------\n",
    "class Gridworld:\n",
    "    def __init__(self):\n",
    "        self.grid_size = GRID_SIZE\n",
    "        self.cell_size = CELL_SIZE\n",
    "        # Define goals: agent 0's goal is bottom-right, agent 1's goal is bottom-left\n",
    "        self.goals = [(GRID_SIZE - 1, GRID_SIZE - 1), (0, GRID_SIZE - 1)]\n",
    "        self.obstacles = self.generate_obstacles()\n",
    "        # Both agents start at top-left (0,0)\n",
    "        self.agents = [(0, 0), (0, 0)]\n",
    "        # Penalty if agents are too close\n",
    "        self.proximity_penalty = -10\n",
    "\n",
    "    def generate_obstacles(self):\n",
    "        obstacles = set()\n",
    "        # Generate random fixed obstacles while avoiding start and goal positions\n",
    "        while len(obstacles) < NUM_OBSTACLES:\n",
    "            x = random.randint(0, self.grid_size - 1)\n",
    "            y = random.randint(0, self.grid_size - 1)\n",
    "            if (x, y) != (0, 0) and (x, y) not in self.goals:\n",
    "                obstacles.add((x, y))\n",
    "        return list(obstacles)\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset agents to starting positions; obstacles remain fixed.\n",
    "        self.agents = [(0, 0), (0, 0)]\n",
    "        return self.get_states()\n",
    "\n",
    "    def get_states(self):\n",
    "        # For each agent, state = [agent_x, agent_y, goal_x, goal_y]\n",
    "        states = []\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            goal = self.goals[idx]\n",
    "            states.append([pos[0], pos[1], goal[0], goal[1]])\n",
    "        return states\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Takes a list of actions (one per agent, where:\n",
    "           0: up, 1: down, 2: left, 3: right)\n",
    "        and updates their positions.\n",
    "        Returns:\n",
    "           next_states: list of states for each agent\n",
    "           reward: a scalar reward computed from proximity and goal achievements\n",
    "           done: True if both agents have reached their goals\n",
    "        \"\"\"\n",
    "        new_positions = []\n",
    "        for idx, (x, y) in enumerate(self.agents):\n",
    "            dx, dy = 0, 0\n",
    "            action = actions[idx]\n",
    "            if action == 0:  # up\n",
    "                dy = -1\n",
    "            elif action == 1:  # down\n",
    "                dy = 1\n",
    "            elif action == 2:  # left\n",
    "                dx = -1\n",
    "            elif action == 3:  # right\n",
    "                dx = 1\n",
    "            new_x, new_y = x + dx, y + dy\n",
    "            # Check boundaries\n",
    "            if new_x < 0 or new_x >= self.grid_size or new_y < 0 or new_y >= self.grid_size:\n",
    "                new_x, new_y = x, y  # invalid move, stay in place\n",
    "            # Check obstacles; if new cell is an obstacle, agent remains in place\n",
    "            if (new_x, new_y) in self.obstacles:\n",
    "                new_x, new_y = x, y\n",
    "            new_positions.append((new_x, new_y))\n",
    "        self.agents = new_positions\n",
    "\n",
    "        # Compute reward and check termination\n",
    "        reward = 0\n",
    "        done = False\n",
    "        # Apply proximity penalty if agents are too close (Euclidean distance < 2)\n",
    "        dist = math.sqrt((self.agents[0][0] - self.agents[1][0])**2 +\n",
    "                         (self.agents[0][1] - self.agents[1][1])**2)\n",
    "        if dist < 2:\n",
    "            reward += self.proximity_penalty\n",
    "        # Reward for each agent reaching its goal\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            if pos == self.goals[idx]:\n",
    "                reward += 10  # arbitrary reward for goal achievement\n",
    "\n",
    "        # Episode ends if both agents have reached their goals\n",
    "        if self.agents[0] == self.goals[0] and self.agents[1] == self.goals[1]:\n",
    "            done = True\n",
    "\n",
    "        next_states = self.get_states()\n",
    "        return next_states, reward, done\n",
    "\n",
    "    def render(self, screen):\n",
    "        # Clear screen\n",
    "        screen.fill((255, 255, 255))\n",
    "        # Draw grid lines\n",
    "        for x in range(0, WINDOW_SIZE, self.cell_size):\n",
    "            pygame.draw.line(screen, (200, 200, 200), (x, 0), (x, WINDOW_SIZE))\n",
    "        for y in range(0, WINDOW_SIZE, self.cell_size):\n",
    "            pygame.draw.line(screen, (200, 200, 200), (0, y), (WINDOW_SIZE, y))\n",
    "        # Draw obstacles (black)\n",
    "        for obs in self.obstacles:\n",
    "            rect = pygame.Rect(obs[0]*self.cell_size, obs[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (0, 0, 0), rect)\n",
    "        # Draw goals (green)\n",
    "        for goal in self.goals:\n",
    "            rect = pygame.Rect(goal[0]*self.cell_size, goal[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (0, 255, 0), rect)\n",
    "        # Draw agents (agent 0 in red, agent 1 in blue)\n",
    "        colors = [(255, 0, 0), (0, 0, 255)]\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            center = (pos[0]*self.cell_size + self.cell_size//2,\n",
    "                      pos[1]*self.cell_size + self.cell_size//2)\n",
    "            pygame.draw.circle(screen, colors[idx], center, self.cell_size//3)\n",
    "        pygame.display.flip()\n",
    "\n",
    "# ----------------------------\n",
    "# Replay Buffer for DQN\n",
    "# ----------------------------\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# ----------------------------\n",
    "# DQN Network and Agent\n",
    "# ----------------------------\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_dim, output_dim, lr=1e-3, gamma=0.99,\n",
    "                 buffer_capacity=10000, batch_size=32):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def select_action(self, state, epsilon):\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(4)  # 4 discrete actions\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "        return q_values.argmax().item()\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.replay_buffer.sample(self.batch_size)\n",
    "        batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*transitions)\n",
    "        \n",
    "        batch_state = torch.FloatTensor(batch_state).to(self.device)\n",
    "        batch_action = torch.LongTensor(batch_action).unsqueeze(1).to(self.device)\n",
    "        batch_reward = torch.FloatTensor(batch_reward).unsqueeze(1).to(self.device)\n",
    "        batch_next_state = torch.FloatTensor(batch_next_state).to(self.device)\n",
    "        batch_done = torch.FloatTensor(batch_done).unsqueeze(1).to(self.device)\n",
    "        \n",
    "        current_q = self.policy_net(batch_state).gather(1, batch_action)\n",
    "        next_q = self.target_net(batch_next_state).max(1)[0].unsqueeze(1)\n",
    "        target_q = batch_reward + self.gamma * next_q * (1 - batch_done)\n",
    "        \n",
    "        loss = nn.MSELoss()(current_q, target_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "# ----------------------------\n",
    "# Training Loop\n",
    "# ----------------------------\n",
    "def train():\n",
    "    num_episodes = 500\n",
    "    max_steps = 50\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.1\n",
    "    epsilon_decay = 0.995\n",
    "    target_update_freq = 10\n",
    "    \n",
    "    env = Gridworld()\n",
    "    \n",
    "    # Each agent's state: [agent_x, agent_y, goal_x, goal_y] -> 4 dimensions.\n",
    "    state_dim = 4\n",
    "    action_dim = 4  # up, down, left, right\n",
    "    agents = [DQNAgent(state_dim, action_dim), DQNAgent(state_dim, action_dim)]\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        states = env.reset()\n",
    "        total_reward = 0\n",
    "        for step in range(max_steps):\n",
    "            actions = []\n",
    "            # Each agent selects an action using its own DQN policy\n",
    "            for i in range(2):\n",
    "                action = agents[i].select_action(states[i], epsilon)\n",
    "                actions.append(action)\n",
    "            next_states, reward, done = env.step(actions)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Store experiences for each agent; here we use the same reward for both\n",
    "            for i in range(2):\n",
    "                agents[i].replay_buffer.push(states[i], actions[i], reward, next_states[i], done)\n",
    "            \n",
    "            states = next_states\n",
    "            \n",
    "            # Update each agent's policy network from its replay buffer\n",
    "            for i in range(2):\n",
    "                agents[i].update()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Update target networks periodically\n",
    "        if episode % target_update_freq == 0:\n",
    "            for agent in agents:\n",
    "                agent.update_target()\n",
    "        \n",
    "        # Epsilon decay\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.3f}\")\n",
    "        \n",
    "        # Optional: To visualize training, you can uncomment and run the render code below.\n",
    "        # pygame.init()\n",
    "        # screen = pygame.display.set_mode((WINDOW_SIZE, WINDOW_SIZE))\n",
    "        # env.render(screen)\n",
    "        # pygame.time.wait(200)\n",
    "        # pygame.quit()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0c098f-4d78-4dc0-8fe8-863c497e6577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pygame\n",
    "\n",
    "# Constants for gridworld and visualization\n",
    "GRID_SIZE = 10\n",
    "CELL_SIZE = 50  # pixels per cell\n",
    "WINDOW_SIZE = GRID_SIZE * CELL_SIZE\n",
    "NUM_OBSTACLES = 10\n",
    "\n",
    "# ----------------------------\n",
    "# Environment (Gridworld)\n",
    "# ----------------------------\n",
    "class Gridworld:\n",
    "    def __init__(self):\n",
    "        self.grid_size = GRID_SIZE\n",
    "        self.cell_size = CELL_SIZE\n",
    "        # Define goals: agent 0's goal is bottom-right, agent 1's goal is bottom-left\n",
    "        self.goals = [(GRID_SIZE - 1, GRID_SIZE - 1), (0, GRID_SIZE - 1)]\n",
    "        self.obstacles = self.generate_obstacles()\n",
    "        # Both agents start at top-left (0,0)\n",
    "        self.agents = [(0, 0), (0, 0)]\n",
    "        # Penalty if agents are too close (Euclidean distance < 2)\n",
    "        self.proximity_penalty = -10\n",
    "\n",
    "    def generate_obstacles(self):\n",
    "        obstacles = set()\n",
    "        # Generate random fixed obstacles while avoiding start and goal positions\n",
    "        while len(obstacles) < NUM_OBSTACLES:\n",
    "            x = random.randint(0, self.grid_size - 1)\n",
    "            y = random.randint(0, self.grid_size - 1)\n",
    "            if (x, y) != (0, 0) and (x, y) not in self.goals:\n",
    "                obstacles.add((x, y))\n",
    "        return list(obstacles)\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset agents to starting positions; obstacles remain fixed.\n",
    "        self.agents = [(0, 0), (0, 0)]\n",
    "        return self.get_states()\n",
    "\n",
    "    def get_states(self):\n",
    "        # Each agent's state: [agent_x, agent_y, goal_x, goal_y]\n",
    "        states = []\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            goal = self.goals[idx]\n",
    "            states.append([pos[0], pos[1], goal[0], goal[1]])\n",
    "        return states\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Expects a list of actions (one per agent: 0: up, 1: down, 2: left, 3: right).\n",
    "        Updates agent positions and returns:\n",
    "          - next_states: list of states for each agent,\n",
    "          - reward: a scalar reward (includes a penalty for proximity),\n",
    "          - done: True if both agents have reached their goals.\n",
    "        \"\"\"\n",
    "        new_positions = []\n",
    "        for idx, (x, y) in enumerate(self.agents):\n",
    "            dx, dy = 0, 0\n",
    "            action = actions[idx]\n",
    "            if action == 0:  # up\n",
    "                dy = -1\n",
    "            elif action == 1:  # down\n",
    "                dy = 1\n",
    "            elif action == 2:  # left\n",
    "                dx = -1\n",
    "            elif action == 3:  # right\n",
    "                dx = 1\n",
    "            new_x, new_y = x + dx, y + dy\n",
    "            # Check boundaries; invalid moves result in staying in place.\n",
    "            if new_x < 0 or new_x >= self.grid_size or new_y < 0 or new_y >= self.grid_size:\n",
    "                new_x, new_y = x, y\n",
    "            # Check obstacles; if moving into an obstacle, agent remains in place.\n",
    "            if (new_x, new_y) in self.obstacles:\n",
    "                new_x, new_y = x, y\n",
    "            new_positions.append((new_x, new_y))\n",
    "        self.agents = new_positions\n",
    "\n",
    "        # Compute reward and termination condition\n",
    "        reward = 0\n",
    "        done = False\n",
    "        # Penalty if agents are too close (Euclidean distance < 2)\n",
    "        dist = math.sqrt((self.agents[0][0] - self.agents[1][0])**2 +\n",
    "                         (self.agents[0][1] - self.agents[1][1])**2)\n",
    "        if dist < 2:\n",
    "            reward += self.proximity_penalty\n",
    "        # Reward for each agent reaching its goal\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            if pos == self.goals[idx]:\n",
    "                reward += 10\n",
    "\n",
    "        if self.agents[0] == self.goals[0] and self.agents[1] == self.goals[1]:\n",
    "            done = True\n",
    "\n",
    "        next_states = self.get_states()\n",
    "        return next_states, reward, done\n",
    "\n",
    "    def render(self, screen):\n",
    "        # Clear screen\n",
    "        screen.fill((255, 255, 255))\n",
    "        # Draw grid lines\n",
    "        for x in range(0, WINDOW_SIZE, self.cell_size):\n",
    "            pygame.draw.line(screen, (200, 200, 200), (x, 0), (x, WINDOW_SIZE))\n",
    "        for y in range(0, WINDOW_SIZE, self.cell_size):\n",
    "            pygame.draw.line(screen, (200, 200, 200), (0, y), (WINDOW_SIZE, y))\n",
    "        # Draw obstacles as black rectangles\n",
    "        for obs in self.obstacles:\n",
    "            rect = pygame.Rect(obs[0]*self.cell_size, obs[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (0, 0, 0), rect)\n",
    "        # Draw goals as green rectangles\n",
    "        for goal in self.goals:\n",
    "            rect = pygame.Rect(goal[0]*self.cell_size, goal[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (0, 255, 0), rect)\n",
    "        # Draw agents as circles (agent 0 in red, agent 1 in blue)\n",
    "        colors = [(255, 0, 0), (0, 0, 255)]\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            center = (pos[0]*self.cell_size + self.cell_size//2,\n",
    "                      pos[1]*self.cell_size + self.cell_size//2)\n",
    "            pygame.draw.circle(screen, colors[idx], center, self.cell_size//3)\n",
    "        pygame.display.flip()\n",
    "\n",
    "# ----------------------------\n",
    "# Replay Buffer for DQN\n",
    "# ----------------------------\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# ----------------------------\n",
    "# DQN Network and Agent\n",
    "# ----------------------------\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_dim, output_dim, lr=1e-3, gamma=0.99,\n",
    "                 buffer_capacity=10000, batch_size=32):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def select_action(self, state, epsilon):\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(4)  # 4 discrete actions\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "        return q_values.argmax().item()\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.replay_buffer.sample(self.batch_size)\n",
    "        batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*transitions)\n",
    "        \n",
    "        batch_state = torch.FloatTensor(batch_state).to(self.device)\n",
    "        batch_action = torch.LongTensor(batch_action).unsqueeze(1).to(self.device)\n",
    "        batch_reward = torch.FloatTensor(batch_reward).unsqueeze(1).to(self.device)\n",
    "        batch_next_state = torch.FloatTensor(batch_next_state).to(self.device)\n",
    "        batch_done = torch.FloatTensor(batch_done).unsqueeze(1).to(self.device)\n",
    "        \n",
    "        current_q = self.policy_net(batch_state).gather(1, batch_action)\n",
    "        next_q = self.target_net(batch_next_state).max(1)[0].unsqueeze(1)\n",
    "        target_q = batch_reward + self.gamma * next_q * (1 - batch_done)\n",
    "        \n",
    "        loss = nn.MSELoss()(current_q, target_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "# ----------------------------\n",
    "# Training Loop with Milestone Rendering\n",
    "# ----------------------------\n",
    "def train():\n",
    "    num_episodes = 500\n",
    "    max_steps = 50\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.1\n",
    "    epsilon_decay = 0.995\n",
    "    target_update_freq = 10\n",
    "    # Define milestone episodes (25%, 50%, 75%, and 100% of training)\n",
    "    milestones = {int(num_episodes * 0.25), int(num_episodes * 0.5), int(num_episodes * 0.75), num_episodes}\n",
    "    \n",
    "    env = Gridworld()\n",
    "    \n",
    "    # Each agent's state has 4 elements: [agent_x, agent_y, goal_x, goal_y]\n",
    "    state_dim = 4\n",
    "    action_dim = 4  # up, down, left, right\n",
    "    agents = [DQNAgent(state_dim, action_dim), DQNAgent(state_dim, action_dim)]\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        states = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        # Check if this is a milestone episode (episode indices are 0-based)\n",
    "        do_render = (episode + 1) in milestones\n",
    "        if do_render:\n",
    "            pygame.init()\n",
    "            screen = pygame.display.set_mode((WINDOW_SIZE, WINDOW_SIZE))\n",
    "            pygame.display.set_caption(f\"Training Episode {episode + 1}\")\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            actions = []\n",
    "            # Each agent selects an action using its own DQN policy\n",
    "            for i in range(2):\n",
    "                action = agents[i].select_action(states[i], epsilon)\n",
    "                actions.append(action)\n",
    "            next_states, reward, done = env.step(actions)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Store experiences for each agent\n",
    "            for i in range(2):\n",
    "                agents[i].replay_buffer.push(states[i], actions[i], reward, next_states[i], done)\n",
    "            \n",
    "            states = next_states\n",
    "            \n",
    "            # Update each agent's policy network\n",
    "            for i in range(2):\n",
    "                agents[i].update()\n",
    "            \n",
    "            if do_render:\n",
    "                env.render(screen)\n",
    "                # Process quit events in visualization mode\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.QUIT:\n",
    "                        pygame.quit()\n",
    "                        return\n",
    "                pygame.time.wait(200)\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        if do_render:\n",
    "            # Pause to allow viewing the final state of the milestone episode\n",
    "            pygame.time.wait(1000)\n",
    "            pygame.quit()\n",
    "        \n",
    "        # Update target networks periodically\n",
    "        if (episode + 1) % target_update_freq == 0:\n",
    "            for agent in agents:\n",
    "                agent.update_target()\n",
    "        \n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        print(f\"Episode {episode + 1}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a49002d-0c9d-4774-8d34-b0128d4534da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pygame\n",
    "\n",
    "# Constants for gridworld and visualization\n",
    "GRID_SIZE = 10\n",
    "CELL_SIZE = 50   # pixels per cell\n",
    "WINDOW_SIZE = GRID_SIZE * CELL_SIZE\n",
    "NUM_OBSTACLES = 10\n",
    "\n",
    "# ----------------------------\n",
    "# Environment (Gridworld)\n",
    "# ----------------------------\n",
    "class Gridworld:\n",
    "    def __init__(self):\n",
    "        self.grid_size = GRID_SIZE\n",
    "        self.cell_size = CELL_SIZE\n",
    "        self.vision_radius = 5  # drones can sense obstacles within 5 cells\n",
    "        # Define goals: agent 0's goal is bottom-right, agent 1's goal is bottom-left\n",
    "        self.goals = [(GRID_SIZE - 1, GRID_SIZE - 1), (0, GRID_SIZE - 1)]\n",
    "        self.obstacles = self.generate_obstacles()\n",
    "        # Both agents start at top-left (0,0)\n",
    "        self.agents = [(0, 0), (0, 0)]\n",
    "        # Penalty values\n",
    "        self.movement_penalty = -1       # per move per agent\n",
    "        self.closeness_penalty = -10     # if Manhattan distance < 2\n",
    "        self.collision_penalty = -50     # if agent attempts to move into an obstacle\n",
    "\n",
    "    def generate_obstacles(self):\n",
    "        obstacles = set()\n",
    "        # Generate random fixed obstacles while avoiding start and goal positions\n",
    "        while len(obstacles) < NUM_OBSTACLES:\n",
    "            x = random.randint(0, self.grid_size - 1)\n",
    "            y = random.randint(0, self.grid_size - 1)\n",
    "            if (x, y) != (0, 0) and (x, y) not in self.goals:\n",
    "                obstacles.add((x, y))\n",
    "        return list(obstacles)\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset agents to starting positions; obstacles remain fixed.\n",
    "        self.agents = [(0, 0), (0, 0)]\n",
    "        return self.get_states()\n",
    "\n",
    "    def get_states(self):\n",
    "        # Each agent's state: [agent_x, agent_y, goal_x, goal_y]\n",
    "        # (Extend this in the future with vision info if desired)\n",
    "        states = []\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            goal = self.goals[idx]\n",
    "            states.append([pos[0], pos[1], goal[0], goal[1]])\n",
    "        return states\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Expects a list of actions (one per agent: 0: up, 1: down, 2: left, 3: right).\n",
    "        Updates agent positions and returns:\n",
    "          - next_states: list of states for each agent,\n",
    "          - reward: a scalar reward that includes:\n",
    "                * movement penalty,\n",
    "                * collision penalty (if agent attempts to move into an obstacle),\n",
    "                * closeness penalty (if Manhattan distance < 2),\n",
    "                * goal achievement rewards.\n",
    "          - done: True if both agents have reached their goals.\n",
    "        \"\"\"\n",
    "        reward = 0\n",
    "        # Apply movement penalty for each agent regardless of move validity.\n",
    "        reward += self.movement_penalty * len(self.agents)\n",
    "        \n",
    "        new_positions = []\n",
    "        for idx, (x, y) in enumerate(self.agents):\n",
    "            dx, dy = 0, 0\n",
    "            action = actions[idx]\n",
    "            if action == 0:  # up\n",
    "                dy = -1\n",
    "            elif action == 1:  # down\n",
    "                dy = 1\n",
    "            elif action == 2:  # left\n",
    "                dx = -1\n",
    "            elif action == 3:  # right\n",
    "                dx = 1\n",
    "            new_x, new_y = x + dx, y + dy\n",
    "            # Check boundaries; invalid moves result in staying in place.\n",
    "            if new_x < 0 or new_x >= self.grid_size or new_y < 0 or new_y >= self.grid_size:\n",
    "                new_x, new_y = x, y\n",
    "            # Check obstacles; if moving into an obstacle, apply collision penalty and stay in place.\n",
    "            if (new_x, new_y) in self.obstacles:\n",
    "                reward += self.collision_penalty\n",
    "                new_x, new_y = x, y\n",
    "            new_positions.append((new_x, new_y))\n",
    "        self.agents = new_positions\n",
    "\n",
    "        # Compute closeness penalty based on Manhattan distance between agents.\n",
    "        manhattan_distance = abs(self.agents[0][0] - self.agents[1][0]) + abs(self.agents[0][1] - self.agents[1][1])\n",
    "        if manhattan_distance < 2:\n",
    "            reward += self.closeness_penalty\n",
    "\n",
    "        # Reward for each agent reaching its goal.\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            if pos == self.goals[idx]:\n",
    "                reward += 10  # goal reward\n",
    "\n",
    "        # Episode ends if both agents have reached their goals.\n",
    "        done = (self.agents[0] == self.goals[0] and self.agents[1] == self.goals[1])\n",
    "        next_states = self.get_states()\n",
    "        return next_states, reward, done\n",
    "\n",
    "    def render(self, screen):\n",
    "        # Clear screen\n",
    "        screen.fill((255, 255, 255))\n",
    "        # Draw grid lines\n",
    "        for x in range(0, WINDOW_SIZE, self.cell_size):\n",
    "            pygame.draw.line(screen, (200, 200, 200), (x, 0), (x, WINDOW_SIZE))\n",
    "        for y in range(0, WINDOW_SIZE, self.cell_size):\n",
    "            pygame.draw.line(screen, (200, 200, 200), (0, y), (WINDOW_SIZE, y))\n",
    "        # Draw obstacles as black rectangles\n",
    "        for obs in self.obstacles:\n",
    "            rect = pygame.Rect(obs[0]*self.cell_size, obs[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (0, 0, 0), rect)\n",
    "        # Draw goals as green rectangles\n",
    "        for goal in self.goals:\n",
    "            rect = pygame.Rect(goal[0]*self.cell_size, goal[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (0, 255, 0), rect)\n",
    "        # Draw agents as circles (agent 0 in red, agent 1 in blue)\n",
    "        colors = [(255, 0, 0), (0, 0, 255)]\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            center = (pos[0]*self.cell_size + self.cell_size//2,\n",
    "                      pos[1]*self.cell_size + self.cell_size//2)\n",
    "            pygame.draw.circle(screen, colors[idx], center, self.cell_size//3)\n",
    "        pygame.display.flip()\n",
    "\n",
    "# ----------------------------\n",
    "# Replay Buffer for DQN\n",
    "# ----------------------------\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# ----------------------------\n",
    "# DQN Network and Agent\n",
    "# ----------------------------\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_dim, output_dim, lr=1e-3, gamma=0.99,\n",
    "                 buffer_capacity=10000, batch_size=32):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def select_action(self, state, epsilon):\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(4)  # 4 discrete actions: up, down, left, right\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "        return q_values.argmax().item()\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.replay_buffer.sample(self.batch_size)\n",
    "        batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*transitions)\n",
    "        \n",
    "        batch_state = torch.FloatTensor(batch_state).to(self.device)\n",
    "        batch_action = torch.LongTensor(batch_action).unsqueeze(1).to(self.device)\n",
    "        batch_reward = torch.FloatTensor(batch_reward).unsqueeze(1).to(self.device)\n",
    "        batch_next_state = torch.FloatTensor(batch_next_state).to(self.device)\n",
    "        batch_done = torch.FloatTensor(batch_done).unsqueeze(1).to(self.device)\n",
    "        \n",
    "        current_q = self.policy_net(batch_state).gather(1, batch_action)\n",
    "        next_q = self.target_net(batch_next_state).max(1)[0].unsqueeze(1)\n",
    "        target_q = batch_reward + self.gamma * next_q * (1 - batch_done)\n",
    "        \n",
    "        loss = nn.MSELoss()(current_q, target_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "# ----------------------------\n",
    "# Training Loop with Milestone Rendering\n",
    "# ----------------------------\n",
    "def train():\n",
    "    num_episodes = 500\n",
    "    max_steps = 50\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.1\n",
    "    epsilon_decay = 0.995\n",
    "    target_update_freq = 10\n",
    "    # Define milestone episodes (25%, 50%, 75%, and 100% of training)\n",
    "    milestones = {int(num_episodes * 0.25), int(num_episodes * 0.5), int(num_episodes * 0.75), num_episodes}\n",
    "    \n",
    "    env = Gridworld()\n",
    "    \n",
    "    # Each agent's state has 4 elements: [agent_x, agent_y, goal_x, goal_y]\n",
    "    state_dim = 4\n",
    "    action_dim = 4  # up, down, left, right\n",
    "    agents = [DQNAgent(state_dim, action_dim), DQNAgent(state_dim, action_dim)]\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        states = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        # Check if this is a milestone episode (episode indices are 0-based)\n",
    "        do_render = (episode + 1) in milestones\n",
    "        if do_render:\n",
    "            pygame.init()\n",
    "            screen = pygame.display.set_mode((WINDOW_SIZE, WINDOW_SIZE))\n",
    "            pygame.display.set_caption(f\"Training Episode {episode + 1}\")\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            actions = []\n",
    "            # Each agent selects an action using its own DQN policy\n",
    "            for i in range(2):\n",
    "                action = agents[i].select_action(states[i], epsilon)\n",
    "                actions.append(action)\n",
    "            next_states, reward, done = env.step(actions)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Store experiences for each agent\n",
    "            for i in range(2):\n",
    "                agents[i].replay_buffer.push(states[i], actions[i], reward, next_states[i], done)\n",
    "            \n",
    "            states = next_states\n",
    "            \n",
    "            # Update each agent's policy network\n",
    "            for i in range(2):\n",
    "                agents[i].update()\n",
    "            \n",
    "            if do_render:\n",
    "                env.render(screen)\n",
    "                # Process quit events in visualization mode\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.QUIT:\n",
    "                        pygame.quit()\n",
    "                        return\n",
    "                pygame.time.wait(200)\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        if do_render:\n",
    "            # Pause to allow viewing the final state of the milestone episode\n",
    "            pygame.time.wait(1000)\n",
    "            pygame.quit()\n",
    "        \n",
    "        # Update target networks periodically\n",
    "        if (episode + 1) % target_update_freq == 0:\n",
    "            for agent in agents:\n",
    "                agent.update_target()\n",
    "        \n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        print(f\"Episode {episode + 1}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913de779-d8fc-4bd5-9d7a-7e744d0e0c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pygame\n",
    "\n",
    "# Constants for gridworld and visualization\n",
    "GRID_SIZE = 15\n",
    "CELL_SIZE = 100   # pixels per cell\n",
    "WINDOW_SIZE = GRID_SIZE * CELL_SIZE\n",
    "NUM_OBSTACLES = 10\n",
    "\n",
    "# ----------------------------\n",
    "# Environment (Gridworld)\n",
    "# ----------------------------\n",
    "class Gridworld:\n",
    "    def __init__(self):\n",
    "        self.grid_size = GRID_SIZE\n",
    "        self.cell_size = CELL_SIZE\n",
    "        self.vision_radius = 5  # drones can sense obstacles within 5 cells\n",
    "        # Define goals: agent 0's goal is bottom-right, agent 1's goal is bottom-left\n",
    "        self.goals = [(GRID_SIZE - 1, GRID_SIZE - 1), (0, GRID_SIZE - 1)]\n",
    "        self.obstacles = self.generate_obstacles()\n",
    "        # Both agents start at top-left (0,0)\n",
    "        self.agents = [(0, 0), (0, 0)]\n",
    "        # Penalty and reward values\n",
    "        self.movement_penalty = -1       # per move per agent\n",
    "        self.closeness_penalty = -10     # if Manhattan distance < 2\n",
    "        self.collision_penalty = -50     # if agent attempts to move into an obstacle\n",
    "        self.goal_reward = 50            # increased goal reward to incentivize reaching the goal\n",
    "        self.shaping_factor = 0.5        # reward shaping factor for progress towards goal\n",
    "\n",
    "    def generate_obstacles(self):\n",
    "        obstacles = set()\n",
    "        # Generate random fixed obstacles while avoiding start and goal positions\n",
    "        while len(obstacles) < NUM_OBSTACLES:\n",
    "            x = random.randint(0, self.grid_size - 1)\n",
    "            y = random.randint(0, self.grid_size - 1)\n",
    "            if (x, y) != (0, 0) and (x, y) not in self.goals:\n",
    "                obstacles.add((x, y))\n",
    "        return list(obstacles)\n",
    "\n",
    "    def get_obstacle_view(self, agent_position):\n",
    "        view = []\n",
    "        ax, ay = agent_position\n",
    "        for dy in range(-self.vision_radius, self.vision_radius + 1):\n",
    "            for dx in range(-self.vision_radius, self.vision_radius + 1):\n",
    "                cx = ax + dx\n",
    "                cy = ay + dy\n",
    "                if 0 <= cx < self.grid_size and 0 <= cy < self.grid_size:\n",
    "                    if (cx, cy) in self.obstacles:\n",
    "                        view.append(1.0)\n",
    "                    else:\n",
    "                        view.append(0.0)\n",
    "                else:\n",
    "                    view.append(0.0)\n",
    "        return view\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset agents to starting positions; obstacles remain fixed.\n",
    "        self.agents = [(0, 0), (0, 0)]\n",
    "        return self.get_states()\n",
    "\n",
    "    def get_states(self):\n",
    "        # Each agent's state: [agent_x, agent_y, goal_x, goal_y] + flattened obstacle view\n",
    "        states = []\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            goal = self.goals[idx]\n",
    "            base_state = [pos[0], pos[1], goal[0], goal[1]]\n",
    "            obstacle_view = self.get_obstacle_view(pos)\n",
    "            states.append(base_state + obstacle_view)\n",
    "        return states\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Expects a list of actions (one per agent: 0: up, 1: down, 2: left, 3: right).\n",
    "        Updates agent positions and returns:\n",
    "          - next_states: list of states for each agent,\n",
    "          - reward: a scalar reward that includes:\n",
    "                * movement penalty,\n",
    "                * collision penalty (if agent attempts to move into an obstacle),\n",
    "                * reward shaping (for progress toward the goal),\n",
    "                * closeness penalty (if Manhattan distance < 2),\n",
    "                * goal achievement rewards.\n",
    "          - done: True if both agents have reached their goals.\n",
    "        \"\"\"\n",
    "        reward = 0\n",
    "        # Apply movement penalty for each agent\n",
    "        reward += self.movement_penalty * len(self.agents)\n",
    "        \n",
    "        # Compute old Manhattan distances for reward shaping\n",
    "        old_distances = []\n",
    "        for idx, (x, y) in enumerate(self.agents):\n",
    "            goal = self.goals[idx]\n",
    "            old_distance = abs(x - goal[0]) + abs(y - goal[1])\n",
    "            old_distances.append(old_distance)\n",
    "        \n",
    "        new_positions = []\n",
    "        for idx, (x, y) in enumerate(self.agents):\n",
    "            dx, dy = 0, 0\n",
    "            action = actions[idx]\n",
    "            if action == 0:  # up\n",
    "                dy = -1\n",
    "            elif action == 1:  # down\n",
    "                dy = 1\n",
    "            elif action == 2:  # left\n",
    "                dx = -1\n",
    "            elif action == 3:  # right\n",
    "                dx = 1\n",
    "            new_x, new_y = x + dx, y + dy\n",
    "            # Check boundaries; invalid moves result in staying in place.\n",
    "            if new_x < 0 or new_x >= self.grid_size or new_y < 0 or new_y >= self.grid_size:\n",
    "                new_x, new_y = x, y\n",
    "            # Check obstacles; if moving into an obstacle, apply collision penalty and stay in place.\n",
    "            if (new_x, new_y) in self.obstacles:\n",
    "                reward += self.collision_penalty\n",
    "                new_x, new_y = x, y\n",
    "            new_positions.append((new_x, new_y))\n",
    "        self.agents = new_positions\n",
    "\n",
    "        # Reward shaping: add reward proportional to progress toward goal\n",
    "        for idx, (x, y) in enumerate(self.agents):\n",
    "            goal = self.goals[idx]\n",
    "            new_distance = abs(x - goal[0]) + abs(y - goal[1])\n",
    "            # If agent gets closer, the difference (old - new) is positive.\n",
    "            shaping_reward = self.shaping_factor * (old_distances[idx] - new_distance)\n",
    "            reward += shaping_reward\n",
    "\n",
    "        # Closeness penalty: if Manhattan distance between agents < 2\n",
    "        manhattan_distance = abs(self.agents[0][0] - self.agents[1][0]) + abs(self.agents[0][1] - self.agents[1][1])\n",
    "        if manhattan_distance < 2:\n",
    "            reward += self.closeness_penalty\n",
    "\n",
    "        # Reward for reaching the goal for each agent.\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            if pos == self.goals[idx]:\n",
    "                reward += self.goal_reward\n",
    "\n",
    "        # Episode ends if both agents have reached their goals.\n",
    "        done = (self.agents[0] == self.goals[0] and self.agents[1] == self.goals[1])\n",
    "        next_states = self.get_states()\n",
    "        return next_states, reward, done\n",
    "\n",
    "    def render(self, screen):\n",
    "        # Clear screen\n",
    "        screen.fill((255, 255, 255))\n",
    "        # Draw grid lines\n",
    "        for x in range(0, WINDOW_SIZE, self.cell_size):\n",
    "            pygame.draw.line(screen, (200, 200, 200), (x, 0), (x, WINDOW_SIZE))\n",
    "        for y in range(0, WINDOW_SIZE, self.cell_size):\n",
    "            pygame.draw.line(screen, (200, 200, 200), (0, y), (WINDOW_SIZE, y))\n",
    "        # Draw obstacles as black rectangles\n",
    "        for obs in self.obstacles:\n",
    "            rect = pygame.Rect(obs[0]*self.cell_size, obs[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (0, 0, 0), rect)\n",
    "        # Draw goals as green rectangles\n",
    "        for goal in self.goals:\n",
    "            rect = pygame.Rect(goal[0]*self.cell_size, goal[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (0, 255, 0), rect)\n",
    "        # Draw agents as circles (agent 0 in red, agent 1 in blue)\n",
    "        colors = [(255, 0, 0), (0, 0, 255)]\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            center = (pos[0]*self.cell_size + self.cell_size//2,\n",
    "                      pos[1]*self.cell_size + self.cell_size//2)\n",
    "            pygame.draw.circle(screen, colors[idx], center, self.cell_size//3)\n",
    "        pygame.display.flip()\n",
    "\n",
    "# ----------------------------\n",
    "# Replay Buffer for DQN\n",
    "# ----------------------------\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# ----------------------------\n",
    "# DQN Network and Agent\n",
    "# ----------------------------\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_dim, output_dim, lr=1e-3, gamma=0.99,\n",
    "                 buffer_capacity=10000, batch_size=32):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def select_action(self, state, epsilon):\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(4)  # 4 discrete actions: up, down, left, right\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "        return q_values.argmax().item()\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.replay_buffer.sample(self.batch_size)\n",
    "        batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*transitions)\n",
    "        \n",
    "        batch_state = torch.FloatTensor(batch_state).to(self.device)\n",
    "        batch_action = torch.LongTensor(batch_action).unsqueeze(1).to(self.device)\n",
    "        batch_reward = torch.FloatTensor(batch_reward).unsqueeze(1).to(self.device)\n",
    "        batch_next_state = torch.FloatTensor(batch_next_state).to(self.device)\n",
    "        batch_done = torch.FloatTensor(batch_done).unsqueeze(1).to(self.device)\n",
    "        \n",
    "        current_q = self.policy_net(batch_state).gather(1, batch_action)\n",
    "        next_q = self.target_net(batch_next_state).max(1)[0].unsqueeze(1)\n",
    "        target_q = batch_reward + self.gamma * next_q * (1 - batch_done)\n",
    "        \n",
    "        loss = nn.MSELoss()(current_q, target_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "# ----------------------------\n",
    "# Training Loop with Milestone Rendering\n",
    "# ----------------------------\n",
    "def train():\n",
    "    num_episodes = 1000\n",
    "    max_steps = 500\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.05\n",
    "    epsilon_decay = 0.9967\n",
    "    target_update_freq = 10\n",
    "    # Define milestone episodes (10%, 50%, 75%, and 100% of training)\n",
    "    milestones = {int(num_episodes * 0.10), int(num_episodes * 0.5), int(num_episodes * 0.75), num_episodes}\n",
    "    \n",
    "    env = Gridworld()\n",
    "    # Each agent's state dimension: 4 + (2*vision_radius+1)^2.\n",
    "    state_dim = 4 + (2 * env.vision_radius + 1) ** 2\n",
    "    action_dim = 4  # up, down, left, right\n",
    "    agents = [DQNAgent(state_dim, action_dim), DQNAgent(state_dim, action_dim)]\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        states = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        # Check if this is a milestone episode (episode indices are 0-based)\n",
    "        do_render = (episode + 1) in milestones\n",
    "        if do_render:\n",
    "            pygame.init()\n",
    "            screen = pygame.display.set_mode((WINDOW_SIZE, WINDOW_SIZE))\n",
    "            pygame.display.set_caption(f\"Training Episode {episode + 1}\")\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            actions = []\n",
    "            # Each agent selects an action using its own DQN policy\n",
    "            for i in range(2):\n",
    "                action = agents[i].select_action(states[i], epsilon)\n",
    "                actions.append(action)\n",
    "            next_states, reward, done = env.step(actions)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Store experiences for each agent\n",
    "            for i in range(2):\n",
    "                agents[i].replay_buffer.push(states[i], actions[i], reward, next_states[i], done)\n",
    "            \n",
    "            states = next_states\n",
    "            \n",
    "            # Update each agent's policy network\n",
    "            for i in range(2):\n",
    "                agents[i].update()\n",
    "            \n",
    "            if do_render:\n",
    "                env.render(screen)\n",
    "                # Process quit events in visualization mode\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.QUIT:\n",
    "                        pygame.quit()\n",
    "                        return\n",
    "                pygame.time.wait(50)\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        if do_render:\n",
    "            # Pause to allow viewing the final state of the milestone episode\n",
    "            pygame.time.wait(1000)\n",
    "            pygame.quit()\n",
    "        \n",
    "        # Update target networks periodically\n",
    "        if (episode + 1) % target_update_freq == 0:\n",
    "            for agent in agents:\n",
    "                agent.update_target()\n",
    "        \n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        print(f\"Episode {episode + 1}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea773b1b-2908-43b2-a412-a6de7d9ecea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pygame\n",
    "\n",
    "# Enable cuDNN benchmarking for performance (if using GPU)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Constants for gridworld and visualization\n",
    "GRID_SIZE = 15\n",
    "CELL_SIZE = 100   # pixels per cell\n",
    "WINDOW_SIZE = GRID_SIZE * CELL_SIZE\n",
    "NUM_OBSTACLES = 10\n",
    "\n",
    "# ----------------------------\n",
    "# Environment (Gridworld)\n",
    "# ----------------------------\n",
    "class Gridworld:\n",
    "    def __init__(self):\n",
    "        self.grid_size = GRID_SIZE\n",
    "        self.cell_size = CELL_SIZE\n",
    "        self.vision_radius = 5  # drones can sense obstacles within 5 cells\n",
    "        # Define goals: agent 0's goal is bottom-right, agent 1's goal is bottom-left\n",
    "        self.goals = [(GRID_SIZE - 1, GRID_SIZE - 1), (0, GRID_SIZE - 1)]\n",
    "        self.obstacles = self.generate_obstacles()\n",
    "        # Both agents start at top-left (0,0)\n",
    "        self.agents = [(0, 0), (0, 0)]\n",
    "        # Penalty and reward values\n",
    "        self.movement_penalty = -1       # per move per agent\n",
    "        self.closeness_penalty = -10     # if Manhattan distance < 2\n",
    "        self.collision_penalty = -50     # if agent attempts to move into an obstacle\n",
    "        self.goal_reward = 50            # increased goal reward to incentivize reaching the goal\n",
    "        self.shaping_factor = 0.5        # reward shaping factor for progress towards goal\n",
    "\n",
    "    def generate_obstacles(self):\n",
    "        obstacles = set()\n",
    "        # Generate random fixed obstacles while avoiding start and goal positions\n",
    "        while len(obstacles) < NUM_OBSTACLES:\n",
    "            x = random.randint(0, self.grid_size - 1)\n",
    "            y = random.randint(0, self.grid_size - 1)\n",
    "            if (x, y) != (0, 0) and (x, y) not in self.goals:\n",
    "                obstacles.add((x, y))\n",
    "        return list(obstacles)\n",
    "\n",
    "    def get_obstacle_view(self, agent_position):\n",
    "        view = []\n",
    "        ax, ay = agent_position\n",
    "        for dy in range(-self.vision_radius, self.vision_radius + 1):\n",
    "            for dx in range(-self.vision_radius, self.vision_radius + 1):\n",
    "                cx = ax + dx\n",
    "                cy = ay + dy\n",
    "                if 0 <= cx < self.grid_size and 0 <= cy < self.grid_size:\n",
    "                    if (cx, cy) in self.obstacles:\n",
    "                        view.append(1.0)\n",
    "                    else:\n",
    "                        view.append(0.0)\n",
    "                else:\n",
    "                    view.append(0.0)\n",
    "        return view\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset agents to starting positions; obstacles remain fixed.\n",
    "        self.agents = [(0, 0), (0, 0)]\n",
    "        return self.get_states()\n",
    "\n",
    "    def get_states(self):\n",
    "        # Each agent's state: [agent_x, agent_y, goal_x, goal_y] + flattened obstacle view\n",
    "        states = []\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            goal = self.goals[idx]\n",
    "            base_state = [pos[0], pos[1], goal[0], goal[1]]\n",
    "            obstacle_view = self.get_obstacle_view(pos)\n",
    "            states.append(base_state + obstacle_view)\n",
    "        return states\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Expects a list of actions (one per agent: 0: up, 1: down, 2: left, 3: right).\n",
    "        Updates agent positions and returns:\n",
    "          - next_states: list of states for each agent,\n",
    "          - reward: a scalar reward that includes:\n",
    "                * movement penalty,\n",
    "                * collision penalty (if agent attempts to move into an obstacle),\n",
    "                * reward shaping (for progress toward the goal),\n",
    "                * closeness penalty (if Manhattan distance < 2),\n",
    "                * goal achievement rewards.\n",
    "          - done: True if both agents have reached their goals.\n",
    "        \"\"\"\n",
    "        reward = 0\n",
    "        # Apply movement penalty for each agent\n",
    "        reward += self.movement_penalty * len(self.agents)\n",
    "        \n",
    "        # Compute old Manhattan distances for reward shaping\n",
    "        old_distances = []\n",
    "        for idx, (x, y) in enumerate(self.agents):\n",
    "            goal = self.goals[idx]\n",
    "            old_distance = abs(x - goal[0]) + abs(y - goal[1])\n",
    "            old_distances.append(old_distance)\n",
    "        \n",
    "        new_positions = []\n",
    "        for idx, (x, y) in enumerate(self.agents):\n",
    "            dx, dy = 0, 0\n",
    "            action = actions[idx]\n",
    "            if action == 0:  # up\n",
    "                dy = -1\n",
    "            elif action == 1:  # down\n",
    "                dy = 1\n",
    "            elif action == 2:  # left\n",
    "                dx = -1\n",
    "            elif action == 3:  # right\n",
    "                dx = 1\n",
    "            new_x, new_y = x + dx, y + dy\n",
    "            # Check boundaries; invalid moves result in staying in place.\n",
    "            if new_x < 0 or new_x >= self.grid_size or new_y < 0 or new_y >= self.grid_size:\n",
    "                new_x, new_y = x, y\n",
    "            # Check obstacles; if moving into an obstacle, apply collision penalty and stay in place.\n",
    "            if (new_x, new_y) in self.obstacles:\n",
    "                reward += self.collision_penalty\n",
    "                new_x, new_y = x, y\n",
    "            new_positions.append((new_x, new_y))\n",
    "        self.agents = new_positions\n",
    "\n",
    "        # Reward shaping: add reward proportional to progress toward goal\n",
    "        for idx, (x, y) in enumerate(self.agents):\n",
    "            goal = self.goals[idx]\n",
    "            new_distance = abs(x - goal[0]) + abs(y - goal[1])\n",
    "            # If agent gets closer, the difference (old - new) is positive.\n",
    "            shaping_reward = self.shaping_factor * (old_distances[idx] - new_distance)\n",
    "            reward += shaping_reward\n",
    "\n",
    "        # Closeness penalty: if Manhattan distance between agents < 3\n",
    "        manhattan_distance = abs(self.agents[0][0] - self.agents[1][0]) + abs(self.agents[0][1] - self.agents[1][1])\n",
    "        if manhattan_distance < 3:\n",
    "            reward += self.closeness_penalty\n",
    "\n",
    "        # Reward for reaching the goal for each agent.\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            if pos == self.goals[idx]:\n",
    "                reward += self.goal_reward\n",
    "\n",
    "        # Episode ends if both agents have reached their goals.\n",
    "        done = (self.agents[0] == self.goals[0] and self.agents[1] == self.goals[1])\n",
    "        next_states = self.get_states()\n",
    "        return next_states, reward, done\n",
    "\n",
    "    def render(self, screen):\n",
    "        # Clear screen\n",
    "        screen.fill((255, 255, 255))\n",
    "        # Draw grid lines\n",
    "        for x in range(0, WINDOW_SIZE, self.cell_size):\n",
    "            pygame.draw.line(screen, (200, 200, 200), (x, 0), (x, WINDOW_SIZE))\n",
    "        for y in range(0, WINDOW_SIZE, self.cell_size):\n",
    "            pygame.draw.line(screen, (200, 200, 200), (0, y), (WINDOW_SIZE, y))\n",
    "        # Draw obstacles as black rectangles\n",
    "        for obs in self.obstacles:\n",
    "            rect = pygame.Rect(obs[0]*self.cell_size, obs[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (0, 0, 0), rect)\n",
    "        # Draw goals as green rectangles\n",
    "        for goal in self.goals:\n",
    "            rect = pygame.Rect(goal[0]*self.cell_size, goal[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (0, 255, 0), rect)\n",
    "        # Draw agents as circles (agent 0 in red, agent 1 in blue)\n",
    "        colors = [(255, 0, 0), (0, 0, 255)]\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            center = (pos[0]*self.cell_size + self.cell_size//2,\n",
    "                      pos[1]*self.cell_size + self.cell_size//2)\n",
    "            pygame.draw.circle(screen, colors[idx], center, self.cell_size//3)\n",
    "        pygame.display.flip()\n",
    "\n",
    "# ----------------------------\n",
    "# Replay Buffer for DQN\n",
    "# ----------------------------\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# ----------------------------\n",
    "# DQN Network and Agent (GPU-enabled)\n",
    "# ----------------------------\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_dim, output_dim, lr=1e-3, gamma=0.99,\n",
    "                 buffer_capacity=10000, batch_size=32):\n",
    "        # Set device to GPU if available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def select_action(self, state, epsilon):\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(4)  # 4 discrete actions: up, down, left, right\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "        return q_values.argmax().item()\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.replay_buffer.sample(self.batch_size)\n",
    "        batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*transitions)\n",
    "        \n",
    "        batch_state = torch.FloatTensor(batch_state).to(self.device)\n",
    "        batch_action = torch.LongTensor(batch_action).unsqueeze(1).to(self.device)\n",
    "        batch_reward = torch.FloatTensor(batch_reward).unsqueeze(1).to(self.device)\n",
    "        batch_next_state = torch.FloatTensor(batch_next_state).to(self.device)\n",
    "        batch_done = torch.FloatTensor(batch_done).unsqueeze(1).to(self.device)\n",
    "        \n",
    "        current_q = self.policy_net(batch_state).gather(1, batch_action)\n",
    "        next_q = self.target_net(batch_next_state).max(1)[0].unsqueeze(1)\n",
    "        target_q = batch_reward + self.gamma * next_q * (1 - batch_done)\n",
    "        \n",
    "        loss = nn.MSELoss()(current_q, target_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "# ----------------------------\n",
    "# Training Loop with Milestone Rendering\n",
    "# ----------------------------\n",
    "def train():\n",
    "    num_episodes = 1000\n",
    "    max_steps = 500\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.05\n",
    "    epsilon_decay = 0.9967\n",
    "    target_update_freq = 10\n",
    "    # Define milestone episodes (10%, 50%, 75%, and 100% of training)\n",
    "    milestones = {int(num_episodes * 0.10), int(num_episodes * 0.5), int(num_episodes * 0.75), num_episodes}\n",
    "    \n",
    "    env = Gridworld()\n",
    "    # Each agent's state dimension: 4 + (2*vision_radius+1)^2.\n",
    "    state_dim = 4 + (2 * env.vision_radius + 1) ** 2\n",
    "    action_dim = 4  # up, down, left, right\n",
    "    agents = [DQNAgent(state_dim, action_dim), DQNAgent(state_dim, action_dim)]\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        states = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        # Check if this is a milestone episode (episode indices are 0-based)\n",
    "        do_render = (episode + 1) in milestones\n",
    "        if do_render:\n",
    "            pygame.init()\n",
    "            screen = pygame.display.set_mode((WINDOW_SIZE, WINDOW_SIZE))\n",
    "            pygame.display.set_caption(f\"Training Episode {episode + 1}\")\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            actions = []\n",
    "            # Each agent selects an action using its own DQN policy\n",
    "            for i in range(2):\n",
    "                action = agents[i].select_action(states[i], epsilon)\n",
    "                actions.append(action)\n",
    "            next_states, reward, done = env.step(actions)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Store experiences for each agent\n",
    "            for i in range(2):\n",
    "                agents[i].replay_buffer.push(states[i], actions[i], reward, next_states[i], done)\n",
    "            \n",
    "            states = next_states\n",
    "            \n",
    "            # Update each agent's policy network\n",
    "            for i in range(2):\n",
    "                agents[i].update()\n",
    "            \n",
    "            if do_render:\n",
    "                env.render(screen)\n",
    "                # Process quit events in visualization mode\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.QUIT:\n",
    "                        pygame.quit()\n",
    "                        return\n",
    "                pygame.time.wait(50)\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        if do_render:\n",
    "            # Pause to allow viewing the final state of the milestone episode\n",
    "            pygame.time.wait(1000)\n",
    "            pygame.quit()\n",
    "        \n",
    "        # Update target networks periodically\n",
    "        if (episode + 1) % target_update_freq == 0:\n",
    "            for agent in agents:\n",
    "                agent.update_target()\n",
    "        \n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        print(f\"Episode {episode + 1}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c333ddb-db92-436b-9753-a1ad967a1f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pygame\n",
    "import cv2  # if you plan to capture video, otherwise ignore\n",
    "\n",
    "# Enable cuDNN benchmarking for performance (if using GPU)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Constants for gridworld and visualization\n",
    "GRID_SIZE = 15\n",
    "CELL_SIZE = 100   # pixels per cell\n",
    "WINDOW_SIZE = GRID_SIZE * CELL_SIZE\n",
    "NUM_OBSTACLES = 10\n",
    "\n",
    "# ----------------------------\n",
    "# Environment (Gridworld)\n",
    "# ----------------------------\n",
    "class Gridworld:\n",
    "    def __init__(self):\n",
    "        self.grid_size = GRID_SIZE\n",
    "        self.cell_size = CELL_SIZE\n",
    "        self.vision_radius = 5  # drones can sense obstacles within 5 cells\n",
    "        # Define goals:\n",
    "        # Agent 0's goal is bottom-right; Agent 1's goal is bottom-left.\n",
    "        self.goals = [(GRID_SIZE - 1, GRID_SIZE - 1), (0, GRID_SIZE - 1)]\n",
    "        \n",
    "        # Pre-defined best paths (drones already know the best route)\n",
    "        # For agent 0: go right along the top row, then down the rightmost column.\n",
    "        self.best_path_agent0 = [(x, 0) for x in range(self.grid_size)] + [(self.grid_size - 1, y) for y in range(1, self.grid_size)]\n",
    "        # For agent 1: go down along the leftmost column.\n",
    "        self.best_path_agent1 = [(0, y) for y in range(self.grid_size)]\n",
    "        \n",
    "        self.obstacles = self.generate_obstacles()\n",
    "        self.congestion_zones = []  # will be updated each step\n",
    "        \n",
    "        # Both agents start at top-left (0,0)\n",
    "        self.agents = [(0, 0), (0, 0)]\n",
    "        \n",
    "        # Penalty and reward values\n",
    "        self.movement_penalty = -1        # per move per agent\n",
    "        self.closeness_penalty = -10      # if Manhattan distance between agents < 3\n",
    "        self.collision_penalty = -50      # if agent attempts to move into an obstacle\n",
    "        self.congestion_penalty = -20     # penalty for passing through a congestion zone\n",
    "        self.goal_reward = 50             # reward for reaching its own goal\n",
    "        self.shaping_factor = 0.5         # reward shaping factor for progress towards goal\n",
    "\n",
    "    def generate_obstacles(self):\n",
    "        obstacles = set()\n",
    "        # Generate random fixed obstacles while avoiding start and goal positions\n",
    "        while len(obstacles) < NUM_OBSTACLES:\n",
    "            x = random.randint(0, self.grid_size - 1)\n",
    "            y = random.randint(0, self.grid_size - 1)\n",
    "            if (x, y) != (0, 0) and (x, y) not in self.goals:\n",
    "                obstacles.add((x, y))\n",
    "        return list(obstacles)\n",
    "\n",
    "    def update_congestion_zones(self):\n",
    "        # Generate up to 3 random congestion zones along the union of both best paths.\n",
    "        num_zones = 3\n",
    "        possible_zones = list(set(self.best_path_agent0 + self.best_path_agent1))\n",
    "        if possible_zones:\n",
    "            self.congestion_zones = random.sample(possible_zones, min(num_zones, len(possible_zones)))\n",
    "        else:\n",
    "            self.congestion_zones = []\n",
    "\n",
    "    def get_obstacle_view(self, agent_position):\n",
    "        view = []\n",
    "        ax, ay = agent_position\n",
    "        for dy in range(-self.vision_radius, self.vision_radius + 1):\n",
    "            for dx in range(-self.vision_radius, self.vision_radius + 1):\n",
    "                cx = ax + dx\n",
    "                cy = ay + dy\n",
    "                if 0 <= cx < self.grid_size and 0 <= cy < self.grid_size:\n",
    "                    view.append(1.0 if (cx, cy) in self.obstacles else 0.0)\n",
    "                else:\n",
    "                    view.append(0.0)\n",
    "        return view\n",
    "\n",
    "    def get_congestion_view(self, agent_position):\n",
    "        view = []\n",
    "        ax, ay = agent_position\n",
    "        for dy in range(-self.vision_radius, self.vision_radius + 1):\n",
    "            for dx in range(-self.vision_radius, self.vision_radius + 1):\n",
    "                cx = ax + dx\n",
    "                cy = ay + dy\n",
    "                if 0 <= cx < self.grid_size and 0 <= cy < self.grid_size:\n",
    "                    view.append(1.0 if (cx, cy) in self.congestion_zones else 0.0)\n",
    "                else:\n",
    "                    view.append(0.0)\n",
    "        return view\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset agents to starting positions; obstacles remain fixed.\n",
    "        self.agents = [(0, 0), (0, 0)]\n",
    "        return self.get_states()\n",
    "\n",
    "    def get_states(self):\n",
    "        # Each agent's state: [agent_x, agent_y, goal_x, goal_y] +\n",
    "        # flattened obstacle view + flattened congestion view.\n",
    "        states = []\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            goal = self.goals[idx]\n",
    "            base_state = [pos[0], pos[1], goal[0], goal[1]]\n",
    "            obstacle_view = self.get_obstacle_view(pos)\n",
    "            congestion_view = self.get_congestion_view(pos)\n",
    "            states.append(base_state + obstacle_view + congestion_view)\n",
    "        return states\n",
    "\n",
    "    def step(self, actions):\n",
    "        # Update congestion zones each step\n",
    "        self.update_congestion_zones()\n",
    "        \n",
    "        reward = 0\n",
    "        # Apply movement penalty for each agent\n",
    "        reward += self.movement_penalty * len(self.agents)\n",
    "        \n",
    "        # Compute old Manhattan distances for reward shaping\n",
    "        old_distances = []\n",
    "        for idx, (x, y) in enumerate(self.agents):\n",
    "            goal = self.goals[idx]\n",
    "            old_distance = abs(x - goal[0]) + abs(y - goal[1])\n",
    "            old_distances.append(old_distance)\n",
    "        \n",
    "        new_positions = []\n",
    "        for idx, (x, y) in enumerate(self.agents):\n",
    "            dx, dy = 0, 0\n",
    "            action = actions[idx]\n",
    "            if action == 0:  # up\n",
    "                dy = -1\n",
    "            elif action == 1:  # down\n",
    "                dy = 1\n",
    "            elif action == 2:  # left\n",
    "                dx = -1\n",
    "            elif action == 3:  # right\n",
    "                dx = 1\n",
    "            new_x, new_y = x + dx, y + dy\n",
    "            # Check boundaries; invalid moves result in staying in place.\n",
    "            if new_x < 0 or new_x >= self.grid_size or new_y < 0 or new_y >= self.grid_size:\n",
    "                new_x, new_y = x, y\n",
    "            # Check obstacles; if moving into an obstacle, apply collision penalty and stay in place.\n",
    "            if (new_x, new_y) in self.obstacles:\n",
    "                reward += self.collision_penalty\n",
    "                new_x, new_y = x, y\n",
    "            new_positions.append((new_x, new_y))\n",
    "        self.agents = new_positions\n",
    "\n",
    "        # Apply congestion zone penalty if an agent is in a congestion zone.\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            if pos in self.congestion_zones:\n",
    "                reward += self.congestion_penalty\n",
    "\n",
    "        # Reward shaping: reward proportional to progress toward goal\n",
    "        for idx, (x, y) in enumerate(self.agents):\n",
    "            goal = self.goals[idx]\n",
    "            new_distance = abs(x - goal[0]) + abs(y - goal[1])\n",
    "            shaping_reward = self.shaping_factor * (old_distances[idx] - new_distance)\n",
    "            reward += shaping_reward\n",
    "\n",
    "        # Closeness penalty: if Manhattan distance between agents < 3\n",
    "        manhattan_distance = abs(self.agents[0][0] - self.agents[1][0]) + abs(self.agents[0][1] - self.agents[1][1])\n",
    "        if manhattan_distance < 3:\n",
    "            reward += self.closeness_penalty\n",
    "\n",
    "        # Reward for reaching the goal for each agent (only their own goal).\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            if pos == self.goals[idx]:\n",
    "                reward += self.goal_reward\n",
    "\n",
    "        # Episode ends if both agents have reached their respective goals.\n",
    "        done = (self.agents[0] == self.goals[0] and self.agents[1] == self.goals[1])\n",
    "        next_states = self.get_states()\n",
    "        return next_states, reward, done\n",
    "\n",
    "    def render(self, screen):\n",
    "        # Clear screen\n",
    "        screen.fill((255, 255, 255))\n",
    "        # Draw grid lines\n",
    "        for x in range(0, WINDOW_SIZE, self.cell_size):\n",
    "            pygame.draw.line(screen, (200, 200, 200), (x, 0), (x, WINDOW_SIZE))\n",
    "        for y in range(0, WINDOW_SIZE, self.cell_size):\n",
    "            pygame.draw.line(screen, (200, 200, 200), (0, y), (WINDOW_SIZE, y))\n",
    "        # Draw obstacles as black rectangles\n",
    "        for obs in self.obstacles:\n",
    "            rect = pygame.Rect(obs[0]*self.cell_size, obs[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (0, 0, 0), rect)\n",
    "        # Draw congestion zones as orange rectangles\n",
    "        for cz in self.congestion_zones:\n",
    "            rect = pygame.Rect(cz[0]*self.cell_size, cz[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (255, 165, 0), rect)\n",
    "        # Draw best paths for each agent\n",
    "        if hasattr(self, \"best_path_agent0\") and len(self.best_path_agent0) > 1:\n",
    "            points0 = [(x * self.cell_size + self.cell_size//2, y * self.cell_size + self.cell_size//2) for (x, y) in self.best_path_agent0]\n",
    "            pygame.draw.lines(screen, (255, 0, 255), False, points0, 3)\n",
    "        if hasattr(self, \"best_path_agent1\") and len(self.best_path_agent1) > 1:\n",
    "            points1 = [(x * self.cell_size + self.cell_size//2, y * self.cell_size + self.cell_size//2) for (x, y) in self.best_path_agent1]\n",
    "            pygame.draw.lines(screen, (0, 255, 255), False, points1, 3)\n",
    "        # Draw goals as green rectangles\n",
    "        for goal in self.goals:\n",
    "            rect = pygame.Rect(goal[0]*self.cell_size, goal[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (0, 255, 0), rect)\n",
    "        # Draw agents as circles (agent 0 in red, agent 1 in blue)\n",
    "        colors = [(255, 0, 0), (0, 0, 255)]\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            center = (pos[0]*self.cell_size + self.cell_size//2,\n",
    "                      pos[1]*self.cell_size + self.cell_size//2)\n",
    "            pygame.draw.circle(screen, colors[idx], center, self.cell_size//3)\n",
    "        pygame.display.flip()\n",
    "\n",
    "# ----------------------------\n",
    "# Replay Buffer for DQN\n",
    "# ----------------------------\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# ----------------------------\n",
    "# DQN Network and Agent (GPU-enabled)\n",
    "# ----------------------------\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_dim, output_dim, lr=1e-3, gamma=0.99,\n",
    "                 buffer_capacity=10000, batch_size=32):\n",
    "        # Set device to GPU if available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def select_action(self, state, epsilon):\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(4)  # 4 discrete actions: up, down, left, right\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "        return q_values.argmax().item()\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.replay_buffer.sample(self.batch_size)\n",
    "        batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*transitions)\n",
    "        \n",
    "        batch_state = torch.FloatTensor(batch_state).to(self.device)\n",
    "        batch_action = torch.LongTensor(batch_action).unsqueeze(1).to(self.device)\n",
    "        batch_reward = torch.FloatTensor(batch_reward).unsqueeze(1).to(self.device)\n",
    "        batch_next_state = torch.FloatTensor(batch_next_state).to(self.device)\n",
    "        batch_done = torch.FloatTensor(batch_done).unsqueeze(1).to(self.device)\n",
    "        \n",
    "        current_q = self.policy_net(batch_state).gather(1, batch_action)\n",
    "        next_q = self.target_net(batch_next_state).max(1)[0].unsqueeze(1)\n",
    "        target_q = batch_reward + self.gamma * next_q * (1 - batch_done)\n",
    "        \n",
    "        loss = nn.MSELoss()(current_q, target_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "# ----------------------------\n",
    "# Training Loop with Milestone Rendering and Video Capture\n",
    "# ----------------------------\n",
    "def train():\n",
    "    num_episodes = 1000\n",
    "    max_steps = 500\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.05\n",
    "    epsilon_decay = 0.9967\n",
    "    target_update_freq = 10\n",
    "    # Define milestone episodes (10%, 50%, 75%, and 100% of training)\n",
    "    milestones = {10, int(num_episodes * 0.10), int(num_episodes * 0.5), int(num_episodes * 0.75), num_episodes}\n",
    "    \n",
    "    # Print which device is being used\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    env = Gridworld()\n",
    "    # Each agent's state dimension:\n",
    "    # 4 (base) + obstacle view + congestion view, each view is (2*vision_radius+1)^2.\n",
    "    state_dim = 4 + 2 * ((2 * env.vision_radius + 1) ** 2)\n",
    "    action_dim = 4  # up, down, left, right\n",
    "    agents = [DQNAgent(state_dim, action_dim), DQNAgent(state_dim, action_dim)]\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        states = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        # Check if this is a milestone episode (episode indices are 0-based)\n",
    "        do_render = (episode + 1) in milestones\n",
    "        video_writer = None\n",
    "        if do_render:\n",
    "            pygame.init()\n",
    "            screen = pygame.display.set_mode((WINDOW_SIZE, WINDOW_SIZE))\n",
    "            pygame.display.set_caption(f\"Training Episode {episode + 1}\")\n",
    "            # Setup video writer (requires OpenCV)\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            video_filename = f\"episode_{episode+1}.mp4\"\n",
    "            fps = 20  # based on wait time of 50ms per frame\n",
    "            video_writer = cv2.VideoWriter(video_filename, fourcc, fps, (WINDOW_SIZE, WINDOW_SIZE))\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            actions = []\n",
    "            # Each agent selects an action using its own DQN policy\n",
    "            for i in range(2):\n",
    "                action = agents[i].select_action(states[i], epsilon)\n",
    "                actions.append(action)\n",
    "            next_states, reward, done = env.step(actions)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Store experiences for each agent\n",
    "            for i in range(2):\n",
    "                agents[i].replay_buffer.push(states[i], actions[i], reward, next_states[i], done)\n",
    "            \n",
    "            states = next_states\n",
    "            \n",
    "            # Update each agent's policy network\n",
    "            for i in range(2):\n",
    "                agents[i].update()\n",
    "            \n",
    "            if do_render:\n",
    "                env.render(screen)\n",
    "                # Capture the current frame\n",
    "                frame = pygame.surfarray.array3d(screen)\n",
    "                frame = np.transpose(frame, (1, 0, 2))  # convert to (height, width, channels)\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                video_writer.write(frame)\n",
    "                # Process quit events in visualization mode\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.QUIT:\n",
    "                        pygame.quit()\n",
    "                        if video_writer is not None:\n",
    "                            video_writer.release()\n",
    "                        return\n",
    "                pygame.time.wait(50)\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        if do_render:\n",
    "            pygame.time.wait(1000)\n",
    "            pygame.quit()\n",
    "            if video_writer is not None:\n",
    "                video_writer.release()\n",
    "        \n",
    "        # Update target networks periodically\n",
    "        if (episode + 1) % target_update_freq == 0:\n",
    "            for agent in agents:\n",
    "                agent.update_target()\n",
    "        \n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        print(f\"Episode {episode + 1}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e93f61b-c762-4e1c-9c38-efb154f4e4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pygame\n",
    "import cv2  # if you plan to capture video, otherwise ignore\n",
    "\n",
    "# Enable cuDNN benchmarking for performance (if using GPU)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Constants for gridworld and visualization\n",
    "GRID_SIZE = 15\n",
    "CELL_SIZE = 100   # pixels per cell\n",
    "WINDOW_SIZE = GRID_SIZE * CELL_SIZE\n",
    "NUM_OBSTACLES = 25  # Increased number of obstacles\n",
    "\n",
    "# ----------------------------\n",
    "# Environment (Gridworld)\n",
    "# ----------------------------\n",
    "class Gridworld:\n",
    "    def __init__(self):\n",
    "        self.grid_size = GRID_SIZE\n",
    "        self.cell_size = CELL_SIZE\n",
    "        self.vision_radius = 5  # drones can sense obstacles within 5 cells\n",
    "        \n",
    "        # Define goals:\n",
    "        # Agent 0's goal is bottom-right; Agent 1's goal is bottom-left.\n",
    "        self.goals = [(GRID_SIZE - 1, GRID_SIZE - 1), (0, GRID_SIZE - 1)]\n",
    "        \n",
    "        # Pre-defined best paths (drones already know the best route)\n",
    "        # For agent 0: go right along the top row, then down the rightmost column.\n",
    "        self.best_path_agent0 = [(x, 0) for x in range(self.grid_size)] + [(self.grid_size - 1, y) for y in range(1, self.grid_size)]\n",
    "        # For agent 1: go down along the leftmost column.\n",
    "        self.best_path_agent1 = [(0, y) for y in range(self.grid_size)]\n",
    "        \n",
    "        self.obstacles = self.generate_obstacles()\n",
    "        self.congestion_zones = []  # Will be updated with a timer\n",
    "        self.congestion_duration = 50  # Congestion zones persist for 50 steps\n",
    "        self.congestion_timer = 0  # Initialize timer\n",
    "        \n",
    "        # Both agents start at top-left (0,0)\n",
    "        self.agents = [(0, 0), (0, 0)]\n",
    "        \n",
    "        # Penalty and reward values\n",
    "        self.movement_penalty = -1        # per move per agent\n",
    "        self.closeness_penalty = -10      # if Manhattan distance between agents < 3\n",
    "        self.collision_penalty = -50      # if agent attempts to move into an obstacle\n",
    "        self.congestion_penalty = -20     # penalty for passing through a congestion zone\n",
    "        self.goal_reward = 50             # reward for reaching its own goal\n",
    "        self.shaping_factor = 0.5         # reward shaping factor for progress toward goal\n",
    "        \n",
    "        # Parameters for following the best path\n",
    "        self.best_path_bonus = 10         # bonus reward for being on the best path\n",
    "        self.deviation_penalty = -10      # penalty for deviating when no congestion is present\n",
    "\n",
    "    def generate_obstacles(self):\n",
    "        obstacles = set()\n",
    "        # Generate random fixed obstacles while avoiding start and goal positions\n",
    "        while len(obstacles) < NUM_OBSTACLES:\n",
    "            x = random.randint(0, self.grid_size - 1)\n",
    "            y = random.randint(0, self.grid_size - 1)\n",
    "            if (x, y) != (0, 0) and (x, y) not in self.goals:\n",
    "                obstacles.add((x, y))\n",
    "        return list(obstacles)\n",
    "\n",
    "    def update_congestion_zones(self):\n",
    "        # Only update congestion zones if the timer has expired.\n",
    "        if self.congestion_timer > 0:\n",
    "            self.congestion_timer -= 1\n",
    "            return\n",
    "        # Generate up to 3 random congestion zones along the union of both best paths.\n",
    "        num_zones = 3\n",
    "        possible_zones = list(set(self.best_path_agent0 + self.best_path_agent1))\n",
    "        if possible_zones:\n",
    "            self.congestion_zones = random.sample(possible_zones, min(num_zones, len(possible_zones)))\n",
    "        else:\n",
    "            self.congestion_zones = []\n",
    "        self.congestion_timer = self.congestion_duration\n",
    "\n",
    "    def get_obstacle_view(self, agent_position):\n",
    "        view = []\n",
    "        ax, ay = agent_position\n",
    "        for dy in range(-self.vision_radius, self.vision_radius + 1):\n",
    "            for dx in range(-self.vision_radius, self.vision_radius + 1):\n",
    "                cx = ax + dx\n",
    "                cy = ay + dy\n",
    "                if 0 <= cx < self.grid_size and 0 <= cy < self.grid_size:\n",
    "                    view.append(1.0 if (cx, cy) in self.obstacles else 0.0)\n",
    "                else:\n",
    "                    view.append(0.0)\n",
    "        return view\n",
    "\n",
    "    def get_congestion_view(self, agent_position):\n",
    "        view = []\n",
    "        ax, ay = agent_position\n",
    "        for dy in range(-self.vision_radius, self.vision_radius + 1):\n",
    "            for dx in range(-self.vision_radius, self.vision_radius + 1):\n",
    "                cx = ax + dx\n",
    "                cy = ay + dy\n",
    "                if 0 <= cx < self.grid_size and 0 <= cy < self.grid_size:\n",
    "                    view.append(1.0 if (cx, cy) in self.congestion_zones else 0.0)\n",
    "                else:\n",
    "                    view.append(0.0)\n",
    "        return view\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset agents to starting positions; obstacles remain fixed.\n",
    "        self.agents = [(0, 0), (0, 0)]\n",
    "        return self.get_states()\n",
    "\n",
    "    def get_states(self):\n",
    "        # Each agent's state: [agent_x, agent_y, goal_x, goal_y] +\n",
    "        # flattened obstacle view + flattened congestion view.\n",
    "        states = []\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            goal = self.goals[idx]\n",
    "            base_state = [pos[0], pos[1], goal[0], goal[1]]\n",
    "            obstacle_view = self.get_obstacle_view(pos)\n",
    "            congestion_view = self.get_congestion_view(pos)\n",
    "            states.append(base_state + obstacle_view + congestion_view)\n",
    "        return states\n",
    "\n",
    "    def step(self, actions):\n",
    "        # Update congestion zones (they persist for a fixed number of steps)\n",
    "        self.update_congestion_zones()\n",
    "        \n",
    "        reward = 0\n",
    "        # Apply movement penalty for each agent\n",
    "        reward += self.movement_penalty * len(self.agents)\n",
    "        \n",
    "        # Compute old Manhattan distances for reward shaping\n",
    "        old_distances = []\n",
    "        for idx, (x, y) in enumerate(self.agents):\n",
    "            goal = self.goals[idx]\n",
    "            old_distance = abs(x - goal[0]) + abs(y - goal[1])\n",
    "            old_distances.append(old_distance)\n",
    "        \n",
    "        new_positions = []\n",
    "        for idx, (x, y) in enumerate(self.agents):\n",
    "            dx, dy = 0, 0\n",
    "            action = actions[idx]\n",
    "            if action == 0:  # up\n",
    "                dy = -1\n",
    "            elif action == 1:  # down\n",
    "                dy = 1\n",
    "            elif action == 2:  # left\n",
    "                dx = -1\n",
    "            elif action == 3:  # right\n",
    "                dx = 1\n",
    "            new_x, new_y = x + dx, y + dy\n",
    "            # Check boundaries; invalid moves result in staying in place.\n",
    "            if new_x < 0 or new_x >= self.grid_size or new_y < 0 or new_y >= self.grid_size:\n",
    "                new_x, new_y = x, y\n",
    "            # Check obstacles; if moving into an obstacle, apply collision penalty and stay in place.\n",
    "            if (new_x, new_y) in self.obstacles:\n",
    "                reward += self.collision_penalty\n",
    "                new_x, new_y = x, y\n",
    "            new_positions.append((new_x, new_y))\n",
    "        self.agents = new_positions\n",
    "\n",
    "        # Apply congestion zone penalty if an agent is in a congestion zone.\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            if pos in self.congestion_zones:\n",
    "                reward += self.congestion_penalty\n",
    "\n",
    "        # Reward shaping: reward proportional to progress toward goal.\n",
    "        for idx, (x, y) in enumerate(self.agents):\n",
    "            goal = self.goals[idx]\n",
    "            new_distance = abs(x - goal[0]) + abs(y - goal[1])\n",
    "            shaping_reward = self.shaping_factor * (old_distances[idx] - new_distance)\n",
    "            reward += shaping_reward\n",
    "\n",
    "        # Best path bonus and deviation penalty.\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            best_path = self.best_path_agent0 if idx == 0 else self.best_path_agent1\n",
    "            if pos in best_path:\n",
    "                reward += self.best_path_bonus\n",
    "            else:\n",
    "                # Only penalize deviation if no congestion is visible in the agent's vision.\n",
    "                congestion_view = self.get_congestion_view(pos)\n",
    "                if not any(cell == 1.0 for cell in congestion_view):\n",
    "                    reward += self.deviation_penalty\n",
    "\n",
    "        # Closeness penalty: if Manhattan distance between agents < 3.\n",
    "        manhattan_distance = abs(self.agents[0][0] - self.agents[1][0]) + abs(self.agents[0][1] - self.agents[1][1])\n",
    "        if manhattan_distance < 3:\n",
    "            reward += self.closeness_penalty\n",
    "\n",
    "        # Reward for reaching the goal for each agent (only for its own goal).\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            if pos == self.goals[idx]:\n",
    "                reward += self.goal_reward\n",
    "\n",
    "        # Episode ends if both agents have reached their respective goals.\n",
    "        done = (self.agents[0] == self.goals[0] and self.agents[1] == self.goals[1])\n",
    "        next_states = self.get_states()\n",
    "        return next_states, reward, done\n",
    "\n",
    "    def render(self, screen):\n",
    "        # Clear screen\n",
    "        screen.fill((255, 255, 255))\n",
    "        # Draw grid lines\n",
    "        for x in range(0, WINDOW_SIZE, self.cell_size):\n",
    "            pygame.draw.line(screen, (200, 200, 200), (x, 0), (x, WINDOW_SIZE))\n",
    "        for y in range(0, WINDOW_SIZE, self.cell_size):\n",
    "            pygame.draw.line(screen, (200, 200, 200), (0, y), (WINDOW_SIZE, y))\n",
    "        # Draw obstacles as black rectangles\n",
    "        for obs in self.obstacles:\n",
    "            rect = pygame.Rect(obs[0]*self.cell_size, obs[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (0, 0, 0), rect)\n",
    "        # Draw congestion zones as orange rectangles\n",
    "        for cz in self.congestion_zones:\n",
    "            rect = pygame.Rect(cz[0]*self.cell_size, cz[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (255, 165, 0), rect)\n",
    "        # Draw best paths for each agent\n",
    "        if hasattr(self, \"best_path_agent0\") and len(self.best_path_agent0) > 1:\n",
    "            points0 = [(x * self.cell_size + self.cell_size//2, y * self.cell_size + self.cell_size//2) for (x, y) in self.best_path_agent0]\n",
    "            pygame.draw.lines(screen, (255, 0, 255), False, points0, 3)\n",
    "        if hasattr(self, \"best_path_agent1\") and len(self.best_path_agent1) > 1:\n",
    "            points1 = [(x * self.cell_size + self.cell_size//2, y * self.cell_size + self.cell_size//2) for (x, y) in self.best_path_agent1]\n",
    "            pygame.draw.lines(screen, (0, 255, 255), False, points1, 3)\n",
    "        # Draw goals as green rectangles\n",
    "        for goal in self.goals:\n",
    "            rect = pygame.Rect(goal[0]*self.cell_size, goal[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (0, 255, 0), rect)\n",
    "        # Draw agents as circles (agent 0 in red, agent 1 in blue)\n",
    "        colors = [(255, 0, 0), (0, 0, 255)]\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            center = (pos[0]*self.cell_size + self.cell_size//2,\n",
    "                      pos[1]*self.cell_size + self.cell_size//2)\n",
    "            pygame.draw.circle(screen, colors[idx], center, self.cell_size//3)\n",
    "        pygame.display.flip()\n",
    "\n",
    "# ----------------------------\n",
    "# Replay Buffer for DQN\n",
    "# ----------------------------\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# ----------------------------\n",
    "# DQN Network and Agent (GPU-enabled)\n",
    "# ----------------------------\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_dim, output_dim, lr=1e-3, gamma=0.99,\n",
    "                 buffer_capacity=10000, batch_size=32):\n",
    "        # Set device to GPU if available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def select_action(self, state, epsilon):\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(4)  # 4 discrete actions: up, down, left, right\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "        return q_values.argmax().item()\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.replay_buffer.sample(self.batch_size)\n",
    "        batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*transitions)\n",
    "        \n",
    "        batch_state = torch.FloatTensor(batch_state).to(self.device)\n",
    "        batch_action = torch.LongTensor(batch_action).unsqueeze(1).to(self.device)\n",
    "        batch_reward = torch.FloatTensor(batch_reward).unsqueeze(1).to(self.device)\n",
    "        batch_next_state = torch.FloatTensor(batch_next_state).to(self.device)\n",
    "        batch_done = torch.FloatTensor(batch_done).unsqueeze(1).to(self.device)\n",
    "        \n",
    "        current_q = self.policy_net(batch_state).gather(1, batch_action)\n",
    "        next_q = self.target_net(batch_next_state).max(1)[0].unsqueeze(1)\n",
    "        target_q = batch_reward + self.gamma * next_q * (1 - batch_done)\n",
    "        \n",
    "        loss = nn.MSELoss()(current_q, target_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "# ----------------------------\n",
    "# Training Loop with Milestone Rendering and Video Capture\n",
    "# ----------------------------\n",
    "def train():\n",
    "    num_episodes = 1000\n",
    "    max_steps = 500\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.05\n",
    "    epsilon_decay = 0.9967\n",
    "    target_update_freq = 10\n",
    "    # Define milestone episodes (10%, 50%, 75%, and 100% of training)\n",
    "    milestones = {10, int(num_episodes * 0.10), int(num_episodes * 0.5), int(num_episodes * 0.75), num_episodes}\n",
    "    \n",
    "    # Print which device is being used\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    env = Gridworld()\n",
    "    # Each agent's state dimension:\n",
    "    # 4 (base) + obstacle view + congestion view, each view is (2*vision_radius+1)^2.\n",
    "    state_dim = 4 + 2 * ((2 * env.vision_radius + 1) ** 2)\n",
    "    action_dim = 4  # up, down, left, right\n",
    "    agents = [DQNAgent(state_dim, action_dim), DQNAgent(state_dim, action_dim)]\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        states = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        # Check if this is a milestone episode (episode indices are 0-based)\n",
    "        do_render = (episode + 1) in milestones\n",
    "        video_writer = None\n",
    "        if do_render:\n",
    "            pygame.init()\n",
    "            screen = pygame.display.set_mode((WINDOW_SIZE, WINDOW_SIZE))\n",
    "            pygame.display.set_caption(f\"Training Episode {episode + 1}\")\n",
    "            # Setup video writer (requires OpenCV)\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            video_filename = f\"episode_{episode+1}.mp4\"\n",
    "            fps = 20  # based on wait time of 50ms per frame\n",
    "            video_writer = cv2.VideoWriter(video_filename, fourcc, fps, (WINDOW_SIZE, WINDOW_SIZE))\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            actions = []\n",
    "            # Each agent selects an action using its own DQN policy\n",
    "            for i in range(2):\n",
    "                action = agents[i].select_action(states[i], epsilon)\n",
    "                actions.append(action)\n",
    "            next_states, reward, done = env.step(actions)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Store experiences for each agent\n",
    "            for i in range(2):\n",
    "                agents[i].replay_buffer.push(states[i], actions[i], reward, next_states[i], done)\n",
    "            \n",
    "            states = next_states\n",
    "            \n",
    "            # Update each agent's policy network\n",
    "            for i in range(2):\n",
    "                agents[i].update()\n",
    "            \n",
    "            if do_render:\n",
    "                env.render(screen)\n",
    "                # Capture the current frame\n",
    "                frame = pygame.surfarray.array3d(screen)\n",
    "                frame = np.transpose(frame, (1, 0, 2))  # convert to (height, width, channels)\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                video_writer.write(frame)\n",
    "                # Process quit events in visualization mode\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.QUIT:\n",
    "                        pygame.quit()\n",
    "                        if video_writer is not None:\n",
    "                            video_writer.release()\n",
    "                        return\n",
    "                pygame.time.wait(50)\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        if do_render:\n",
    "            pygame.time.wait(1000)\n",
    "            pygame.quit()\n",
    "            if video_writer is not None:\n",
    "                video_writer.release()\n",
    "        \n",
    "        # Update target networks periodically\n",
    "        if (episode + 1) % target_update_freq == 0:\n",
    "            for agent in agents:\n",
    "                agent.update_target()\n",
    "        \n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        print(f\"Episode {episode + 1}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ae5d80a-0c19-4560-86fa-15b1f7daf618",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torch' has no attribute 'fx' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\EZ\\Lib\\site-packages\\torch\\__init__.py:2222\u001b[0m\n\u001b[0;32m   2218\u001b[0m sys\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.classes\u001b[39m\u001b[38;5;124m\"\u001b[39m, classes)\n\u001b[0;32m   2220\u001b[0m \u001b[38;5;66;03m# quantization depends on torch.fx and torch.ops\u001b[39;00m\n\u001b[0;32m   2221\u001b[0m \u001b[38;5;66;03m# Import quantization\u001b[39;00m\n\u001b[1;32m-> 2222\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m quantization \u001b[38;5;28;01mas\u001b[39;00m quantization  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m   2224\u001b[0m \u001b[38;5;66;03m# Import the quasi random sampler\u001b[39;00m\n\u001b[0;32m   2225\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m quasirandom \u001b[38;5;28;01mas\u001b[39;00m quasirandom  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\EZ\\Lib\\site-packages\\torch\\quantization\\__init__.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfuser_method_mappings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mobserver\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquant_type\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization_mappings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\EZ\\Lib\\site-packages\\torch\\quantization\\qconfig.py:9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# flake8: noqa: F401\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mThis file is in the process of migration to `torch/ao/quantization`, and\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mis kept here for compatibility while the migration process is ongoing.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03mhere.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mao\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     _add_module_to_qconfig_obs_ctr,\n\u001b[0;32m     11\u001b[0m     _assert_valid_qconfig,\n\u001b[0;32m     12\u001b[0m     default_activation_only_qconfig,\n\u001b[0;32m     13\u001b[0m     default_debug_qconfig,\n\u001b[0;32m     14\u001b[0m     default_dynamic_qconfig,\n\u001b[0;32m     15\u001b[0m     default_per_channel_qconfig,\n\u001b[0;32m     16\u001b[0m     default_qat_qconfig,\n\u001b[0;32m     17\u001b[0m     default_qat_qconfig_v2,\n\u001b[0;32m     18\u001b[0m     default_qconfig,\n\u001b[0;32m     19\u001b[0m     default_weight_only_qconfig,\n\u001b[0;32m     20\u001b[0m     float16_dynamic_qconfig,\n\u001b[0;32m     21\u001b[0m     float16_static_qconfig,\n\u001b[0;32m     22\u001b[0m     float_qparams_weight_only_qconfig,\n\u001b[0;32m     23\u001b[0m     get_default_qat_qconfig,\n\u001b[0;32m     24\u001b[0m     get_default_qconfig,\n\u001b[0;32m     25\u001b[0m     per_channel_dynamic_qconfig,\n\u001b[0;32m     26\u001b[0m     QConfig,\n\u001b[0;32m     27\u001b[0m     qconfig_equals,\n\u001b[0;32m     28\u001b[0m     QConfigAny,\n\u001b[0;32m     29\u001b[0m     QConfigDynamic,\n\u001b[0;32m     30\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\EZ\\Lib\\site-packages\\torch\\ao\\quantization\\__init__.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfuser_method_mappings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mobserver\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpt2e\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_numeric_debugger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     compare_results,\n\u001b[0;32m     14\u001b[0m     CUSTOM_KEY,\n\u001b[0;32m     15\u001b[0m     extract_results_from_loggers,\n\u001b[0;32m     16\u001b[0m     generate_numeric_debug_handle,\n\u001b[0;32m     17\u001b[0m     NUMERIC_DEBUG_HANDLE_KEY,\n\u001b[0;32m     18\u001b[0m     prepare_for_propagation_comparison,\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpt2e\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     21\u001b[0m     _allow_exported_model_train_eval \u001b[38;5;28;01mas\u001b[39;00m allow_exported_model_train_eval,\n\u001b[0;32m     22\u001b[0m     _move_exported_model_to_eval \u001b[38;5;28;01mas\u001b[39;00m move_exported_model_to_eval,\n\u001b[0;32m     23\u001b[0m     _move_exported_model_to_train \u001b[38;5;28;01mas\u001b[39;00m move_exported_model_to_train,\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\EZ\\Lib\\site-packages\\torch\\ao\\quantization\\pt2e\\_numeric_debugger.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mao\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mns\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_sqnr\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mao\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpt2e\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_control_flow_submodules\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexport\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExportedProgram\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GraphModule, Node\n",
      "File \u001b[1;32m~\\.conda\\envs\\EZ\\Lib\\site-packages\\torch\\ao\\quantization\\pt2e\\graph_utils.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Node\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msource_matcher_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     check_subgraphs_connected,\n\u001b[0;32m     10\u001b[0m     get_source_partitions,\n\u001b[0;32m     11\u001b[0m     SourcePartition,\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     15\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfind_sequential_partitions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_control_flow_submodules\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_equivalent_types\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate_equivalent_types_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     20\u001b[0m ]\n\u001b[0;32m     22\u001b[0m _EQUIVALENT_TYPES: List[Set] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     23\u001b[0m     {torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mConv1d, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mconv1d},\n\u001b[0;32m     24\u001b[0m     {torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mConv2d, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mconv2d},\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     {torch\u001b[38;5;241m.\u001b[39mmul, operator\u001b[38;5;241m.\u001b[39mmul, operator\u001b[38;5;241m.\u001b[39mimul, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmul\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmul_\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     31\u001b[0m ]\n",
      "File \u001b[1;32m~\\.conda\\envs\\EZ\\Lib\\site-packages\\torch\\fx\\passes\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     graph_drawer,\n\u001b[0;32m      3\u001b[0m     graph_manipulation,\n\u001b[0;32m      4\u001b[0m     net_min_base,\n\u001b[0;32m      5\u001b[0m     operator_support,\n\u001b[0;32m      6\u001b[0m     param_fetch,\n\u001b[0;32m      7\u001b[0m     reinplace,\n\u001b[0;32m      8\u001b[0m     runtime_assert,\n\u001b[0;32m      9\u001b[0m     shape_prop,\n\u001b[0;32m     10\u001b[0m     split_module,\n\u001b[0;32m     11\u001b[0m     split_utils,\n\u001b[0;32m     12\u001b[0m     splitter_base,\n\u001b[0;32m     13\u001b[0m     tools_common,\n\u001b[0;32m     14\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\EZ\\Lib\\site-packages\\torch\\fx\\passes\\graph_drawer.py:13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnode\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _format_arg, _get_qualified_name\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moperator_schemas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m normalize_function\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshape_prop\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorMetadata\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydot\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\EZ\\Lib\\site-packages\\torch\\fx\\passes\\shape_prop.py:85\u001b[0m\n\u001b[0;32m     77\u001b[0m             qparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maxis\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mq_per_channel_axis()  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TensorMetadata(\n\u001b[0;32m     80\u001b[0m         shape, dtype, requires_grad, stride, memory_format, is_quantized, qparams\n\u001b[0;32m     81\u001b[0m     )\n\u001b[0;32m     84\u001b[0m \u001b[38;5;129m@compatibility\u001b[39m(is_backward_compatible\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mShapeProp\u001b[39;00m(torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mInterpreter):\n\u001b[0;32m     86\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124;03m    Execute an FX graph Node-by-Node and\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03m    record the shape and type of the result\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    129\u001b[0m \n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gm, fake_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'torch' has no attribute 'fx' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pygame\n",
    "import cv2  # for video capture\n",
    "\n",
    "# Enable cuDNN benchmarking for performance (if using GPU)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Constants for gridworld and visualization\n",
    "GRID_SIZE = 15\n",
    "CELL_SIZE = 50   # decreased cell size so that the grid is smaller on a 1440p screen\n",
    "WINDOW_SIZE = GRID_SIZE * CELL_SIZE\n",
    "NUM_OBSTACLES = 25  # increased number of obstacles\n",
    "\n",
    "# ----------------------------\n",
    "# Environment (Gridworld)\n",
    "# ----------------------------\n",
    "class Gridworld:\n",
    "    def __init__(self):\n",
    "        self.grid_size = GRID_SIZE\n",
    "        self.cell_size = CELL_SIZE\n",
    "        self.vision_radius = 5  # drones can sense obstacles within 5 cells\n",
    "        \n",
    "        # Define goals:\n",
    "        # Agent 0's goal is bottom-right; Agent 1's goal is bottom-left.\n",
    "        self.goals = [(GRID_SIZE - 1, GRID_SIZE - 1), (0, GRID_SIZE - 1)]\n",
    "        \n",
    "        self.obstacles = self.generate_obstacles()\n",
    "        \n",
    "        # Compute best paths using A* (only considering obstacles)\n",
    "        self.best_path_agent0 = self.a_star((0, 0), self.goals[0])\n",
    "        self.best_path_agent1 = self.a_star((0, 0), self.goals[1])\n",
    "        \n",
    "        # Congestion zones (dynamic) with a timer\n",
    "        self.congestion_zones = []\n",
    "        self.congestion_duration = 50  # congestion zones persist for 50 steps\n",
    "        self.congestion_timer = 0\n",
    "        \n",
    "        # Both agents start at top-left (0,0)\n",
    "        self.agents = [(0, 0), (0, 0)]\n",
    "        \n",
    "        # Penalty and reward values\n",
    "        self.movement_penalty = -1\n",
    "        self.closeness_penalty = -10\n",
    "        self.collision_penalty = -50\n",
    "        self.congestion_penalty = -20\n",
    "        self.goal_reward = 50\n",
    "        self.shaping_factor = 0.5\n",
    "        \n",
    "        # Parameters for following the best path\n",
    "        self.best_path_bonus = 10\n",
    "        self.deviation_penalty = -10\n",
    "\n",
    "    def generate_obstacles(self):\n",
    "        obstacles = set()\n",
    "        while len(obstacles) < NUM_OBSTACLES:\n",
    "            x = random.randint(0, self.grid_size - 1)\n",
    "            y = random.randint(0, self.grid_size - 1)\n",
    "            if (x, y) != (0, 0) and (x, y) not in self.goals:\n",
    "                obstacles.add((x, y))\n",
    "        return list(obstacles)\n",
    "\n",
    "    def heuristic(self, node, goal):\n",
    "        return abs(node[0] - goal[0]) + abs(node[1] - goal[1])\n",
    "\n",
    "    def get_neighbors(self, node):\n",
    "        (x, y) = node\n",
    "        neighbors = []\n",
    "        for dx, dy in [(1,0), (-1,0), (0,1), (0,-1)]:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if 0 <= nx < self.grid_size and 0 <= ny < self.grid_size:\n",
    "                if (nx, ny) not in self.obstacles:\n",
    "                    neighbors.append((nx, ny))\n",
    "        return neighbors\n",
    "\n",
    "    def a_star(self, start, goal):\n",
    "        import heapq\n",
    "        open_set = []\n",
    "        heapq.heappush(open_set, (0, start))\n",
    "        came_from = {}\n",
    "        g_score = {start: 0}\n",
    "        while open_set:\n",
    "            current_f, current = heapq.heappop(open_set)\n",
    "            if current == goal:\n",
    "                path = [current]\n",
    "                while current in came_from:\n",
    "                    current = came_from[current]\n",
    "                    path.append(current)\n",
    "                path.reverse()\n",
    "                return path\n",
    "            for neighbor in self.get_neighbors(current):\n",
    "                tentative_g = g_score[current] + 1\n",
    "                if neighbor not in g_score or tentative_g < g_score[neighbor]:\n",
    "                    came_from[neighbor] = current\n",
    "                    g_score[neighbor] = tentative_g\n",
    "                    f_score = tentative_g + self.heuristic(neighbor, goal)\n",
    "                    heapq.heappush(open_set, (f_score, neighbor))\n",
    "        return []\n",
    "\n",
    "    def update_congestion_zones(self):\n",
    "        if self.congestion_timer > 0:\n",
    "            self.congestion_timer -= 1\n",
    "            return\n",
    "        num_zones = 6  # increased number of congestion zones\n",
    "        possible_zones = list(set(self.best_path_agent0 + self.best_path_agent1))\n",
    "        if possible_zones:\n",
    "            self.congestion_zones = random.sample(possible_zones, min(num_zones, len(possible_zones)))\n",
    "        else:\n",
    "            self.congestion_zones = []\n",
    "        self.congestion_timer = self.congestion_duration\n",
    "\n",
    "    def get_obstacle_view(self, agent_position):\n",
    "        view = []\n",
    "        ax, ay = agent_position\n",
    "        for dy in range(-self.vision_radius, self.vision_radius + 1):\n",
    "            for dx in range(-self.vision_radius, self.vision_radius + 1):\n",
    "                cx, cy = ax + dx, ay + dy\n",
    "                if 0 <= cx < self.grid_size and 0 <= cy < self.grid_size:\n",
    "                    view.append(1.0 if (cx, cy) in self.obstacles else 0.0)\n",
    "                else:\n",
    "                    view.append(0.0)\n",
    "        return view\n",
    "\n",
    "    def get_congestion_view(self, agent_position):\n",
    "        view = []\n",
    "        ax, ay = agent_position\n",
    "        for dy in range(-self.vision_radius, self.vision_radius + 1):\n",
    "            for dx in range(-self.vision_radius, self.vision_radius + 1):\n",
    "                cx, cy = ax + dx, ay + dy\n",
    "                if 0 <= cx < self.grid_size and 0 <= cy < self.grid_size:\n",
    "                    view.append(1.0 if (cx, cy) in self.congestion_zones else 0.0)\n",
    "                else:\n",
    "                    view.append(0.0)\n",
    "        return view\n",
    "\n",
    "    def reset(self):\n",
    "        self.agents = [(0, 0), (0, 0)]\n",
    "        return self.get_states()\n",
    "\n",
    "    def get_states(self):\n",
    "        states = []\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            goal = self.goals[idx]\n",
    "            base_state = [pos[0], pos[1], goal[0], goal[1]]\n",
    "            obstacle_view = self.get_obstacle_view(pos)\n",
    "            congestion_view = self.get_congestion_view(pos)\n",
    "            states.append(base_state + obstacle_view + congestion_view)\n",
    "        return states\n",
    "\n",
    "    def step(self, actions):\n",
    "        self.update_congestion_zones()\n",
    "        reward = 0\n",
    "        reward += self.movement_penalty * len(self.agents)\n",
    "        \n",
    "        old_distances = []\n",
    "        for idx, (x, y) in enumerate(self.agents):\n",
    "            goal = self.goals[idx]\n",
    "            old_distances.append(abs(x - goal[0]) + abs(y - goal[1]))\n",
    "        \n",
    "        new_positions = []\n",
    "        for idx, (x, y) in enumerate(self.agents):\n",
    "            dx, dy = 0, 0\n",
    "            action = actions[idx]\n",
    "            if action == 0:\n",
    "                dy = -1\n",
    "            elif action == 1:\n",
    "                dy = 1\n",
    "            elif action == 2:\n",
    "                dx = -1\n",
    "            elif action == 3:\n",
    "                dx = 1\n",
    "            new_x, new_y = x + dx, y + dy\n",
    "            if new_x < 0 or new_x >= self.grid_size or new_y < 0 or new_y >= self.grid_size:\n",
    "                new_x, new_y = x, y\n",
    "            if (new_x, new_y) in self.obstacles:\n",
    "                reward += self.collision_penalty\n",
    "                new_x, new_y = x, y\n",
    "            new_positions.append((new_x, new_y))\n",
    "        self.agents = new_positions\n",
    "        \n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            if pos in self.congestion_zones:\n",
    "                reward += self.congestion_penalty\n",
    "        \n",
    "        for idx, (x, y) in enumerate(self.agents):\n",
    "            goal = self.goals[idx]\n",
    "            new_distance = abs(x - goal[0]) + abs(y - goal[1])\n",
    "            reward += self.shaping_factor * (old_distances[idx] - new_distance)\n",
    "        \n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            best_path = self.best_path_agent0 if idx == 0 else self.best_path_agent1\n",
    "            if pos in best_path:\n",
    "                reward += self.best_path_bonus\n",
    "            else:\n",
    "                congestion_view = self.get_congestion_view(pos)\n",
    "                if not any(cell == 1.0 for cell in congestion_view):\n",
    "                    reward += self.deviation_penalty\n",
    "        \n",
    "        manhattan_distance = abs(self.agents[0][0] - self.agents[1][0]) + abs(self.agents[0][1] - self.agents[1][1])\n",
    "        if manhattan_distance < 3:\n",
    "            reward += self.closeness_penalty\n",
    "        \n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            if pos == self.goals[idx]:\n",
    "                reward += self.goal_reward\n",
    "        \n",
    "        done = (self.agents[0] == self.goals[0] and self.agents[1] == self.goals[1])\n",
    "        next_states = self.get_states()\n",
    "        return next_states, reward, done\n",
    "\n",
    "    def render(self, screen):\n",
    "        screen.fill((255, 255, 255))\n",
    "        for x in range(0, WINDOW_SIZE, self.cell_size):\n",
    "            pygame.draw.line(screen, (200, 200, 200), (x, 0), (x, WINDOW_SIZE))\n",
    "        for y in range(0, WINDOW_SIZE, self.cell_size):\n",
    "            pygame.draw.line(screen, (200, 200, 200), (0, y), (WINDOW_SIZE, y))\n",
    "        for obs in self.obstacles:\n",
    "            rect = pygame.Rect(obs[0]*self.cell_size, obs[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (0, 0, 0), rect)\n",
    "        for cz in self.congestion_zones:\n",
    "            rect = pygame.Rect(cz[0]*self.cell_size, cz[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, (255, 165, 0), rect)\n",
    "        # Draw computed best paths\n",
    "        if self.best_path_agent0 and len(self.best_path_agent0) > 1:\n",
    "            points0 = [(x * self.cell_size + self.cell_size//2, y * self.cell_size + self.cell_size//2)\n",
    "                       for (x, y) in self.best_path_agent0]\n",
    "            pygame.draw.lines(screen, (255, 0, 255), False, points0, 3)\n",
    "        if self.best_path_agent1 and len(self.best_path_agent1) > 1:\n",
    "            points1 = [(x * self.cell_size + self.cell_size//2, y * self.cell_size + self.cell_size//2)\n",
    "                       for (x, y) in self.best_path_agent1]\n",
    "            pygame.draw.lines(screen, (0, 255, 255), False, points1, 3)\n",
    "        # Draw goals with colors matching the drones:\n",
    "        goal_colors = [(255, 0, 0), (0, 0, 255)]\n",
    "        for idx, goal in enumerate(self.goals):\n",
    "            rect = pygame.Rect(goal[0]*self.cell_size, goal[1]*self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(screen, goal_colors[idx], rect)\n",
    "        # Draw agents as circles (agent 0 in red, agent 1 in blue)\n",
    "        colors = [(255, 0, 0), (0, 0, 255)]\n",
    "        for idx, pos in enumerate(self.agents):\n",
    "            center = (pos[0]*self.cell_size + self.cell_size//2,\n",
    "                      pos[1]*self.cell_size + self.cell_size//2)\n",
    "            pygame.draw.circle(screen, colors[idx], center, self.cell_size//3)\n",
    "        pygame.display.flip()\n",
    "\n",
    "# ----------------------------\n",
    "# Replay Buffer for DQN\n",
    "# ----------------------------\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# ----------------------------\n",
    "# DQN Network and Agent (GPU-enabled)\n",
    "# ----------------------------\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_dim, output_dim, lr=1e-3, gamma=0.99,\n",
    "                 buffer_capacity=10000, batch_size=32):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net = DQN(input_dim, output_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def select_action(self, state, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(4)\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "        return q_values.argmax().item()\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.replay_buffer.sample(self.batch_size)\n",
    "        batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*transitions)\n",
    "        \n",
    "        batch_state = torch.FloatTensor(batch_state).to(self.device)\n",
    "        batch_action = torch.LongTensor(batch_action).unsqueeze(1).to(self.device)\n",
    "        batch_reward = torch.FloatTensor(batch_reward).unsqueeze(1).to(self.device)\n",
    "        batch_next_state = torch.FloatTensor(batch_next_state).to(self.device)\n",
    "        batch_done = torch.FloatTensor(batch_done).unsqueeze(1).to(self.device)\n",
    "        \n",
    "        current_q = self.policy_net(batch_state).gather(1, batch_action)\n",
    "        next_q = self.target_net(batch_next_state).max(1)[0].unsqueeze(1)\n",
    "        target_q = batch_reward + self.gamma * next_q * (1 - batch_done)\n",
    "        \n",
    "        loss = nn.MSELoss()(current_q, target_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "# ----------------------------\n",
    "# Training Loop with Milestone Rendering and Video Capture\n",
    "# ----------------------------\n",
    "def train():\n",
    "    num_episodes = 1000\n",
    "    max_steps = 500\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.05\n",
    "    epsilon_decay = 0.9967\n",
    "    target_update_freq = 10\n",
    "    milestones = {int(num_episodes * 0.5), int(num_episodes * 0.75), num_episodes}\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    env = Gridworld()\n",
    "    state_dim = 4 + 2 * ((2 * env.vision_radius + 1) ** 2)\n",
    "    action_dim = 4\n",
    "    agents = [DQNAgent(state_dim, action_dim), DQNAgent(state_dim, action_dim)]\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        states = env.reset()\n",
    "        total_reward = 0\n",
    "        do_render = (episode + 1) in milestones\n",
    "        video_writer = None\n",
    "        if do_render:\n",
    "            pygame.init()\n",
    "            screen = pygame.display.set_mode((WINDOW_SIZE, WINDOW_SIZE))\n",
    "            pygame.display.set_caption(f\"Training Episode {episode + 1}\")\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            video_filename = f\"episode_{episode+1}.mp4\"\n",
    "            fps = 20\n",
    "            video_writer = cv2.VideoWriter(video_filename, fourcc, fps, (WINDOW_SIZE, WINDOW_SIZE))\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            actions = []\n",
    "            for i in range(2):\n",
    "                action = agents[i].select_action(states[i], epsilon)\n",
    "                actions.append(action)\n",
    "            next_states, reward, done = env.step(actions)\n",
    "            total_reward += reward\n",
    "            \n",
    "            for i in range(2):\n",
    "                agents[i].replay_buffer.push(states[i], actions[i], reward, next_states[i], done)\n",
    "            \n",
    "            states = next_states\n",
    "            \n",
    "            for i in range(2):\n",
    "                agents[i].update()\n",
    "            \n",
    "            if do_render:\n",
    "                env.render(screen)\n",
    "                frame = pygame.surfarray.array3d(screen)\n",
    "                frame = np.transpose(frame, (1, 0, 2))\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                video_writer.write(frame)\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.QUIT:\n",
    "                        pygame.quit()\n",
    "                        if video_writer is not None:\n",
    "                            video_writer.release()\n",
    "                        return\n",
    "                pygame.time.wait(50)\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        if do_render:\n",
    "            pygame.time.wait(1000)\n",
    "            pygame.quit()\n",
    "            if video_writer is not None:\n",
    "                video_writer.release()\n",
    "        \n",
    "        if (episode + 1) % target_update_freq == 0:\n",
    "            for agent in agents:\n",
    "                agent.update_target()\n",
    "        \n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        print(f\"Episode {episode + 1}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2de061-ad51-4d02-8d83-39b8d4068f13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
